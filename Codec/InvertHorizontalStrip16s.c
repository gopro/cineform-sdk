/*! @file InvertHorizontalStrip16s.c

*  @brief 
*
*  @version 1.0.0
*
*  (C) Copyright 2017 GoPro Inc (http://gopro.com/).
*
*  Licensed under either:
*  - Apache License, Version 2.0, http://www.apache.org/licenses/LICENSE-2.0  
*  - MIT license, http://opensource.org/licenses/MIT
*  at your option.
*
*  Unless required by applicable law or agreed to in writing, software
*  distributed under the License is distributed on an "AS IS" BASIS,
*  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
*  See the License for the specific language governing permissions and
*  limitations under the License.
*
*/
#include "config.h"
#include "timing.h"

#ifndef DEBUG
#define DEBUG  (1 && _DEBUG)
#endif
#define TIMING (1 && _TIMING)
#define XMMOPT (1 && _XMMOPT)

#define PREFETCH (1 && _PREFETCH)

#include <assert.h>
#include <math.h>
#include <limits.h>
#include <mmintrin.h>		// MMX intrinsics
#include <emmintrin.h>		// SSE2 intrinsics

#include "spatial.h"
#include "filter.h"			// Declarations of filter routines
//#include "image.h"		// Image processing data types
//#include "ipp.h"			// Use Intel Performance Primitives
//#include "debug.h"
#include "codec.h"
#include "buffer.h"
#include "quantize.h"
#include "convert.h"
#include "decoder.h"
#include "bayer.h"
#include "swap.h"
#include "RGB2YUV.h"

#define _PREROLL 1		// Enable loop preprocessing for memory alignment

// Forward reference (to avoid including encoder.h)
//typedef struct encoder ENCODER;
struct encoder;

#if DEBUG
// Make the logfile available for debugging
#include <stdio.h>
extern FILE *logfile;
#endif

#ifndef _QUANTIZE_SPATIAL_LOWPASS
#define _QUANTIZE_SPATIAL_LOWPASS	0
#endif

//#ifndef _UNALIGNED
//#define _UNALIGNED	0
//#endif
#ifndef _UNALIGNED
#define _UNALIGNED	0
//#elif (_UNALIGNED == 1)    // Hack for VS2012 and beyond as default behavior for VS sets _UNALIGNED == __unaligned keyword)
//do nothing
#else
#undef _UNALIGNED
#define _UNALIGNED	0
#endif

#ifndef _FASTLOOP
#define _FASTLOOP	1
#endif


// Shifts used to remove prescaling in the thumbnail spatial transform
#define V210_HORIZONTAL_SHIFT	2
#define V210_VERTICAL_SHIFT		0





#if _PROCESSOR_DISPATCH

__declspec(cpu_dispatch(Pentium_4,Generic))

void InvertHorizontalStrip16s(PIXEL *lowpass_band,	// Horizontal lowpass coefficients
							  int lowpass_pitch,	// Distance between rows in bytes
							  PIXEL *highpass_band,	// Horizontal highpass coefficients
							  int highpass_pitch,	// Distance between rows in bytes
							  PIXEL *output_image,	// Row of reconstructed results
							  int output_pitch,		// Distance between rows in bytes
							  ROI roi)			// Height and width of the strip
{
	// Stub routine for processor specific dispatch
}

#endif


#if _PROCESSOR_GENERIC

#if _PROCESSOR_DISPATCH
__declspec(cpu_specific(Generic))
#endif


// Apply the inverse horizontal transform to reconstruct a strip of rows
void InvertHorizontalStrip16s(PIXEL *lowpass_band,	// Horizontal lowpass coefficients
							  int lowpass_pitch,	// Distance between rows in bytes
							  PIXEL *highpass_band,	// Horizontal highpass coefficients
							  int highpass_pitch,	// Distance between rows in bytes
							  PIXEL *output_image,	// Row of reconstructed results
							  int output_pitch,		// Distance between rows in bytes
							  ROI roi)			// Height and width of the strip
{



	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 4;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m64 low1_pi16;	// Lowpass coefficients
		__m64 low2_pi16;
		__m64 high1_pi16;	// Highpass coefficients
		__m64 high2_pi16;

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

		// The fast loop computes output points starting at the third column
		__m64 *outptr = (__m64 *)&output[2];

		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Place the even result in the even column
		output[0] = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Place the odd result in the odd column
		output[1] = SATURATE(odd);

#if (1 && XMMOPT)

		// Preload the first four lowpass coefficients
		low1_pi16 = *((__m64 *)&lowpass[column]);

		// Preload the first four highpass coefficients
		high1_pi16 = *((__m64 *)&highpass[column]);

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m64 even_pi16;	// Result of convolution with even filter
			__m64 odd_pi16;		// Result of convolution with odd filter
			__m64 temp_pi16;
			//__m64 temp2_pi16;
			__m64 out_pi16;		// Reconstructed data
			__m64 mask_pi16;
			__m64 half_pi16;
			__m64 lsb_pi16;
			__m64 sign_pi16;
			__m64 high_pi16;

			// Preload the next four lowpass coefficients
			low2_pi16 = *((__m64 *)&lowpass[column+4]);


			half_pi16 = _mm_set1_pi16(4);


			// Compute the first two even and two odd output points //

			// Apply the even reconstruction filter to the lowpass band
			/*even_pi16 = low1_pi16;  // +1
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1)); +8
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); +1 +8
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); -1
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16); -1
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);*/

//DAN031304 -- correct inverse filter
			even_pi16 = low1_pi16;//+1x
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); //-1x
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16); //+1a -1c
			even_pi16 = _mm_adds_pi16(even_pi16, half_pi16); //+4
			even_pi16 = _mm_srai_pi16(even_pi16, 3);  // (+1a -1c) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(0, 3, 2, 1)); //+8x
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); // (+1a +8b -1c) >> 3


			// Shift the highpass correction by one column
			high1_pi16 = _mm_srli_si64(high1_pi16, 16);

			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			even_pi16 = _mm_srai_pi16(even_pi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			/*odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);*/

			odd_pi16 = low1_pi16;//-1x
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); //+1x
			odd_pi16 = _mm_subs_pi16(temp_pi16, odd_pi16); //-1a +1c
			odd_pi16 = _mm_adds_pi16(odd_pi16, half_pi16); //+4
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);  // (-1a +1c) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(0, 3, 2, 1)); //+8x
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16); // (-1a +8b +1c) >> 3

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());
			*(outptr++) = out_pi16;


			// Compute the second two even and two odd output points //

			// Preload the highpass correction
			high2_pi16 = *((__m64 *)&highpass[column+4]);

			// Shift in the new pixels for the next stage of the loop
			low1_pi16 = _mm_srli_si64(low1_pi16, 32);
			temp_pi16 = _mm_slli_si64(low2_pi16, 32);
			low1_pi16 = _mm_or_si64(low1_pi16, temp_pi16);

			// Apply the even reconstruction filter to the lowpass band
		/*	even_pi16 = low1_pi16;
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);*/

			even_pi16 = low1_pi16;//+1x
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); //-1x
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16); //+1a -1c
			even_pi16 = _mm_adds_pi16(even_pi16, half_pi16); //+4
			even_pi16 = _mm_srai_pi16(even_pi16, 3);  // (+1a -1c) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(0, 3, 2, 1)); //+8x
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); // (+1a +8b -1c) >> 3

			// Shift in the next two highpass coefficients
			high1_pi16 = _mm_srli_si64(high1_pi16, 2*16);
			high1_pi16 = _mm_or_si64(high1_pi16, _mm_slli_si64(high2_pi16, 16));

			// Prescale for 8bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			even_pi16 = _mm_srai_pi16(even_pi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
		/*	odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);*/

			odd_pi16 = low1_pi16;//-1x
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); //+1x
			odd_pi16 = _mm_subs_pi16(temp_pi16, odd_pi16); //-1a +1c
			odd_pi16 = _mm_adds_pi16(odd_pi16, half_pi16); //+4
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);  // (-1a +1c) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(0, 3, 2, 1)); //+8x
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16); // (-1a +8b +1c) >> 3


			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());
			*(outptr++) = out_pi16;

			// The second four lowpass coefficients will be the current values
			low1_pi16 = low2_pi16;

			// The second four highpass coefficients will be the current values
			high1_pi16 = high2_pi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Process the rest of the columns up to the last column in the row
		colptr = (PIXEL *)outptr;

		for (; column < last_column; column++)
		{
			int32_t even = 0;		// Result of convolution with even filter
			int32_t odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += ROUNDING(even,8);
			even = DivideByShift(even, 3);
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			even = DivideByShift(even, 1);

			// Place the even result in the even column
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += ROUNDING(odd,8);
			odd = DivideByShift(odd, 3);
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			odd = DivideByShift(odd, 1);

			// Place the odd result in the odd column
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Place the even result in the even column
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Place the odd result in the odd column
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}

	//_mm_empty();	// Clear the mmx register state
}

#endif


#if _PROCESSOR_PENTIUM_4

#if _PROCESSOR_DISPATCH
__declspec(cpu_specific(Pentium_4))
#endif

// Apply the inverse horizontal transform to reconstruct a strip of rows
void InvertHorizontalStrip16s(PIXEL *lowpass_band,	// Horizontal lowpass coefficients
							  int lowpass_pitch,	// Distance between rows in bytes
							  PIXEL *highpass_band,	// Horizontal highpass coefficients
							  int highpass_pitch,	// Distance between rows in bytes
							  PIXEL *output_image,	// Row of reconstructed results
							  int output_pitch,		// Distance between rows in bytes
							  ROI roi)				// Height and width of the strip
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 8;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m128i low1_epi16;		// Lowpass coefficients
		__m128i low2_epi16;
		__m128i high1_epi16;	// Highpass coefficients
		__m128i high2_epi16;

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

#if _UNALIGNED
		// The fast loop computes output points starting at the third column
		__m128i *outptr = (__m128i *)&output[2];
#else
		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];

		// Two 16-bit coefficients from the previous loop iteration
		//short remainder[2];
#endif
		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even >>= 1;//DAN20050913 - DivideByShift(even, 1);

#if _UNALIGNED
		// Place the even result in the even column
		output[0] = SATURATE(even);
#else
		// The output value will be stored later
		//remainder[0] = SATURATE(even);
#endif
		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd >>= 1;//DAN20050913 - DivideByShift(odd, 1);

#if _UNALIGNED
		// Place the odd result in the odd column
		output[1] = SATURATE(odd);
#else
		// The output value will be stored later
		//remainder[1] = SATURATE(odd);
#endif

#if (_FASTLOOP && XMMOPT && 1)

		// Preload the first four lowpass coefficients
		low1_epi16 = _mm_load_si128((__m128i *)&lowpass[column]);
		//low1 : a,b,c,d,e,f,g,h

		// Preload the first four highpass coefficients
		high1_epi16 = _mm_load_si128((__m128i *)&highpass[column]);
		//high1 : A,B,C,D,E,F,G,H

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i out_epi16;		// Reconstructed data
			//__m128i mask_epi16;
			__m128i half_epi16;
			//__m128i lsb_epi16;
			//__m128i sign_epi16;
			__m128i high_epi16;

			uint32_t temp;		// Temporary register for last two values

			// Preload the next four lowpass coefficients
			low2_epi16 = _mm_load_si128((__m128i *)&lowpass[column+8]);


			// Compute the first two even and two odd output points //


//DAN031304 -- correct inverse filter
			half_epi16 = _mm_set1_epi16(4); //was 4 but 7 makes for more accurate rounding to prevent luma/chroma shifts -- DAN 6/2/03

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			//even = low1
			//even : a,b,c,d,e,f,g,h
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			//temp >>= 16*2
			//temp : 0,0,a,b,c,d,e,f
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			//even -= temp
			//even : a,b,c-a,d-b,e-c,f-d,g-e,h-f
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			//even += 4,4,4,4,4,4,4,4;
			//even : a+4,b+4,c-a+4,d-b+4,e-c+4,f-d+4,g-e+4,h-f+4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			//even >>= 3;
			//even : (a+4)/8,(b+4)/8,(c-a+4)/8,(d-b+4)/8,(e-c+4)/8,(f-d+4)/8,(g-e+4)/8,(h-f+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			//even += temp
			//even : (a+4)/8+0,(b+4)/8+a,(c-a+4)/8+b,(d-b+4)/8+c,(e-c+4)/8+d,(f-d+4)/8+e,(g-e+4)/8+f,(h-f+4)/8+g



			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);
			//high1 >>= 8*2; 	//128bit
			//high1 : 0,A,B,C,D,E,F,G

			// Prescale for 8bit output - DAN 4/5/02
			high_epi16 = high1_epi16;
			//high = high1
			//high " 0,A,B,C,D,E,F,G

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);
			//even += high
			//even : a/8,A+(b+8a)/8,B+(c+8b-a)/8,C+(d+8c-b)/8,D+(e+8d-c)/8,E+(f+8e-d)/8,F+(g+8f-e)/8,G+(h+8g-f)/8
			even_epi16 = _mm_srai_epi16(even_epi16, 1);
			//even >>= 1
			//even : a/16,(A+(b+8a)/8)/2,(B+(c+8b-a)/8)/2,(C+(d+8c-b)/8)/2,(D+(e+8d-c)/8)/2,(E+(f+8e-d)/8)/2,(F+(g+8f-e)/8)/2,(G+(h+8g-f)/8)/2

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			//odd = low1
			//odd : 0,0,a,b,c,d,e,f,g,h
			temp_epi16 = low1_epi16;
			//temp >>= 16*2
			//temp : a,b,c,d,e,f,g,h
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			//odd -= temp
			//odd : a,b,a-c,b-d,c-e,d-f,e-g,f-h
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			//odd += 4,4,4,4,4,4,4,4;
			//odd : a+4,b+4,a-c+4,b-d+4,c-e+4,d-f+4,e-g+4,f-h+4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			//odd >>= 3;
			//odd : (a+4)/8,(b+4)/8,(a-c+4)/8,(b-d+4)/8,(c-e+4)/8,(d-f+4)/8,(e-g+4)/8,(f-h+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			//odd += temp
			//odd : (a+4)/8+0,(b+4)/8+a,(a-c+4)/8+b,(b-d+4)/8+c,(c-e+4)/8+d,(d-f+4)/8+e,(e-g+4)/8+f,(f-h+4)/8+g



			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
			//odd -= high
			//odd : -a/8,(8a-b)/8-A,(8b+a-c)/8-B,(8c+b-d)/8-C,(8d+c-e)/8-D,(8e+d-f)/8-E,(8f+e-g)/8-F,(8g+f-h)/8-G
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);
			//odd >>= 1;
			//odd : -a/16,((8a-b)/8-A)/2,((8b+a-c)/8-B)/2,((8c+b-d)/8-C)/2,((8d+c-e)/8-D)/2,((8e+d-f)/8-E)/2,((8f+e-g)/8-F)/2,((8g+f-h)/8-G)/2

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out = ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2, ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2


			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2
			temp = _mm_cvtsi128_si32(out_epi16);
			//temp32 =  ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, ((8d+c-e)/8-D)/2, even32
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, odd16, even16

			// Store eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);




			// Compute the second two even and two odd output points //

			// Preload the highpass correction
			high2_epi16 = _mm_load_si128((__m128i *)&highpass[column+8]);

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);


			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			//even = low1
			//even : a,b,c,d,e,f,g,h
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			//temp >>= 16*2
			//temp : 0,0,a,b,c,d,e,f
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			//even -= temp
			//even : a,b,c-a,d-b,e-c,f-d,g-e,h-f
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			//even += 4,4,4,4,4,4,4,4;
			//even : a+4,b+4,c-a+4,d-b+4,e-c+4,f-d+4,g-e+4,h-f+4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			//even >>= 3;
			//even : (a+4)/8,(b+4)/8,(c-a+4)/8,(d-b+4)/8,(e-c+4)/8,(f-d+4)/8,(g-e+4)/8,(h-f+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			//even += temp
			//even : (a+4)/8+0,(b+4)/8+a,(c-a+4)/8+b,(d-b+4)/8+c,(e-c+4)/8+d,(f-d+4)/8+e,(g-e+4)/8+f,(h-f+4)/8+g



			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			high_epi16 = high1_epi16;

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			//odd = low1
			//odd : 0,0,a,b,c,d,e,f,g,h
			temp_epi16 = low1_epi16;
			//temp >>= 16*2
			//temp : a,b,c,d,e,f,g,h
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			//odd -= temp
			//odd : a,b,a-c,b-d,c-e,d-f,e-g,f-h
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			//odd += 4,4,4,4,4,4,4,4;
			//odd : a+4,b+4,a-c+4,b-d+4,c-e+4,d-f+4,e-g+4,f-h+4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			//odd >>= 3;
			//odd : (a+4)/8,(b+4)/8,(a-c+4)/8,(b-d+4)/8,(c-e+4)/8,(d-f+4)/8,(e-g+4)/8,(f-h+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			//odd += temp
			//odd : (a+4)/8+0,(b+4)/8+a,(a-c+4)/8+b,(b-d+4)/8+c,(c-e+4)/8+d,(d-f+4)/8+e,(e-g+4)/8+f,(f-h+4)/8+g



			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out_epi16 = _mm_max_epi16(out_epi16, _mm_setzero_si64());



			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);

			// Store eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);

			// The second four lowpass coefficients will be the current values
			low1_epi16 = low2_epi16;

			// The second four highpass coefficients will be the current values
			high1_epi16 = high2_epi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Get the pointer to the next output value
		colptr = (PIXEL *)outptr;

#if _UNALIGNED
		// The last two output points have already been stored
#elif 0
		// Store the last two output points produced by the loop
		*(colptr++) = remainder[0];
		*(colptr++) = remainder[1];
#else
		// Store the last two output points produced by the loop
		*(colptr++) = SATURATE(even);
		*(colptr++) = SATURATE(odd);
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column++)
		{
			short even = 0;		// Result of convolution with even filter
			short odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band

			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += 4;  //DAN20050921
			even >>= 3;
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			even >>= 1;//DAN20050913 - DivideByShift(even, 1);

			// Place the even result in the even column
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += 4;   //DAN20050921
			odd >>= 3;
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			odd >>= 1;//DAN20050913 - DivideByShift(odd, 1);

			// Place the odd result in the odd column
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even >>= 1;//DAN20050913 - DivideByShift(even, 1);

		// Place the even result in the even column
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd >>= 1;//DAN20050913 - DivideByShift(odd, 1);

		// Place the odd result in the odd column
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}
}

#endif



// Apply the inverse horizontal transform to reconstruct a strip of rows
void InvertHorizontalStrip16s10bitLimit(PIXEL *lowpass_band,	// Horizontal lowpass coefficients
							  int lowpass_pitch,	// Distance between rows in bytes
							  PIXEL *highpass_band,	// Horizontal highpass coefficients
							  int highpass_pitch,	// Distance between rows in bytes
							  PIXEL *output_image,	// Row of reconstructed results
							  int output_pitch,		// Distance between rows in bytes
							  ROI roi)				// Height and width of the strip
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 8;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m128i low1_epi16;		// Lowpass coefficients
		__m128i low2_epi16;
		__m128i high1_epi16;	// Highpass coefficients
		__m128i high2_epi16;

		__m128i overflowprotect_epi16 = _mm_set1_epi16(0x7fff-2047);

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

#if _UNALIGNED
		// The fast loop computes output points starting at the third column
		__m128i *outptr = (__m128i *)&output[2];
#else
		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];

		// Two 16-bit coefficients from the previous loop iteration
		//short remainder[2];
#endif
		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

#if _UNALIGNED
		// Place the even result in the even column
		output[0] = SATURATE(even);
#else
		// The output value will be stored later
		//remainder[0] = SATURATE(even);
#endif
		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

#if _UNALIGNED
		// Place the odd result in the odd column
		output[1] = SATURATE(odd);
#else
		// The output value will be stored later
		//remainder[1] = SATURATE(odd);
#endif

#if (_FASTLOOP && XMMOPT)

		// Preload the first four lowpass coefficients
		low1_epi16 = _mm_load_si128((__m128i *)&lowpass[column]);
		//low1 : a,b,c,d,e,f,g,h

		// Preload the first four highpass coefficients
		high1_epi16 = _mm_load_si128((__m128i *)&highpass[column]);
		//high1 : A,B,C,D,E,F,G,H

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i out_epi16;		// Reconstructed data
			//__m128i mask_epi16;
			__m128i half_epi16;
			//__m128i lsb_epi16;
			//__m128i sign_epi16;
			__m128i high_epi16;

			uint32_t temp;		// Temporary register for last two values

			// Preload the next four lowpass coefficients
			low2_epi16 = _mm_load_si128((__m128i *)&lowpass[column+8]);


			// Compute the first two even and two odd output points //


//DAN031304 -- correct inverse filter
			half_epi16 = _mm_set1_epi16(4); //was 4 but 7 makes for more accurate rounding to prevent luma/chroma shifts -- DAN 6/2/03

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			//even = low1
			//even : a,b,c,d,e,f,g,h
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			//temp >>= 16*2
			//temp : 0,0,a,b,c,d,e,f
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			//even -= temp
			//even : a,b,c-a,d-b,e-c,f-d,g-e,h-f
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			//even += 4,4,4,4,4,4,4,4;
			//even : a+4,b+4,c-a+4,d-b+4,e-c+4,f-d+4,g-e+4,h-f+4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			//even >>= 3;
			//even : (a+4)/8,(b+4)/8,(c-a+4)/8,(d-b+4)/8,(e-c+4)/8,(f-d+4)/8,(g-e+4)/8,(h-f+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			//even += temp
			//even : (a+4)/8+0,(b+4)/8+a,(c-a+4)/8+b,(d-b+4)/8+c,(e-c+4)/8+d,(f-d+4)/8+e,(g-e+4)/8+f,(h-f+4)/8+g



			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);
			//high1 >>= 8*2; 	//128bit
			//high1 : 0,A,B,C,D,E,F,G

			// Prescale for 8bit output - DAN 4/5/02
			high_epi16 = high1_epi16;
			//high = high1
			//high " 0,A,B,C,D,E,F,G

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, overflowprotect_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, overflowprotect_epi16);
			//even += high
			//even : a/8,A+(b+8a)/8,B+(c+8b-a)/8,C+(d+8c-b)/8,D+(e+8d-c)/8,E+(f+8e-d)/8,F+(g+8f-e)/8,G+(h+8g-f)/8
			even_epi16 = _mm_srai_epi16(even_epi16, 1);
			//even >>= 1
			//even : a/16,(A+(b+8a)/8)/2,(B+(c+8b-a)/8)/2,(C+(d+8c-b)/8)/2,(D+(e+8d-c)/8)/2,(E+(f+8e-d)/8)/2,(F+(g+8f-e)/8)/2,(G+(h+8g-f)/8)/2

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			//odd = low1
			//odd : 0,0,a,b,c,d,e,f,g,h
			temp_epi16 = low1_epi16;
			//temp >>= 16*2
			//temp : a,b,c,d,e,f,g,h
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			//odd -= temp
			//odd : a,b,a-c,b-d,c-e,d-f,e-g,f-h
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			//odd += 4,4,4,4,4,4,4,4;
			//odd : a+4,b+4,a-c+4,b-d+4,c-e+4,d-f+4,e-g+4,f-h+4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			//odd >>= 3;
			//odd : (a+4)/8,(b+4)/8,(a-c+4)/8,(b-d+4)/8,(c-e+4)/8,(d-f+4)/8,(e-g+4)/8,(f-h+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			//odd += temp
			//odd : (a+4)/8+0,(b+4)/8+a,(a-c+4)/8+b,(b-d+4)/8+c,(c-e+4)/8+d,(d-f+4)/8+e,(e-g+4)/8+f,(f-h+4)/8+g



			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, overflowprotect_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, overflowprotect_epi16);
			//odd -= high
			//odd : -a/8,(8a-b)/8-A,(8b+a-c)/8-B,(8c+b-d)/8-C,(8d+c-e)/8-D,(8e+d-f)/8-E,(8f+e-g)/8-F,(8g+f-h)/8-G
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);
			//odd >>= 1;
			//odd : -a/16,((8a-b)/8-A)/2,((8b+a-c)/8-B)/2,((8c+b-d)/8-C)/2,((8d+c-e)/8-D)/2,((8e+d-f)/8-E)/2,((8f+e-g)/8-F)/2,((8g+f-h)/8-G)/2

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out = ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2, ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2


			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2
			temp = _mm_cvtsi128_si32(out_epi16);
			//temp32 =  ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, ((8d+c-e)/8-D)/2, even32
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, odd16, even16

			// Store eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);




			// Compute the second two even and two odd output points //

			// Preload the highpass correction
			high2_epi16 = _mm_load_si128((__m128i *)&highpass[column+8]);

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);


			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			//even = low1
			//even : a,b,c,d,e,f,g,h
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			//temp >>= 16*2
			//temp : 0,0,a,b,c,d,e,f
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			//even -= temp
			//even : a,b,c-a,d-b,e-c,f-d,g-e,h-f
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			//even += 4,4,4,4,4,4,4,4;
			//even : a+4,b+4,c-a+4,d-b+4,e-c+4,f-d+4,g-e+4,h-f+4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			//even >>= 3;
			//even : (a+4)/8,(b+4)/8,(c-a+4)/8,(d-b+4)/8,(e-c+4)/8,(f-d+4)/8,(g-e+4)/8,(h-f+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			//even += temp
			//even : (a+4)/8+0,(b+4)/8+a,(c-a+4)/8+b,(d-b+4)/8+c,(e-c+4)/8+d,(f-d+4)/8+e,(g-e+4)/8+f,(h-f+4)/8+g



			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			high_epi16 = high1_epi16;

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, overflowprotect_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, overflowprotect_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			//odd = low1
			//odd : 0,0,a,b,c,d,e,f,g,h
			temp_epi16 = low1_epi16;
			//temp >>= 16*2
			//temp : a,b,c,d,e,f,g,h
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			//odd -= temp
			//odd : a,b,a-c,b-d,c-e,d-f,e-g,f-h
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			//odd += 4,4,4,4,4,4,4,4;
			//odd : a+4,b+4,a-c+4,b-d+4,c-e+4,d-f+4,e-g+4,f-h+4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			//odd >>= 3;
			//odd : (a+4)/8,(b+4)/8,(a-c+4)/8,(b-d+4)/8,(c-e+4)/8,(d-f+4)/8,(e-g+4)/8,(f-h+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			//odd += temp
			//odd : (a+4)/8+0,(b+4)/8+a,(a-c+4)/8+b,(b-d+4)/8+c,(c-e+4)/8+d,(d-f+4)/8+e,(e-g+4)/8+f,(f-h+4)/8+g



			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, overflowprotect_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, overflowprotect_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out_epi16 = _mm_max_epi16(out_epi16, _mm_setzero_si64());



			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);

			// Store eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);

			// The second four lowpass coefficients will be the current values
			low1_epi16 = low2_epi16;

			// The second four highpass coefficients will be the current values
			high1_epi16 = high2_epi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Get the pointer to the next output value
		colptr = (PIXEL *)outptr;

#if _UNALIGNED
		// The last two output points have already been stored
#elif 0
		// Store the last two output points produced by the loop
		*(colptr++) = remainder[0];
		*(colptr++) = remainder[1];
#else
		// Store the last two output points produced by the loop
		*(colptr++) = SATURATE(even);
		*(colptr++) = SATURATE(odd);
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column++)
		{
			short even = 0;		// Result of convolution with even filter
			short odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band

			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += 4;   //DAN20050921
			even >>= 3;
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			even = DivideByShift(even, 1);

			if(even < 0) even = 0;
			if(even > 1023) even = 1023;

			// Place the even result in the even column
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += 4;  //DAN20050921
			odd >>= 3;
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			odd = DivideByShift(odd, 1);

			if(odd < 0) odd = 0;
			if(odd > 1023) odd = 1023;

			// Place the odd result in the odd column
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		if(even < 0) even = 0;
		if(even > 1023) even = 1023;

		// Place the even result in the even column
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		if(odd < 0) odd = 0;
		if(odd > 1023) odd = 1023;

		// Place the odd result in the odd column
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}
}





#if _PROCESSOR_DISPATCH
__declspec(cpu_dispatch(Pentium_4,Generic))

void InvertHorizontalStripDescale16s(PIXEL *lowpass_band, int lowpass_pitch,
									 PIXEL *highpass_band, int highpass_pitch,
									 PIXEL *output_image, int output_pitch,
									 ROI roi, int descale)
{
	// Stub routine for processor specific dispatch
}
#endif


#if _PROCESSOR_GENERIC
#if _PROCESSOR_DISPATCH
__declspec(cpu_specific(Generic))
#endif

void InvertHorizontalStripDescale16s(PIXEL *lowpass_band, int lowpass_pitch,
									 PIXEL *highpass_band, int highpass_pitch,
									 PIXEL *output_image, int output_pitch,
									 ROI roi, int descale)
{
//	int InvertHorizontalStripDescale16s_MMX_not_done =  1;
//	assert(InvertHorizontalStripDescale16s_MMX_not_done);

	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 4;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;
	int descaleshift = 0;

	// The algorithm incorporates descaling by a factor of two
	if(descale == 2)
		descaleshift = 1;

	// Check that the descaling value is reasonable
	assert(descaleshift >= 0);

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m64 low1_pi16;	// Lowpass coefficients
		__m64 low2_pi16;
		__m64 high1_pi16;	// Highpass coefficients
		__m64 high2_pi16;

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

		// The fast loop computes output points starting at the third column
		__m64 *outptr = (__m64 *)&output[2];

		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		//even = DivideByShift(even, 1);
		even <<= descaleshift;
		// Place the even result in the even column
		output[0] = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		//odd = DivideByShift(odd, 1);
		odd <<= descaleshift;
		// Place the odd result in the odd column
		output[1] = SATURATE(odd);

#if (1 && XMMOPT)

		// Preload the first four lowpass coefficients
		low1_pi16 = *((__m64 *)&lowpass[column]);

		// Preload the first four highpass coefficients
		high1_pi16 = *((__m64 *)&highpass[column]);

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m64 even_pi16;	// Result of convolution with even filter
			__m64 odd_pi16;		// Result of convolution with odd filter
			__m64 temp_pi16;
			//__m64 temp2_pi16;
			__m64 out_pi16;		// Reconstructed data
			__m64 mask_pi16;
			__m64 half_pi16;
			__m64 lsb_pi16;
			__m64 sign_pi16;
			__m64 high_pi16;


			half_pi16 = _mm_set1_pi16(4);


			// Preload the next four lowpass coefficients
			low2_pi16 = *((__m64 *)&lowpass[column+4]);


			// Compute the first two even and two odd output points //
			even_pi16 = low1_pi16;//+1x
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); //-1x
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16); //+1a -1c
			even_pi16 = _mm_adds_pi16(even_pi16, half_pi16); // rounding
			even_pi16 = _mm_srai_pi16(even_pi16, 3);  // (+1a -1c) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(0, 3, 2, 1)); //+8x
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); // (+1a +8b -1c) >> 3


			// Shift the highpass correction by one column
			high1_pi16 = _mm_srli_si64(high1_pi16, 16);

			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			//even_pi16 = _mm_srai_pi16(even_pi16, 1);

			odd_pi16 = low1_pi16;//-1x
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); //+1x
			odd_pi16 = _mm_subs_pi16(temp_pi16, odd_pi16); //-1a +1c
			odd_pi16 = _mm_adds_pi16(odd_pi16, half_pi16); // rounding
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);  // (-1a +1c) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(0, 3, 2, 1)); //+8x
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16); // (-1a +8b +1c) >> 3

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			//odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());

			//descale
			out_pi16 = _mm_slli_pi16(out_pi16, descaleshift);

			*(outptr++) = out_pi16;


			// Compute the second two even and two odd output points //

			// Preload the highpass correction
			high2_pi16 = *((__m64 *)&highpass[column+4]);

			// Shift in the new pixels for the next stage of the loop
			low1_pi16 = _mm_srli_si64(low1_pi16, 32);
			temp_pi16 = _mm_slli_si64(low2_pi16, 32);
			low1_pi16 = _mm_or_si64(low1_pi16, temp_pi16);

			even_pi16 = low1_pi16;//+1x
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); //-1x
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16); //+1a -1c
			even_pi16 = _mm_adds_pi16(even_pi16, half_pi16); // rounding
			even_pi16 = _mm_srai_pi16(even_pi16, 3);  // (+1a -1c) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(0, 3, 2, 1)); //+8x
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); // (+1a +8b -1c) >> 3

			// Shift in the next two highpass coefficients
			high1_pi16 = _mm_srli_si64(high1_pi16, 2*16);
			high1_pi16 = _mm_or_si64(high1_pi16, _mm_slli_si64(high2_pi16, 16));

			// Prescale for 8bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			//even_pi16 = _mm_srai_pi16(even_pi16, 1);

			odd_pi16 = low1_pi16;//-1x
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); //+1x
			odd_pi16 = _mm_subs_pi16(temp_pi16, odd_pi16); //-1a +1c
			odd_pi16 = _mm_adds_pi16(odd_pi16, half_pi16); // rounding
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);  // (-1a +1c) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(0, 3, 2, 1)); //+8x
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16); // (-1a +8b +1c) >> 3


			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			//odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());

			//descale
			out_pi16 = _mm_slli_pi16(out_pi16, descaleshift);

			*(outptr++) = out_pi16;

			// The second four lowpass coefficients will be the current values
			low1_pi16 = low2_pi16;

			// The second four highpass coefficients will be the current values
			high1_pi16 = high2_pi16;
		}


		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Get the pointer to the next output value
		colptr = (PIXEL *)outptr;

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column++)
		{
			short even = 0;		// Result of convolution with even filter
			short odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += 4;  //DAN20050921
			even >>= 3;
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			//even = DivideByShift(even, 1);

			// Remove any scaling used during encoding
			even <<= descaleshift;

			// Place the even result in the even column
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += 4;  //DAN20050921
			odd >>= 3;
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			//odd = DivideByShift(odd, 1);

			// Remove any scaling used during encoding
			odd <<= descaleshift;

			// Place the odd result in the odd column
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		//even = DivideByShift(even, 1);

		// Remove any scaling used during encoding
		even <<= descaleshift;

		// Place the even result in the even column
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		//odd = DivideByShift(odd, 1);

		// Remove any scaling used during encoding
		odd <<= descaleshift;

		// Place the odd result in the odd column
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}

	//_mm_empty();	// Clear the mmx register state
}
#endif // _PROCESSOR_GENERIC


#if _PROCESSOR_PENTIUM_4
#if _PROCESSOR_DISPATCH
__declspec(cpu_specific(Pentium_4))
#endif

// Apply the inverse horizontal transform to reconstruct a strip of rows
void InvertHorizontalStripDescale16s(PIXEL *lowpass_band, int lowpass_pitch,
									 PIXEL *highpass_band, int highpass_pitch,
									 PIXEL *output_image, int output_pitch,
									 ROI roi, int descale)
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 8;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;
	int descaleshift = 0;

	// The algorithm incorporates descaling by a factor of two
	if(descale == 2)
		descaleshift = 1;

	// Check that the descaling value is reasonable
	assert(descaleshift >= 0);

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m128i low1_epi16;		// Lowpass coefficients
		__m128i low2_epi16;
		__m128i high1_epi16;	// Highpass coefficients
		__m128i high2_epi16;

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

#if _UNALIGNED
		// The fast loop computes output points starting at the third column
		__m128i *outptr = (__m128i *)&output[2];
#else
		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];

		// Two 16-bit coefficients from the previous loop iteration
		//short remainder[2];
#endif
		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		//even = DivideByShift(even, 1);

		// Remove any scaling used during encoding
		//even <<= descaleshift;

#if _UNALIGNED
		// Place the even result in the even column
		output[0] = SATURATE(even);
#else
		// The output value will be stored later
		//remainder[0] = SATURATE(even);
#endif
		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		//odd = DivideByShift(odd, 1);

		// Remove any scaling used during encoding
		//odd <<= descaleshift;

#if _UNALIGNED
		// Place the odd result in the odd column
		output[1] = SATURATE(odd);
#else
		// The output value will be stored later
		//remainder[1] = SATURATE(odd);
#endif

#if (_FASTLOOP && XMMOPT)

		// Preload the first four lowpass coefficients
		low1_epi16 = _mm_load_si128((__m128i *)&lowpass[column]);
		//low1 : a,b,c,d,e,f,g,h

		// Preload the first four highpass coefficients
		high1_epi16 = _mm_load_si128((__m128i *)&highpass[column]);
		//high1 : A,B,C,D,E,F,G,H

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i out_epi16;		// Reconstructed data
			//__m128i mask_epi16;
			__m128i half_epi16;
			//__m128i lsb_epi16;
			//__m128i sign_epi16;
			__m128i high_epi16;

		//	__m128i descale_si128 = _mm_cvtsi32_si128(descaleshift);

			uint32_t temp;		// Temporary register for last two values

			// Preload the next four lowpass coefficients
			low2_epi16 = _mm_load_si128((__m128i *)&lowpass[column+8]);


			// ***** Compute the first two even and two odd output points ****


			//was 4 but 7 makes for more accurate rounding to prevent luma/chroma shifts -- DAN 6/2/03

//DAN031304 -- correct inverse filter
			half_epi16 = _mm_set1_epi16(4);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			//even = low1
			//even : a,b,c,d,e,f,g,h
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			//temp >>= 16*2
			//temp : 0,0,a,b,c,d,e,f
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			//even -= temp
			//even : a,b,c-a,d-b,e-c,f-d,g-e,h-f
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			//even += 4,4,4,4,4,4,4,4;
			//even : a+4,b+4,c-a+4,d-b+4,e-c+4,f-d+4,g-e+4,h-f+4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			//even >>= 3;
			//even : (a+4)/8,(b+4)/8,(c-a+4)/8,(d-b+4)/8,(e-c+4)/8,(f-d+4)/8,(g-e+4)/8,(h-f+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			//even += temp
			//even : (a+4)/8+0,(b+4)/8+a,(c-a+4)/8+b,(d-b+4)/8+c,(e-c+4)/8+d,(f-d+4)/8+e,(g-e+4)/8+f,(h-f+4)/8+g


			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);
			//high1 >>= 8*2; 	//128bit
			//high1 : 0,A,B,C,D,E,F,G

			// Prescale for 8bit output - DAN 4/5/02
			high_epi16 = high1_epi16;
			//high = high1
			//high " 0,A,B,C,D,E,F,G

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);
			//even += high
			//even : a/8,A+(b+8a)/8,B+(c+8b-a)/8,C+(d+8c-b)/8,D+(e+8d-c)/8,E+(f+8e-d)/8,F+(g+8f-e)/8,G+(h+8g-f)/8
			//even_epi16 = _mm_srai_epi16(even_epi16, 1);
			//even >>= 1
			//even : a/16,(A+(b+8a)/8)/2,(B+(c+8b-a)/8)/2,(C+(d+8c-b)/8)/2,(D+(e+8d-c)/8)/2,(E+(f+8e-d)/8)/2,(F+(g+8f-e)/8)/2,(G+(h+8g-f)/8)/2

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			//odd = low1
			//odd : 0,0,a,b,c,d,e,f,g,h
			temp_epi16 = low1_epi16;
			//temp >>= 16*2
			//temp : a,b,c,d,e,f,g,h
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			//odd -= temp
			//odd : a,b,a-c,b-d,c-e,d-f,e-g,f-h
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			//odd += 4,4,4,4,4,4,4,4;
			//odd : a+4,b+4,a-c+4,b-d+4,c-e+4,d-f+4,e-g+4,f-h+4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			//odd >>= 3;
			//odd : (a+4)/8,(b+4)/8,(a-c+4)/8,(b-d+4)/8,(c-e+4)/8,(d-f+4)/8,(e-g+4)/8,(f-h+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			//odd += temp
			//odd : (a+4)/8+0,(b+4)/8+a,(a-c+4)/8+b,(b-d+4)/8+c,(c-e+4)/8+d,(d-f+4)/8+e,(e-g+4)/8+f,(f-h+4)/8+g


			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
			//odd -= high
			//odd : -a/8,(8a-b)/8-A,(8b+a-c)/8-B,(8c+b-d)/8-C,(8d+c-e)/8-D,(8e+d-f)/8-E,(8f+e-g)/8-F,(8g+f-h)/8-G
			//odd_epi16 = _mm_srai_epi16(odd_epi16, 1);
			//odd >>= 1;
			//odd : -a/16,((8a-b)/8-A)/2,((8b+a-c)/8-B)/2,((8c+b-d)/8-C)/2,((8d+c-e)/8-D)/2,((8e+d-f)/8-E)/2,((8f+e-g)/8-F)/2,((8g+f-h)/8-G)/2

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out = ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2, ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2


			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2
			temp = _mm_cvtsi128_si32(out_epi16);
			//temp32 =  ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, ((8d+c-e)/8-D)/2, even32
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, odd16, even16

			// Remove any scaling used during encoding
		//	out_epi16 = _mm_sll_epi16(out_epi16, descale_si128);
			out_epi16 = _mm_adds_epi16(out_epi16, out_epi16);

			// Store eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);


			// ***** Compute the second two even and two odd output points *****

			// Preload the highpass correction
			high2_epi16 = _mm_load_si128((__m128i *)&highpass[column+8]);

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);


			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			//even = low1
			//even : a,b,c,d,e,f,g,h
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			//temp >>= 16*2
			//temp : 0,0,a,b,c,d,e,f
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			//even -= temp
			//even : a,b,c-a,d-b,e-c,f-d,g-e,h-f
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			//even += 4,4,4,4,4,4,4,4;
			//even : a+4,b+4,c-a+4,d-b+4,e-c+4,f-d+4,g-e+4,h-f+4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			//even >>= 3;
			//even : (a+4)/8,(b+4)/8,(c-a+4)/8,(d-b+4)/8,(e-c+4)/8,(f-d+4)/8,(g-e+4)/8,(h-f+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			//even += temp
			//even : (a+4)/8+0,(b+4)/8+a,(c-a+4)/8+b,(d-b+4)/8+c,(e-c+4)/8+d,(f-d+4)/8+e,(g-e+4)/8+f,(h-f+4)/8+g


			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			high_epi16 = high1_epi16;

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);
			//even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			//odd = low1
			//odd : 0,0,a,b,c,d,e,f,g,h
			temp_epi16 = low1_epi16;
			//temp >>= 16*2
			//temp : a,b,c,d,e,f,g,h
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			//odd -= temp
			//odd : a,b,a-c,b-d,c-e,d-f,e-g,f-h
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			//odd += 4,4,4,4,4,4,4,4;
			//odd : a+4,b+4,a-c+4,b-d+4,c-e+4,d-f+4,e-g+4,f-h+4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			//odd >>= 3;
			//odd : (a+4)/8,(b+4)/8,(a-c+4)/8,(b-d+4)/8,(c-e+4)/8,(d-f+4)/8,(e-g+4)/8,(f-h+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			//odd += temp
			//odd : (a+4)/8+0,(b+4)/8+a,(a-c+4)/8+b,(b-d+4)/8+c,(c-e+4)/8+d,(d-f+4)/8+e,(e-g+4)/8+f,(f-h+4)/8+g


			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
			//odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out_epi16 = _mm_max_epi16(out_epi16, _mm_setzero_si64());


			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);

			// Remove any scaling used during encoding
		//	out_epi16 = _mm_sll_epi16(out_epi16, descale_si128);
			out_epi16 = _mm_adds_epi16(out_epi16, out_epi16);

			// Store eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);

			// The second four lowpass coefficients will be the current values
			low1_epi16 = low2_epi16;

			// The second four highpass coefficients will be the current values
			high1_epi16 = high2_epi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Get the pointer to the next output value
		colptr = (PIXEL *)outptr;

#if _UNALIGNED
		// The last two output points have already been stored
#elif 0
		// Store the last two output points produced by the loop
		*(colptr++) = remainder[0];
		*(colptr++) = remainder[1];
#else
		// Store the last two output points produced by the loop
		even <<= descaleshift;
		odd <<= descaleshift;

		*(colptr++) = SATURATE(even);
		*(colptr++) = SATURATE(odd);
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column++)
		{
			int even = 0;		// Result of convolution with even filter
			int odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += 4;  //DAN20050921
			even >>= 3;
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			//even = DivideByShift(even, 1);

			// Remove any scaling used during encoding
			even <<= descaleshift;

			// Place the even result in the even column
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += 4;  //DAN20050921
			odd >>= 3;
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			//odd = DivideByShift(odd, 1);

			// Remove any scaling used during encoding
			odd <<= descaleshift;

			// Place the odd result in the odd column
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		//even = DivideByShift(even, 1);

		// Remove any scaling used during encoding
		even <<= descaleshift;

		// Place the even result in the even column
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		//odd = DivideByShift(odd, 1);

		// Remove any scaling used during encoding
		odd <<= descaleshift;

		// Place the odd result in the odd column
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}
}
#endif //P4



#if _PROCESSOR_DISPATCH
__declspec(cpu_dispatch(Pentium_4,Generic))

void InvertHorizontalStrip1x16s(PIXEL *lowpass_band, int lowpass_pitch,
								PIXEL *highpass_band, int highpass_pitch,
								PIXEL *output_image, int output_pitch,
								ROI roi)
{
	// Stub routine for processor specific dispatch
}
#endif


#if _PROCESSOR_GENERIC
#if _PROCESSOR_DISPATCH
__declspec(cpu_specific(Generic))
#endif

void InvertHorizontalStrip1x16s(PIXEL *lowpass_band, int lowpass_pitch,
								PIXEL *highpass_band, int highpass_pitch,
								PIXEL *output_image, int output_pitch,
								ROI roi)
{
	int InvertHorizontalStrip1x16s_MMX_not_done =  1;
	assert(InvertHorizontalStrip1x16s_MMX_not_done);
}
#endif // _PROCESSOR_GENERIC


#if _PROCESSOR_PENTIUM_4
#if _PROCESSOR_DISPATCH
__declspec(cpu_specific(Pentium_4))
#endif

// Apply the inverse horizontal transform to reconstruct a strip of rows
void InvertHorizontalStrip1x16s(PIXEL *lowpass_band, int lowpass_pitch,
								PIXEL *highpass_band, int highpass_pitch,
								PIXEL *output_image, int output_pitch,
								ROI roi)
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 8;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m128i low1_epi16;		// Lowpass coefficients
		__m128i low2_epi16;
		__m128i high1_epi16;	// Highpass coefficients
		__m128i high2_epi16;

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

#if _UNALIGNED
		// The fast loop computes output points starting at the third column
		__m128i *outptr = (__m128i *)&output[2];
#else
		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];

		// Two 16-bit coefficients from the previous loop iteration
		//short remainder[2];
#endif
		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		//even = DivideByShift(even, 1);

#if _UNALIGNED
		// Place the even result in the even column
		output[0] = SATURATE(even);
#else
		// The output value will be stored later
		//remainder[0] = SATURATE(even);
#endif
		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		//odd = DivideByShift(odd, 1);

#if _UNALIGNED
		// Place the odd result in the odd column
		output[1] = SATURATE(odd);
#else
		// The output value will be stored later
		//remainder[1] = SATURATE(odd);
#endif

#if (_FASTLOOP && XMMOPT)

		// Preload the first four lowpass coefficients
		low1_epi16 = _mm_load_si128((__m128i *)&lowpass[column]);
		//low1 : a,b,c,d,e,f,g,h

		// Preload the first four highpass coefficients
		high1_epi16 = _mm_load_si128((__m128i *)&highpass[column]);
		//high1 : A,B,C,D,E,F,G,H

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i out_epi16;		// Reconstructed data
			//__m128i mask_epi16;
			__m128i half_epi16;
			//__m128i lsb_epi16;
			//__m128i sign_epi16;
			__m128i high_epi16;

			uint32_t temp;		// Temporary register for last two values

			// Preload the next four lowpass coefficients
			low2_epi16 = _mm_load_si128((__m128i *)&lowpass[column+8]);


			// Compute the first two even and two odd output points //


			half_epi16 = _mm_set1_epi16(4); //was 4 but 7 makes for more accurate rounding to prevent luma/chroma shifts -- DAN 6/2/03

//DAN031304 -- correct inverse filter
			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			//even = low1
			//even : a,b,c,d,e,f,g,h
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			//temp >>= 16*2
			//temp : 0,0,a,b,c,d,e,f
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			//even -= temp
			//even : a,b,c-a,d-b,e-c,f-d,g-e,h-f
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			//even += 4,4,4,4,4,4,4,4;
			//even : a+4,b+4,c-a+4,d-b+4,e-c+4,f-d+4,g-e+4,h-f+4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			//even >>= 3;
			//even : (a+4)/8,(b+4)/8,(c-a+4)/8,(d-b+4)/8,(e-c+4)/8,(f-d+4)/8,(g-e+4)/8,(h-f+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			//even += temp
			//even : (a+4)/8+0,(b+4)/8+a,(c-a+4)/8+b,(d-b+4)/8+c,(e-c+4)/8+d,(f-d+4)/8+e,(g-e+4)/8+f,(h-f+4)/8+g



			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);
			//high1 >>= 8*2; 	//128bit
			//high1 : 0,A,B,C,D,E,F,G

			// Prescale for 8bit output - DAN 4/5/02
			high_epi16 = high1_epi16;
			//high = high1
			//high " 0,A,B,C,D,E,F,G

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);
			//even += high
			//even : a/8,A+(b+8a)/8,B+(c+8b-a)/8,C+(d+8c-b)/8,D+(e+8d-c)/8,E+(f+8e-d)/8,F+(g+8f-e)/8,G+(h+8g-f)/8
			//even_epi16 = _mm_srai_epi16(even_epi16, 1);
			//even >>= 1
			//even : a/16,(A+(b+8a)/8)/2,(B+(c+8b-a)/8)/2,(C+(d+8c-b)/8)/2,(D+(e+8d-c)/8)/2,(E+(f+8e-d)/8)/2,(F+(g+8f-e)/8)/2,(G+(h+8g-f)/8)/2

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			//odd = low1
			//odd : 0,0,a,b,c,d,e,f,g,h
			temp_epi16 = low1_epi16;
			//temp >>= 16*2
			//temp : a,b,c,d,e,f,g,h
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			//odd -= temp
			//odd : a,b,a-c,b-d,c-e,d-f,e-g,f-h
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			//odd += 4,4,4,4,4,4,4,4;
			//odd : a+4,b+4,a-c+4,b-d+4,c-e+4,d-f+4,e-g+4,f-h+4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			//odd >>= 3;
			//odd : (a+4)/8,(b+4)/8,(a-c+4)/8,(b-d+4)/8,(c-e+4)/8,(d-f+4)/8,(e-g+4)/8,(f-h+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			//odd += temp
			//odd : (a+4)/8+0,(b+4)/8+a,(a-c+4)/8+b,(b-d+4)/8+c,(c-e+4)/8+d,(d-f+4)/8+e,(e-g+4)/8+f,(f-h+4)/8+g



			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
			//odd -= high
			//odd : -a/8,(8a-b)/8-A,(8b+a-c)/8-B,(8c+b-d)/8-C,(8d+c-e)/8-D,(8e+d-f)/8-E,(8f+e-g)/8-F,(8g+f-h)/8-G
			//odd_epi16 = _mm_srai_epi16(odd_epi16, 1);
			//odd >>= 1;
			//odd : -a/16,((8a-b)/8-A)/2,((8b+a-c)/8-B)/2,((8c+b-d)/8-C)/2,((8d+c-e)/8-D)/2,((8e+d-f)/8-E)/2,((8f+e-g)/8-F)/2,((8g+f-h)/8-G)/2

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out = ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2, ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2


			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2
			temp = _mm_cvtsi128_si32(out_epi16);
			//temp32 =  ((8d+c-e)/8-D)/2,(D+(e+8d-c)/8)/2
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, ((8d+c-e)/8-D)/2, even32
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);
			//out : ((8e+d-f)/8-E)/2,(E+(f+8e-d)/8)/2, ((8f+e-g)/8-F)/2,(F+(g+8f-e)/8)/2, ((8g+f-h)/8-G)/2,(G+(h+8g-f)/8)/2, odd16, even16

			// Store eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);




			// Compute the second two even and two odd output points //

			// Preload the highpass correction
			high2_epi16 = _mm_load_si128((__m128i *)&highpass[column+8]);

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);


			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			//even = low1
			//even : a,b,c,d,e,f,g,h
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			//temp >>= 16*2
			//temp : 0,0,a,b,c,d,e,f
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			//even -= temp
			//even : a,b,c-a,d-b,e-c,f-d,g-e,h-f
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			//even += 4,4,4,4,4,4,4,4;
			//even : a+4,b+4,c-a+4,d-b+4,e-c+4,f-d+4,g-e+4,h-f+4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			//even >>= 3;
			//even : (a+4)/8,(b+4)/8,(c-a+4)/8,(d-b+4)/8,(e-c+4)/8,(f-d+4)/8,(g-e+4)/8,(h-f+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			//even += temp
			//even : (a+4)/8+0,(b+4)/8+a,(c-a+4)/8+b,(d-b+4)/8+c,(e-c+4)/8+d,(f-d+4)/8+e,(g-e+4)/8+f,(h-f+4)/8+g



			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			high_epi16 = high1_epi16;

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);
			//even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			//odd = low1
			//odd : 0,0,a,b,c,d,e,f,g,h
			temp_epi16 = low1_epi16;
			//temp >>= 16*2
			//temp : a,b,c,d,e,f,g,h
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			//odd -= temp
			//odd : a,b,a-c,b-d,c-e,d-f,e-g,f-h
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			//odd += 4,4,4,4,4,4,4,4;
			//odd : a+4,b+4,a-c+4,b-d+4,c-e+4,d-f+4,e-g+4,f-h+4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			//odd >>= 3;
			//odd : (a+4)/8,(b+4)/8,(a-c+4)/8,(b-d+4)/8,(c-e+4)/8,(d-f+4)/8,(e-g+4)/8,(f-h+4)/8
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			//temp = low1 >> 16
			//temp : 0,a,b,c,d,e,f,g
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			//odd += temp
			//odd : (a+4)/8+0,(b+4)/8+a,(a-c+4)/8+b,(b-d+4)/8+c,(c-e+4)/8+d,(d-f+4)/8+e,(e-g+4)/8+f,(f-h+4)/8+g



			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
			//odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out_epi16 = _mm_max_epi16(out_epi16, _mm_setzero_si64());



			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);

			// Store eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);

			// The second four lowpass coefficients will be the current values
			low1_epi16 = low2_epi16;

			// The second four highpass coefficients will be the current values
			high1_epi16 = high2_epi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Get the pointer to the next output value
		colptr = (PIXEL *)outptr;

#if _UNALIGNED
		// The last two output points have already been stored
#elif 0
		// Store the last two output points produced by the loop
		*(colptr++) = remainder[0];
		*(colptr++) = remainder[1];
#else
		// Store the last two output points produced by the loop
		*(colptr++) = SATURATE(even);
		*(colptr++) = SATURATE(odd);
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column++)
		{
			short even = 0;		// Result of convolution with even filter
			short odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += 4;   //DAN20050921
			even >>= 3;
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			//even = DivideByShift(even, 1);

			// Place the even result in the even column
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += 4;  //DAN20050921
			odd >>= 3;
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			//odd = DivideByShift(odd, 1);

			// Place the odd result in the odd column
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		//even = DivideByShift(even, 1);

		// Place the even result in the even column
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		//odd = DivideByShift(odd, 1);

		// Place the odd result in the odd column
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}
}
#endif //P4

#if 0
// Apply the inverse horizontal transform to reconstruct a strip of prescaled rows
void InvertHorizontalStripScaled16s(PIXEL *lowpass_band,	// Horizontal lowpass coefficients
									int lowpass_pitch,		// Distance between rows in bytes
									PIXEL *highpass_band,	// Horizontal highpass coefficients
									int highpass_pitch,		// Distance between rows in bytes
									PIXEL *output_image,	// Row of reconstructed results
									int output_pitch,		// Distance between rows in bytes
									ROI roi)				// Height and width of the strip
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 4;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m64 low1_pi16;	// Lowpass coefficients
		__m64 low2_pi16;
		__m64 high1_pi16;	// Highpass coefficients
		__m64 high2_pi16;

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

		// The fast loop computes output points starting at the third column
		__m64 *outptr = (__m64 *)&output[2];

		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Place the even result in the even column
		even <<= V210_HORIZONTAL_SHIFT;
		output[0] = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Place the odd result in the odd column
		odd <<= V210_HORIZONTAL_SHIFT;
		output[1] = SATURATE(odd);

#if (0 && XMMOPT)

		// Preload the first four lowpass coefficients
		low1_pi16 = *((__m64 *)&lowpass[column]);

		// Preload the first four highpass coefficients
		high1_pi16 = *((__m64 *)&highpass[column]);

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m64 even_pi16;	// Result of convolution with even filter
			__m64 odd_pi16;		// Result of convolution with odd filter
			__m64 temp_pi16;
			__m64 out_pi16;		// Reconstructed data
			__m64 mask_pi16;
			__m64 half_pi16;
			__m64 lsb_pi16;
			__m64 sign_pi16;
			__m64 high_pi16;

			// Preload the next four lowpass coefficients
			low2_pi16 = *((__m64 *)&lowpass[column+4]);


			/***** Compute the first two even and two odd output points *****/

			// Apply the even reconstruction filter to the lowpass band
			even_pi16 = low1_pi16;
//DAN031304 -- wrong inverse filter
			assert(0); //fix filter
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);

			// Apply the rounding adjustment
			even_pi16 = _mm_adds_pi16(even_pi16, _mm_set1_pi16(4));
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);
			//even_pi16 = _mm_srli_pi16(even_pi16, 3);

			// Shift the highpass correction by one column
			high1_pi16 = _mm_srli_si64(high1_pi16, 16);

			// Prescale for 8bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			even_pi16 = _mm_srai_pi16(even_pi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);

			// Apply the rounding adjustment
			odd_pi16 = _mm_adds_pi16(odd_pi16, _mm_set1_pi16(4));
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);
			//odd_pi16 = _mm_srli_pi16(odd_pi16, 3);

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());

			// Scale the results
			out_pi16 = _mm_slli_pi16(out_pi16, V210_HORIZONTAL_SHIFT);

			// Store the results
			*(outptr++) = out_pi16;


			/***** Compute the second two even and two odd output points *****/

			// Preload the highpass correction
			high2_pi16 = *((__m64 *)&highpass[column+4]);

			// Shift in the new pixels for the next stage of the loop
			low1_pi16 = _mm_srli_si64(low1_pi16, 32);
			temp_pi16 = _mm_slli_si64(low2_pi16, 32);
			low1_pi16 = _mm_or_si64(low1_pi16, temp_pi16);

			// Apply the even reconstruction filter to the lowpass band
			even_pi16 = low1_pi16;
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);

			// Apply the rounding adjustment
			even_pi16 = _mm_adds_pi16(even_pi16, _mm_set1_pi16(4));
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);
			//even_pi16 = _mm_srli_pi16(even_pi16, 3);

			// Shift in the next two highpass coefficients
			high1_pi16 = _mm_srli_si64(high1_pi16, 2*16);
			high1_pi16 = _mm_or_si64(high1_pi16, _mm_slli_si64(high2_pi16, 16));

			// Prescale for 8-bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			even_pi16 = _mm_srai_pi16(even_pi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);

			// Apply the rounding adjustment
			odd_pi16 = _mm_adds_pi16(odd_pi16, _mm_set1_pi16(4));
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);
			//odd_pi16 = _mm_srli_pi16(odd_pi16, 3);

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());

			// Scale the results
			out_pi16 = _mm_slli_pi16(out_pi16, V210_HORIZONTAL_SHIFT);

			// Store the results
			*(outptr++) = out_pi16;

			// The second four lowpass coefficients will be the current values
			low1_pi16 = low2_pi16;

			// The second four highpass coefficients will be the current values
			high1_pi16 = high2_pi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Process the rest of the columns up to the last column in the row
		colptr = (PIXEL *)outptr;

		for (; column < last_column; column++)
		{
			int32_t even = 0;		// Result of convolution with even filter
			int32_t odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += ROUNDING(even,8);
			even = DivideByShift(even, 3);
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			even = DivideByShift(even, 1);

			// Place the even result in the even column
			even <<= V210_HORIZONTAL_SHIFT;
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += ROUNDING(odd,8);
			odd = DivideByShift(odd, 3);
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			odd = DivideByShift(odd, 1);

			// Place the odd result in the odd column
			odd <<= V210_HORIZONTAL_SHIFT;
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Place the even result in the even column
		even <<= V210_HORIZONTAL_SHIFT;
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Place the odd result in the odd column
		odd <<= V210_HORIZONTAL_SHIFT;
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}

	//_mm_empty();	// Clear the mmx register state
}
#endif

#if _PROCESSOR_DISPATCH

__declspec(cpu_dispatch(Pentium_4,Generic))

void InvertHorizontalStripPrescaled16s(PIXEL *lowpass_band,		// Horizontal lowpass coefficients
									   int lowpass_pitch,		// Distance between rows in bytes
									   PIXEL *highpass_band,	// Horizontal highpass coefficients
									   int highpass_pitch,		// Distance between rows in bytes
									   PIXEL *output_image,		// Row of reconstructed results
									   int output_pitch,		// Distance between rows in bytes
									   ROI roi)					// Height and width of the strip
{
	// Stub routine for processor specific dispatch
}

#endif


#if _PROCESSOR_GENERIC

#if _PROCESSOR_DISPATCH
__declspec(cpu_specific(Generic))
#endif

// Apply the inverse horizontal transform to reconstruct a strip of prescaled rows
void InvertHorizontalStripPrescaled16s(PIXEL *lowpass_band,		// Horizontal lowpass coefficients
									   int lowpass_pitch,		// Distance between rows in bytes
									   PIXEL *highpass_band,	// Horizontal highpass coefficients
									   int highpass_pitch,		// Distance between rows in bytes
									   PIXEL *output_image,		// Row of reconstructed results
									   int output_pitch,		// Distance between rows in bytes
									   ROI roi)					// Height and width of the strip
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 4;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m64 low1_pi16;	// Lowpass coefficients
		__m64 low2_pi16;
		__m64 high1_pi16;	// Highpass coefficients
		__m64 high2_pi16;

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

		// The fast loop computes output points starting at the third column
		__m64 *outptr = (__m64 *)&output[2];

		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];

#if (_LOWPASS_PRESCALE == 0)
		even = DivideByShift(even, 1);
#elif (_LOWPASS_PRESCALE > 1)
		even <<= (_LOWPASS_PRESCALE - 1);
#endif
		// Place the even result in the even column
		output[0] = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];

#if (_LOWPASS_PRESCALE == 0)
		odd = DivideByShift(odd, 1);
#elif (_LOWPASS_PRESCALE > 1)
		odd <<= (_LOWPASS_PRESCALE - 1);
#endif
		// Place the odd result in the odd column
		output[1] = SATURATE(odd);

#if (1 && XMMOPT)

		// Preload the first four lowpass coefficients
		low1_pi16 = *((__m64 *)&lowpass[column]);

		// Preload the first four highpass coefficients
		high1_pi16 = *((__m64 *)&highpass[column]);

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m64 even_pi16;	// Result of convolution with even filter
			__m64 odd_pi16;		// Result of convolution with odd filter
			__m64 temp_pi16;
			__m64 out_pi16;		// Reconstructed data
			__m64 mask_pi16;
			__m64 half_pi16;
			__m64 lsb_pi16;
			__m64 sign_pi16;
			__m64 high_pi16;

			// Preload the next four lowpass coefficients
			low2_pi16 = *((__m64 *)&lowpass[column+4]);


			// Compute the first two even and two odd output points //

			// Apply the even reconstruction filter to the lowpass band
//DAN031304 -- wrong inverse filter
			assert(0); //fix filter
			even_pi16 = low1_pi16;
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);

			// Apply the rounding adjustment
			even_pi16 = _mm_adds_pi16(even_pi16, _mm_set1_pi16(4));
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);

			// Shift the highpass correction by one column
			high1_pi16 = _mm_srli_si64(high1_pi16, 16);

			// Prescale for 8bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);

#if (_LOWPASS_PRESCALE == 0)
			even_pi16 = _mm_srai_pi16(even_pi16, 1);
#elif (_LOWPASS_PRESCALE > 1)
			even_pi16 = _mm_slli_pi16(even_pi16, (_LOWPASS_PRESCALE - 1));
#endif
			// Apply the odd reconstruction filter to the lowpass band
			odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);

			// Apply the rounding adjustment
			odd_pi16 = _mm_adds_pi16(odd_pi16, _mm_set1_pi16(4));
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);

#if (_LOWPASS_PRESCALE == 0)
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);
#elif (_LOWPASS_PRESCALE > 1)
			odd_pi16 = _mm_slli_pi16(odd_pi16, (_LOWPASS_PRESCALE - 1));
#endif
			// Interleave the even and odd results
			out_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());
			*(outptr++) = out_pi16;


			// Compute the second two even and two odd output points //

			// Preload the highpass correction
			high2_pi16 = *((__m64 *)&highpass[column+4]);

			// Shift in the new pixels for the next stage of the loop
			low1_pi16 = _mm_srli_si64(low1_pi16, 32);
			temp_pi16 = _mm_slli_si64(low2_pi16, 32);
			low1_pi16 = _mm_or_si64(low1_pi16, temp_pi16);

			// Apply the even reconstruction filter to the lowpass band
			even_pi16 = low1_pi16;
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);

			// Apply the rounding adjustment
			even_pi16 = _mm_adds_pi16(even_pi16, _mm_set1_pi16(4));
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);

			// Shift in the next two highpass coefficients
			high1_pi16 = _mm_srli_si64(high1_pi16, 2*16);
			high1_pi16 = _mm_or_si64(high1_pi16, _mm_slli_si64(high2_pi16, 16));

			// Prescale for 8-bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);

#if (_LOWPASS_PRESCALE == 0)
			even_pi16 = _mm_srai_pi16(even_pi16, 1);
#elif (_LOWPASS_PRESCALE > 1)
			even_pi16 = _mm_slli_pi16(even_pi16, (_LOWPASS_PRESCALE - 1));
#endif
			// Apply the odd reconstruction filter to the lowpass band
			odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);

			// Apply the rounding adjustment
			odd_pi16 = _mm_adds_pi16(odd_pi16, _mm_set1_pi16(4));
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
#if (_LOWPASS_PRESCALE == 0)
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);
#elif (_LOWPASS_PRESCALE > 1)
			odd_pi16 = _mm_slli_pi16(odd_pi16, (_LOWPASS_PRESCALE - 1));
#endif
			// Interleave the even and odd results
			out_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());
			*(outptr++) = out_pi16;

			// The second four lowpass coefficients will be the current values
			low1_pi16 = low2_pi16;

			// The second four highpass coefficients will be the current values
			high1_pi16 = high2_pi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Process the rest of the columns up to the last column in the row
		colptr = (PIXEL *)outptr;

		for (; column < last_column; column++)
		{
			int32_t even = 0;		// Result of convolution with even filter
			int32_t odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += ROUNDING(even,8);
			even = DivideByShift(even, 3);
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];

#if (_LOWPASS_PRESCALE == 0)
			even = DivideByShift(even, 1);
#elif (_LOWPASS_PRESCALE > 1)
			even <<= (_LOWPASS_PRESCALE - 1);
#endif
			// Place the even result in the even column
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += ROUNDING(odd,8);
			odd = DivideByShift(odd, 3);
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];

#if (_LOWPASS_PRESCALE == 0)
			odd = DivideByShift(odd, 1);
#elif (_LOWPASS_PRESCALE > 1)
			odd <<= (_LOWPASS_PRESCALE - 1);
#endif
			// Place the odd result in the odd column
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];

#if (_LOWPASS_PRESCALE == 0)
		even = DivideByShift(even, 1);
#elif (_LOWPASS_PRESCALE > 1)
		even <<= (_LOWPASS_PRESCALE - 1);
#endif
		// Place the even result in the even column
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];

#if (_LOWPASS_PRESCALE == 0)
		odd = DivideByShift(odd, 1);
#elif (_LOWPASS_PRESCALE > 1)
		odd <<= (_LOWPASS_PRESCALE - 1);
#endif
		// Place the odd result in the odd column
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}

	//_mm_empty();	// Clear the mmx register state
}

#endif


#if _PROCESSOR_PENTIUM_4

#if _PROCESSOR_DISPATCH
__declspec(cpu_specific(Pentium_4))
#endif

// Apply the inverse horizontal transform to reconstruct a strip of prescaled rows
void InvertHorizontalStripPrescaled16s(PIXEL *lowpass_band,		// Horizontal lowpass coefficients
									   int lowpass_pitch,		// Distance between rows in bytes
									   PIXEL *highpass_band,	// Horizontal highpass coefficients
									   int highpass_pitch,		// Distance between rows in bytes
									   PIXEL *output_image,		// Row of reconstructed results
									   int output_pitch,		// Distance between rows in bytes
									   ROI roi)					// Height and width of the strip
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	PIXEL *output = output_image;
	const int column_step = 8;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m128i low1_epi16;		// Lowpass coefficients
		__m128i low2_epi16;
		__m128i high1_epi16;	// Highpass coefficients
		__m128i high2_epi16;

		PIXEL *colptr;

		int32_t even;
		int32_t odd;

#if _UNALIGNED
		// The fast loop computes output points starting at the third column
		__m128i *outptr = (__m128i *)&output[2];
#else
		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];

		// Two 16-bit coefficients from the previous loop iteration
		//short remainder[2];
#endif
		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];

#if (_LOWPASS_PRESCALE == 0)
		even = DivideByShift(even, 1);
#elif (_LOWPASS_PRESCALE > 1)
		even <<= (_LOWPASS_PRESCALE - 1);
#endif
#if _UNALIGNED
		// Place the even result in the even column
		output[0] = SATURATE(even);
#else
		// The even result will be stored later
		//remainder[0] = SATURATE(even);
#endif
		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];

#if (_LOWPASS_PRESCALE == 0)
		odd = DivideByShift(odd, 1);
#elif (_LOWPASS_PRESCALE > 1)
		odd <<= (_LOWPASS_PRESCALE - 1);
#endif
#if _UNALIGNED
		// Place the odd result in the odd column
		output[1] = SATURATE(odd);
#else
		// The odd result will be stored later
		//remainder[1] = SATURATE(odd);
#endif

#if (_FASTLOOP && XMMOPT)

		// Check that the input and output addresses are properly aligned
		assert(ISALIGNED16(lowpass));
		assert(ISALIGNED16(highpass));
		//assert(ISALIGNED16(outptr));

		// Preload the first eight lowpass coefficients
		low1_epi16 = _mm_load_si128((__m128i *)&lowpass[column]);

		// Preload the first four highpass coefficients
		high1_epi16 = _mm_load_si128((__m128i *)&highpass[column]);

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i out_epi16;		// Reconstructed data
			//__m128i mask_epi16;
			//__m128i half_epi16;
			//__m128i lsb_epi16;
			//__m128i sign_epi16;
			__m128i high_epi16;
			uint32_t temp;		// Temporary register for last two values

			// Preload the next four lowpass coefficients
			low2_epi16 = _mm_load_si128((__m128i *)&lowpass[column+8]);


			// Compute the first two even and two odd output points //

			// Apply the even reconstruction filter to the lowpass band
//DAN031304 -- wrong inverse filter
			assert(0); //fix filter
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_slli_epi16(low1_epi16, 3);
			temp_epi16 = _mm_srli_si128(temp_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			temp_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);

			// Apply the rounding adjustment
			even_epi16 = _mm_adds_epi16(even_epi16, _mm_set1_epi16(4));
			// Divide by eight
			even_epi16 = _mm_srai_epi16(even_epi16, 3);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Prescale for 8bit output - DAN 4/5/02
			high_epi16 = high1_epi16;

			// Add the highpass correction
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);

#if (_LOWPASS_PRESCALE == 0)
			even_epi16 = _mm_srai_epi16(even_epi16, 1);
#elif (_LOWPASS_PRESCALE > 1)
			even_epi16 = _mm_slli_epi16(even_epi16, (_LOWPASS_PRESCALE - 1));
#endif
			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_slli_epi16(low1_epi16, 3);
			odd_epi16 = _mm_srli_si128(odd_epi16, 1*2);
			temp_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, low1_epi16);

			// Apply the rounding adjustment
			odd_epi16 = _mm_adds_epi16(odd_epi16, _mm_set1_epi16(4));
			// Divide by eight
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);

			// Subtract the highpass correction
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);

#if (_LOWPASS_PRESCALE == 0)
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);
#elif (_LOWPASS_PRESCALE > 1)
			odd_epi16 = _mm_slli_epi16(odd_epi16, (_LOWPASS_PRESCALE - 1));
#endif
			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out_epi16 = _mm_max_epi16(out_epi16, _mm_setzero_si128());

#if _UNALIGNED
			// Store the first eight output values
			_mm_storeu_si128(outptr++, out_epi16);
#elif 0
			// Combine the new output values with the two values from the previous phase
			temp_epi16 = _mm_srli_si128(out_epi16, 6*2);
			temp = _mm_cvtsi128_si32(temp_epi16);
			out_epi16 = _mm_slli_si128(out_epi16, 2*2);
			out_epi16 = _mm_or_si128(out_epi16, _mm_cvtsi32_si128(*((int *)remainder)));

			// Save the remaining two output values
			*((int *)remainder) = temp;

			// Store the first eight output values
			_mm_store_si128(outptr++, out_epi16);
#else
			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);

			// Store the first eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);
#endif

			// Compute the second four even and four odd output points //

			// Preload the highpass correction
			high2_epi16 = _mm_load_si128((__m128i *)&highpass[column+8]);

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_slli_epi16(low1_epi16, 3);
			temp_epi16 = _mm_srli_si128(temp_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);
			temp_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);

			// Apply the rounding adjustment
			even_epi16 = _mm_adds_epi16(even_epi16, _mm_set1_epi16(4));
			// Divide by eight
			even_epi16 = _mm_srai_epi16(even_epi16, 3);

			// Shift in the next five highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Prescale for 8-bit output - DAN 4/5/02
			high_epi16 = high1_epi16;

			// Add the highpass correction
			even_epi16 = _mm_adds_epi16(even_epi16, high_epi16);

#if (_LOWPASS_PRESCALE == 0)
			even_epi16 = _mm_srai_epi16(even_epi16, 1);
#elif (_LOWPASS_PRESCALE > 1)
			even_epi16 = _mm_slli_epi16(even_epi16, (_LOWPASS_PRESCALE - 1));
#endif
			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_slli_epi16(low1_epi16, 3);
			odd_epi16 = _mm_srli_si128(odd_epi16, 1*2);
			temp_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, low1_epi16);

			// Apply the rounding adjustment
			odd_epi16 = _mm_adds_epi16(odd_epi16, _mm_set1_epi16(4));
			// Divide by eight
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);

			// Subtract the highpass correction
			odd_epi16 = _mm_subs_epi16(odd_epi16, high_epi16);
#if (_LOWPASS_PRESCALE == 0)
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);
#elif (_LOWPASS_PRESCALE > 1)
			odd_epi16 = _mm_slli_epi16(odd_epi16, (_LOWPASS_PRESCALE - 1));
#endif
			// Interleave the even and odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			//out_epi16 = _mm_max_epi16(out_epi16, _mm_setzero_si128());

#if _UNALIGNED
			// Store the second eight output values
			_mm_storeu_si128(outptr++, out_epi16);
#elif 0
			// Combine the new output values with the two values from the previous phase
			temp_epi16 = _mm_srli_si128(out_epi16, 6*2);
			temp = _mm_cvtsi128_si32(temp_epi16);
			out_epi16 = _mm_slli_si128(out_epi16, 2*2);
			out_epi16 = _mm_or_si128(out_epi16, _mm_cvtsi32_si128(*((int *)remainder)));

			// Save the remaining two output values
			*((int *)remainder) = temp;

			// Store the second eight output values
			_mm_store_si128(outptr++, out_epi16);
#else
			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, even, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, odd, 1);

			// Store the first eight output values
			_mm_store_si128(outptr++, out_epi16);

			// Save the remaining two output values
			even = (short)temp;
			odd = (short)(temp >> 16);
#endif
			// The second four lowpass coefficients will be the current values
			low1_epi16 = low2_epi16;

			// The second four highpass coefficients will be the current values
			high1_epi16 = high2_epi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

#endif

		// The fast processing loop is one column behind the actual column
		column++;

		// Get the pointer to the next output value
		colptr = (PIXEL *)outptr;

#if _UNALIGNED
		// The last two output points have already been stored
#elif 0
		// Store the last two output points produced by the loop
		*(colptr++) = remainder[0];
		*(colptr++) = remainder[1];
#else
		// Store the last two output points produced by the loop
		*(colptr++) = SATURATE(even);
		*(colptr++) = SATURATE(odd);
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column++)
		{
			int32_t even = 0;		// Result of convolution with even filter
			int32_t odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += ROUNDING(even,8);
			even = DivideByShift(even, 3);
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];

#if (_LOWPASS_PRESCALE == 0)
			even = DivideByShift(even, 1);
#elif (_LOWPASS_PRESCALE > 1)
			even <<= (_LOWPASS_PRESCALE - 1);
#endif
			// Place the even result in the even column
			*(colptr++) = SATURATE(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += ROUNDING(odd,8);
			odd = DivideByShift(odd, 3);
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];

#if (_LOWPASS_PRESCALE == 0)
			odd = DivideByShift(odd, 1);
#elif (_LOWPASS_PRESCALE > 1)
			odd <<= (_LOWPASS_PRESCALE - 1);
#endif
			// Place the odd result in the odd column
			*(colptr++) = SATURATE(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];

#if (_LOWPASS_PRESCALE == 0)
		even = DivideByShift(even, 1);
#elif (_LOWPASS_PRESCALE > 1)
		even <<= (_LOWPASS_PRESCALE - 1);
#endif
		// Place the even result in the even column
		*(colptr++) = SATURATE(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];

#if (_LOWPASS_PRESCALE == 0)
		odd = DivideByShift(odd, 1);
#elif (_LOWPASS_PRESCALE > 1)
		odd <<= (_LOWPASS_PRESCALE - 1);
#endif
		// Place the odd result in the odd column
		*(colptr++) = SATURATE(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}
}

#endif


// Apply the inverse horizontal transform to reconstruct a strip of rows into packed YUV pixels
void InvertHorizontalStrip16sToYUYV(PIXEL *lowpass_band[],	// Horizontal lowpass coefficients
									int lowpass_pitch[],	// Distance between rows in bytes
									PIXEL *highpass_band[],	// Horizontal highpass coefficients
									int highpass_pitch[],	// Distance between rows in bytes
									uint8_t *output_image,		// Row of reconstructed results
									int output_pitch,		// Distance between rows in bytes
									ROI roi,				// Height and width of the strip
									int precision)			// Precision of the original video
{
	int num_channels = CODEC_NUM_CHANNELS;
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *y_lowpass_ptr = lowpass_band[0];
	PIXEL *u_lowpass_ptr = lowpass_band[2];
	PIXEL *v_lowpass_ptr = lowpass_band[1];
	PIXEL *y_highpass_ptr = highpass_band[0];
	PIXEL *u_highpass_ptr = highpass_band[2];
	PIXEL *v_highpass_ptr = highpass_band[1];

	uint8_t *output = output_image;

	// Process sixteen luma coefficients per loop iteration
	const int column_step = 16;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;
	int post_column = last_column - (last_column % column_step);

	// Need at least four luma values of border processing up to the last column
	//const int post_border = 2;

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	const int descale_shift = (precision - 8); //DAN060725 -- dither is > 8-bit
	const int descale_offset = (precision - 8); //DAN060725 -- dither is > 8-bit

	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);

	// Adjust the end of the fast loop if necessary for border processing
	//if (post_column > (last_column - post_border))  // DAN08112004 - ignore end row calc -- use SSE to edge.
	//	post_column -= column_step;

	// Check that there is enough margin to accommodate border processing
//	assert(post_column <= (last_column - post_border));

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
#if (1 && XMMOPT)
		__m128i y_low1_epi16;		// Lowpass coefficients
		__m128i y_low2_epi16;
		__m128i u_low1_epi16;
		__m128i u_low2_epi16;
		__m128i v_low1_epi16;
		__m128i v_low2_epi16;

		__m128i y_high1_epi16;		// Highpass coefficients
		__m128i y_high2_epi16;
		__m128i u_high1_epi16;
		__m128i u_high2_epi16;
		__m128i v_high1_epi16;
		__m128i v_high2_epi16;

		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];
#endif
		uint8_t *colptr = (uint8_t *)&output[0];

		int32_t y_even_value;
		int32_t u_even_value;
		int32_t v_even_value;
		int32_t y_odd_value;
		int32_t u_odd_value;
		int32_t v_odd_value;


		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two luma output points with special filters for the left border
		int32_t even = 0;
		int32_t odd = 0;

		int chroma_column;

		__m128i rounding1_pi16 = _mm_set1_epi16(0); // for 6bit matm
		__m128i rounding2_pi16 = _mm_set1_epi16(0); // for 6bit matm

		if(descale_offset>=2)
		{
			int mask = (1<<(descale_offset-1))-1; //DAN20090601
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 0);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 1);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 2);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 3);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 4);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 5);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 6);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 7);

			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 0);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 1);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 2);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 3);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 4);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 5);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 6);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 7);

			rounding1_pi16 = _mm_adds_epi16(rounding1_pi16, _mm_set1_epi16(10*mask/32)); //DAN20090601
			rounding2_pi16 = _mm_adds_epi16(rounding2_pi16, _mm_set1_epi16(10*mask/32));
		}

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * y_lowpass_ptr[column + 0];
		even -=  4 * y_lowpass_ptr[column + 1];
		even +=  1 * y_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += y_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		y_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * y_lowpass_ptr[column + 0];
		odd += 4 * y_lowpass_ptr[column + 1];
		odd -= 1 * y_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= y_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		y_odd_value = odd;


		// Process the first two u chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * u_lowpass_ptr[column + 0];
		even -=  4 * u_lowpass_ptr[column + 1];
		even +=  1 * u_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += u_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		u_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * u_lowpass_ptr[column + 0];
		odd += 4 * u_lowpass_ptr[column + 1];
		odd -= 1 * u_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= u_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		u_odd_value = odd;


		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * v_lowpass_ptr[column + 0];
		even -=  4 * v_lowpass_ptr[column + 1];
		even +=  1 * v_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += v_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		v_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * v_lowpass_ptr[column + 0];
		odd += 4 * v_lowpass_ptr[column + 1];
		odd -= 1 * v_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= v_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		v_odd_value = odd;

#if (1 && XMMOPT)

		// Preload the first eight lowpass luma coefficients
		y_low1_epi16 = _mm_load_si128((__m128i *)&y_lowpass_ptr[0]);

		// Preload the first eight highpass luma coefficients
		y_high1_epi16 = _mm_load_si128((__m128i *)&y_highpass_ptr[0]);

		// Preload the first eight lowpass u chroma coefficients
		u_low1_epi16 = _mm_load_si128((__m128i *)&u_lowpass_ptr[0]);

		// Preload the first eight highpass u chroma coefficients
		u_high1_epi16 = _mm_load_si128((__m128i *)&u_highpass_ptr[0]);

		// Preload the first eight lowpass v chroma coefficients
		v_low1_epi16 = _mm_load_si128((__m128i *)&v_lowpass_ptr[0]);

		// Preload the first eight highpass v chroma coefficients
 		v_high1_epi16 = _mm_load_si128((__m128i *)&v_highpass_ptr[0]);


		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i y1_output_epi16;
			__m128i y2_output_epi16;
			__m128i y3_output_epi16;
			__m128i y4_output_epi16;
			__m128i u1_output_epi16;
			__m128i u2_output_epi16;
			__m128i v1_output_epi16;
			__m128i v2_output_epi16;
			__m128i uv_epi16;
			__m128i yuv1_epi16;
			__m128i yuv2_epi16;
			__m128i yuv1_epi8;
			__m128i yuv2_epi8;
			__m128i yuv3_epi8;
			__m128i yuv4_epi8;

			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i out_epi16;		// Reconstructed data
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			__m128i low1_epi16;
			__m128i low2_epi16;
			__m128i high1_epi16;
			__m128i high2_epi16;

			//DAN031304 -- correct inverse filter
			__m128i half_epi16 = _mm_set1_epi16(4);
			__m128i offset_epi16 = _mm_set1_epi16(2048);

			uint32_t temp;		// Temporary register for last two values

			chroma_column = column/2;


			/***** Compute the first eight luma output values *****/

			// Preload the second eight lowpass coefficients
			y_low2_epi16 = _mm_load_si128((__m128i *)&y_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			y_high2_epi16 = _mm_load_si128((__m128i *)&y_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = y_low1_epi16;
			high1_epi16 = y_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);


			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, y_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, y_odd_value, 1);

			// Save the eight luma values for packing later
			y1_output_epi16 = out_epi16;

			// Save the remaining two output values
			y_even_value = (short)temp;
			y_odd_value = (short)(temp >> 16);


			/***** Compute the second eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = y_low2_epi16;
			high2_epi16 = y_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, y_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, y_odd_value, 1);

			// Save the eight luma values for packing later
			y2_output_epi16 = out_epi16;

			// Save the remaining two output values
			y_even_value = (short)temp;
			y_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are used later in the loop
			y_low1_epi16 = y_low2_epi16;

			// The second eight highpass coefficients are used later in the loop
			y_high1_epi16 = y_high2_epi16;


			/***** Compute the first eight u chroma output values *****/

			// Preload the second eight lowpass coefficients
			u_low2_epi16 = _mm_load_si128((__m128i *)&u_lowpass_ptr[chroma_column + 8]);

			// Preload the second eight highpass coefficients
			u_high2_epi16 = _mm_load_si128((__m128i *)&u_highpass_ptr[chroma_column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = u_low1_epi16;
			high1_epi16 = u_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, u_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, u_odd_value, 1);

			// Save the eight u chroma values for packing later
			u1_output_epi16 = out_epi16;

			// Save the remaining two output values
			u_even_value = (short)temp;
			u_odd_value = (short)(temp >> 16);


			/***** Compute the second eight u chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = u_low2_epi16;
			high2_epi16 = u_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, u_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, u_odd_value, 1);

			// Save the eight u chroma values for packing later
			u2_output_epi16 = out_epi16;

			// Save the remaining two output values
			u_even_value = (short)temp;
			u_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			u_low1_epi16 = u_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			u_high1_epi16 = u_high2_epi16;


			/***** Compute the third eight luma output values *****/

			// Preload the third eight lowpass coefficients
			y_low2_epi16 = _mm_load_si128((__m128i *)&y_lowpass_ptr[column + 16]);

			// Preload the third eight highpass coefficients
			y_high2_epi16 = _mm_load_si128((__m128i *)&y_highpass_ptr[column + 16]);

			// Move the current set of coefficients to working registers
			low1_epi16 = y_low1_epi16;
			high1_epi16 = y_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, y_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, y_odd_value, 1);

			// Save the eight luma values for packing later
			y3_output_epi16 = out_epi16;

			// Save the remaining two output values
			y_even_value = (short)temp;
			y_odd_value = (short)(temp >> 16);


			/***** Compute the fourth eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = y_low2_epi16;
			high2_epi16 = y_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, y_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, y_odd_value, 1);

			// Save the eight luma values for packing later
			y4_output_epi16 = out_epi16;

			// Save the remaining two output values
			y_even_value = (short)temp;
			y_odd_value = (short)(temp >> 16);

			// The third eight lowpass coefficients are the current values in the next iteration
			y_low1_epi16 = y_low2_epi16;

			// The third eight highpass coefficients are the current values in the next iteration
			y_high1_epi16 = y_high2_epi16;


			/***** Compute the first eight v chroma output values *****/

			// Preload the second eight lowpass coefficients
			v_low2_epi16 = _mm_load_si128((__m128i *)&v_lowpass_ptr[chroma_column + 8]);

			// Preload the second eight highpass coefficients
			v_high2_epi16 = _mm_load_si128((__m128i *)&v_highpass_ptr[chroma_column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = v_low1_epi16;
			high1_epi16 = v_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, v_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, v_odd_value, 1);

			// Save the eight u chroma values for packing later
			v1_output_epi16 = out_epi16;

			// Save the remaining two output values
			v_even_value = (short)temp;
			v_odd_value = (short)(temp >> 16);


			/***** Compute the second eight v chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = v_low2_epi16;
			high2_epi16 = v_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, v_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, v_odd_value, 1);

			// Save the eight u chroma values for packing later
			v2_output_epi16 = out_epi16;

			// Save the remaining two output values
			v_even_value = (short)temp;
			v_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			v_low1_epi16 = v_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			v_high1_epi16 = v_high2_epi16;


			/***** Interleave the luma and chroma values *****/

			// Interleave the first four values from each chroma channel
			uv_epi16 = _mm_unpacklo_epi16(u1_output_epi16, v1_output_epi16);

			// Interleave the first eight chroma values with the first eight luma values
			yuv1_epi16 = _mm_unpacklo_epi16(y1_output_epi16, uv_epi16);
			yuv2_epi16 = _mm_unpackhi_epi16(y1_output_epi16, uv_epi16);

			// Pack the first sixteen bytes of luma and chroma
			yuv1_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

			// Store the first sixteen bytes of output values
			_mm_store_si128(outptr++, yuv1_epi8);

			// Interleave the second four values from each chroma channel
			uv_epi16 = _mm_unpackhi_epi16(u1_output_epi16, v1_output_epi16);

			// Interleave the second eight chroma values with the second eight luma values
			yuv1_epi16 = _mm_unpacklo_epi16(y2_output_epi16, uv_epi16);
			yuv2_epi16 = _mm_unpackhi_epi16(y2_output_epi16, uv_epi16);

			// Pack the second sixteen bytes of luma and chroma
			yuv2_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

			// Store the second sixteen bytes of output values
			_mm_store_si128(outptr++, yuv2_epi8);

			// Interleave the third four values from each chroma channel
			uv_epi16 = _mm_unpacklo_epi16(u2_output_epi16, v2_output_epi16);

			// Interleave the third eight chroma values with the third eight luma values
			yuv1_epi16 = _mm_unpacklo_epi16(y3_output_epi16, uv_epi16);
			yuv2_epi16 = _mm_unpackhi_epi16(y3_output_epi16, uv_epi16);

			// Pack the first sixteen bytes of luma and chroma
			yuv3_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

			// Store the first sixteen bytes of output values
			_mm_store_si128(outptr++, yuv3_epi8);

			// Interleave the fourth four values from each chroma channel
			uv_epi16 = _mm_unpackhi_epi16(u2_output_epi16, v2_output_epi16);

			// Interleave the second eight chroma values with the second eight luma values
			yuv1_epi16 = _mm_unpacklo_epi16(y4_output_epi16, uv_epi16);
			yuv2_epi16 = _mm_unpackhi_epi16(y4_output_epi16, uv_epi16);

			// Pack the second sixteen bytes of luma and chroma
			yuv4_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

			// Store the second sixteen bytes of output values
			_mm_store_si128(outptr++, yuv4_epi8);
		}

		// Should have exited the loop with the column equal to the post processing column
		//	assert(column == post_column);

		colptr = (uint8_t *)outptr;
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column += 2)
		{
			int y1_even_value;
			int y2_even_value;
			int y1_odd_value;
			int y2_odd_value;

			chroma_column = column/2;


			/***** First pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += y_lowpass_ptr[column - 1];
			even -= y_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += y_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += y_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the luma result for later output in the correct order
			y1_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= y_lowpass_ptr[column - 1];
			odd += y_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += y_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= y_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the luma result for later output in the correct order
			y1_odd_value = odd;


			/***** Pair of u chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += u_lowpass_ptr[chroma_column - 1];
			even -= u_lowpass_ptr[chroma_column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += u_lowpass_ptr[chroma_column + 0];

			// Add the highpass correction
			even += u_highpass_ptr[chroma_column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the u chroma result for later output in the correct order
			u_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= u_lowpass_ptr[chroma_column - 1];
			odd += u_lowpass_ptr[chroma_column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += u_lowpass_ptr[chroma_column + 0];

			// Subtract the highpass correction
			odd -= u_highpass_ptr[chroma_column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the u chroma result for later output in the correct order
			u_odd_value = odd;


			/***** Second pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += y_lowpass_ptr[column + 0];
			even -= y_lowpass_ptr[column + 2];
			even += 4; //DAN20050921
			even >>= 3;
			even += y_lowpass_ptr[column + 1];

			// Add the highpass correction
			even += y_highpass_ptr[column + 1];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the luma result for later output in the correct order
			y2_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= y_lowpass_ptr[column + 0];
			odd += y_lowpass_ptr[column + 2];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += y_lowpass_ptr[column + 1];

			// Subtract the highpass correction
			odd -= y_highpass_ptr[column + 1];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the luma result for later output in the correct order
			y2_odd_value = odd;


			/***** Pair of v chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += v_lowpass_ptr[chroma_column - 1];
			even -= v_lowpass_ptr[chroma_column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += v_lowpass_ptr[chroma_column + 0];

			// Add the highpass correction
			even += v_highpass_ptr[chroma_column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the v chroma result for later output in the correct order
			v_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= v_lowpass_ptr[chroma_column - 1];
			odd += v_lowpass_ptr[chroma_column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += v_lowpass_ptr[chroma_column + 0];

			// Subtract the highpass correction
			odd -= v_highpass_ptr[chroma_column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the v chroma result for later output in the correct order
			v_odd_value = odd;


			// Output the luma and chroma values in the correct order
			*(colptr++) = SATURATE_8U(y1_even_value);
			*(colptr++) = SATURATE_8U(u_even_value);
			*(colptr++) = SATURATE_8U(y1_odd_value);
			*(colptr++) = SATURATE_8U(v_even_value);

			// Need to output the second set of values?
			if ((column + 1) < last_column)
			{
				*(colptr++) = SATURATE_8U(y2_even_value);
				*(colptr++) = SATURATE_8U(u_odd_value);
				*(colptr++) = SATURATE_8U(y2_odd_value);
				*(colptr++) = SATURATE_8U(v_odd_value);
			}
			else
			{
				column++;
				break;
			}
		}

		// Should have exited the loop at the column for right border processing
		//	assert(column == last_column);

		column = last_column - 1;
		colptr -= 4;

		// Compute the last chroma column
		chroma_column = column/2;

		// Process the last luma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * y_lowpass_ptr[column + 0];
		even += 4 * y_lowpass_ptr[column - 1];
		even -= 1 * y_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += y_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the luma result for later output in the correct order
		y_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * y_lowpass_ptr[column + 0];
		odd -=  4 * y_lowpass_ptr[column - 1];
		odd +=  1 * y_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= y_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the luma result for later output in the correct order
		y_odd_value = odd;

		// Process the last u chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * u_lowpass_ptr[chroma_column + 0];
		even += 4 * u_lowpass_ptr[chroma_column - 1];
		even -= 1 * u_lowpass_ptr[chroma_column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += u_highpass_ptr[chroma_column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		u_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * u_lowpass_ptr[chroma_column + 0];
		odd -=  4 * u_lowpass_ptr[chroma_column - 1];
		odd +=  1 * u_lowpass_ptr[chroma_column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= u_highpass_ptr[chroma_column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		u_odd_value = odd;

		// Process the last v chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * v_lowpass_ptr[chroma_column + 0];
		even += 4 * v_lowpass_ptr[chroma_column - 1];
		even -= 1 * v_lowpass_ptr[chroma_column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += v_highpass_ptr[chroma_column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		v_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * v_lowpass_ptr[chroma_column + 0];
		odd -=  4 * v_lowpass_ptr[chroma_column - 1];
		odd +=  1 * v_lowpass_ptr[chroma_column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= v_highpass_ptr[chroma_column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		v_odd_value = odd;

		//DAN06052005 - Fix for PSNR errors in UV on right edge
		colptr-=4;
		colptr++; // Y fine
		*(colptr++) = SATURATE_8U(u_even_value);
		colptr++; // Y2 fine
		*(colptr++) = SATURATE_8U(v_even_value);

		// Output the last luma and chroma values in the correct order
		*(colptr++) = SATURATE_8U(y_even_value);
		*(colptr++) = SATURATE_8U(u_odd_value);
		*(colptr++) = SATURATE_8U(y_odd_value);
		*(colptr++) = SATURATE_8U(v_odd_value);

		// Advance to the next row of coefficients in each channel
		y_lowpass_ptr += lowpass_pitch[0];
		u_lowpass_ptr += lowpass_pitch[1];
		v_lowpass_ptr += lowpass_pitch[2];
		y_highpass_ptr += highpass_pitch[0];
		u_highpass_ptr += highpass_pitch[1];
		v_highpass_ptr += highpass_pitch[2];

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}

// Apply the inverse horizontal transform to reconstruct a strip of rows into packed UYVY
void InvertHorizontalStrip16sToUYVY(PIXEL *lowpass_band[],	// Horizontal lowpass coefficients
									int lowpass_pitch[],	// Distance between rows in bytes
									PIXEL *highpass_band[],	// Horizontal highpass coefficients
									int highpass_pitch[],	// Distance between rows in bytes
									uint8_t *output_image,		// Row of reconstructed results
									int output_pitch,		// Distance between rows in bytes
									ROI roi,				// Height and width of the strip
									int precision)			// Precision of the original video
{
	int num_channels = CODEC_NUM_CHANNELS;
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *y_lowpass_ptr = lowpass_band[0];
	PIXEL *u_lowpass_ptr = lowpass_band[2];
	PIXEL *v_lowpass_ptr = lowpass_band[1];
	PIXEL *y_highpass_ptr = highpass_band[0];
	PIXEL *u_highpass_ptr = highpass_band[2];
	PIXEL *v_highpass_ptr = highpass_band[1];

	uint8_t *output = output_image;

	// Process sixteen luma coefficients per loop iteration
	const int column_step = 16;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;
	int post_column = last_column - (last_column % column_step);

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	int descale_shift = (precision - 8); //DAN060725 -- dither is > 8-bit
	int descale_offset = (precision - 8); //DAN060725 -- dither is > 8-bit

	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);

	// Adjust the end of the fast loop if necessary for border processing
	//if (post_column > (last_column - post_border))  // DAN08112004 - ignore end row calc -- use SSE to edge.
	//	post_column -= column_step;

	// Check that there is enough margin to accommodate border processing
//	assert(post_column <= (last_column - post_border));

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
#if (1 && XMMOPT)
		__m128i y_low1_epi16;		// Lowpass coefficients
		__m128i y_low2_epi16;
		__m128i u_low1_epi16;
		__m128i u_low2_epi16;
		__m128i v_low1_epi16;
		__m128i v_low2_epi16;

		__m128i y_high1_epi16;		// Highpass coefficients
		__m128i y_high2_epi16;
		__m128i u_high1_epi16;
		__m128i u_high2_epi16;
		__m128i v_high1_epi16;
		__m128i v_high2_epi16;

		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];
#endif
		uint8_t *colptr = (uint8_t *)&output[0];

		int32_t y_even_value;
		int32_t u_even_value;
		int32_t v_even_value;
		int32_t y_odd_value;
		int32_t u_odd_value;
		int32_t v_odd_value;


		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two luma output points with special filters for the left border
		int32_t even = 0;
		int32_t odd = 0;

		int chroma_column;

		__m128i rounding1_pi16 = _mm_set1_epi16(0); // for 6bit matm
		__m128i rounding2_pi16 = _mm_set1_epi16(0); // for 6bit matm

		if(descale_offset>=2)
		{
			int mask = (1<<(descale_offset-1))-1; //DAN20090601
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 0);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 1);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 2);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 3);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 4);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 5);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 6);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 7);

			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 0);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 1);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 2);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 3);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 4);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 5);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 6);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 7);

			rounding1_pi16 = _mm_adds_epi16(rounding1_pi16, _mm_set1_epi16(10*mask/32)); //DAN20090601
			rounding2_pi16 = _mm_adds_epi16(rounding2_pi16, _mm_set1_epi16(10*mask/32));
		}

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * y_lowpass_ptr[column + 0];
		even -=  4 * y_lowpass_ptr[column + 1];
		even +=  1 * y_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += y_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		y_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * y_lowpass_ptr[column + 0];
		odd += 4 * y_lowpass_ptr[column + 1];
		odd -= 1 * y_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= y_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		y_odd_value = odd;


		// Process the first two u chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * u_lowpass_ptr[column + 0];
		even -=  4 * u_lowpass_ptr[column + 1];
		even +=  1 * u_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += u_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		u_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * u_lowpass_ptr[column + 0];
		odd += 4 * u_lowpass_ptr[column + 1];
		odd -= 1 * u_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= u_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		u_odd_value = odd;


		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * v_lowpass_ptr[column + 0];
		even -=  4 * v_lowpass_ptr[column + 1];
		even +=  1 * v_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += v_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		v_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * v_lowpass_ptr[column + 0];
		odd += 4 * v_lowpass_ptr[column + 1];
		odd -= 1 * v_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= v_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		v_odd_value = odd;

#if (1 && XMMOPT)

		// Preload the first eight lowpass luma coefficients
		y_low1_epi16 = _mm_load_si128((__m128i *)&y_lowpass_ptr[0]);

		// Preload the first eight highpass luma coefficients
		y_high1_epi16 = _mm_load_si128((__m128i *)&y_highpass_ptr[0]);

		// Preload the first eight lowpass u chroma coefficients
		u_low1_epi16 = _mm_load_si128((__m128i *)&u_lowpass_ptr[0]);

		// Preload the first eight highpass u chroma coefficients
		u_high1_epi16 = _mm_load_si128((__m128i *)&u_highpass_ptr[0]);

		// Preload the first eight lowpass v chroma coefficients
		v_low1_epi16 = _mm_load_si128((__m128i *)&v_lowpass_ptr[0]);

		// Preload the first eight highpass v chroma coefficients
 		v_high1_epi16 = _mm_load_si128((__m128i *)&v_highpass_ptr[0]);

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i y1_output_epi16;
			__m128i y2_output_epi16;
			__m128i y3_output_epi16;
			__m128i y4_output_epi16;
			__m128i u1_output_epi16;
			__m128i u2_output_epi16;
			__m128i v1_output_epi16;
			__m128i v2_output_epi16;
			__m128i uv_epi16;
			__m128i yuv1_epi16;
			__m128i yuv2_epi16;
			__m128i yuv1_epi8;
			__m128i yuv2_epi8;
			__m128i yuv3_epi8;
			__m128i yuv4_epi8;

			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i out_epi16;		// Reconstructed data
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			__m128i low1_epi16;
			__m128i low2_epi16;
			__m128i high1_epi16;
			__m128i high2_epi16;

			//DAN031304 -- correct inverse filter
			__m128i half_epi16 = _mm_set1_epi16(4);
			__m128i offset_epi16 = _mm_set1_epi16(2048);

			uint32_t temp;		// Temporary register for last two values

			chroma_column = column/2;


			/***** Compute the first eight luma output values *****/

			// Preload the second eight lowpass coefficients
			y_low2_epi16 = _mm_load_si128((__m128i *)&y_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			y_high2_epi16 = _mm_load_si128((__m128i *)&y_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = y_low1_epi16;
			high1_epi16 = y_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);


			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, y_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, y_odd_value, 1);

			// Save the eight luma values for packing later
			y1_output_epi16 = out_epi16;

			// Save the remaining two output values
			y_even_value = (short)temp;
			y_odd_value = (short)(temp >> 16);


			/***** Compute the second eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = y_low2_epi16;
			high2_epi16 = y_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, y_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, y_odd_value, 1);

			// Save the eight luma values for packing later
			y2_output_epi16 = out_epi16;

			// Save the remaining two output values
			y_even_value = (short)temp;
			y_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are used later in the loop
			y_low1_epi16 = y_low2_epi16;

			// The second eight highpass coefficients are used later in the loop
			y_high1_epi16 = y_high2_epi16;


			/***** Compute the first eight u chroma output values *****/

			// Preload the second eight lowpass coefficients
			u_low2_epi16 = _mm_load_si128((__m128i *)&u_lowpass_ptr[chroma_column + 8]);

			// Preload the second eight highpass coefficients
			u_high2_epi16 = _mm_load_si128((__m128i *)&u_highpass_ptr[chroma_column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = u_low1_epi16;
			high1_epi16 = u_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, u_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, u_odd_value, 1);

			// Save the eight u chroma values for packing later
			u1_output_epi16 = out_epi16;

			// Save the remaining two output values
			u_even_value = (short)temp;
			u_odd_value = (short)(temp >> 16);


			/***** Compute the second eight u chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = u_low2_epi16;
			high2_epi16 = u_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, u_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, u_odd_value, 1);

			// Save the eight u chroma values for packing later
			u2_output_epi16 = out_epi16;

			// Save the remaining two output values
			u_even_value = (short)temp;
			u_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			u_low1_epi16 = u_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			u_high1_epi16 = u_high2_epi16;


			/***** Compute the third eight luma output values *****/

			// Preload the third eight lowpass coefficients
			y_low2_epi16 = _mm_load_si128((__m128i *)&y_lowpass_ptr[column + 16]);

			// Preload the third eight highpass coefficients
			y_high2_epi16 = _mm_load_si128((__m128i *)&y_highpass_ptr[column + 16]);

			// Move the current set of coefficients to working registers
			low1_epi16 = y_low1_epi16;
			high1_epi16 = y_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, y_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, y_odd_value, 1);

			// Save the eight luma values for packing later
			y3_output_epi16 = out_epi16;

			// Save the remaining two output values
			y_even_value = (short)temp;
			y_odd_value = (short)(temp >> 16);


			/***** Compute the fourth eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = y_low2_epi16;
			high2_epi16 = y_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, y_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, y_odd_value, 1);

			// Save the eight luma values for packing later
			y4_output_epi16 = out_epi16;

			// Save the remaining two output values
			y_even_value = (short)temp;
			y_odd_value = (short)(temp >> 16);

			// The third eight lowpass coefficients are the current values in the next iteration
			y_low1_epi16 = y_low2_epi16;

			// The third eight highpass coefficients are the current values in the next iteration
			y_high1_epi16 = y_high2_epi16;


			/***** Compute the first eight v chroma output values *****/

			// Preload the second eight lowpass coefficients
			v_low2_epi16 = _mm_load_si128((__m128i *)&v_lowpass_ptr[chroma_column + 8]);

			// Preload the second eight highpass coefficients
			v_high2_epi16 = _mm_load_si128((__m128i *)&v_highpass_ptr[chroma_column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = v_low1_epi16;
			high1_epi16 = v_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, v_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, v_odd_value, 1);

			// Save the eight u chroma values for packing later
			v1_output_epi16 = out_epi16;

			// Save the remaining two output values
			v_even_value = (short)temp;
			v_odd_value = (short)(temp >> 16);


			/***** Compute the second eight v chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = v_low2_epi16;
			high2_epi16 = v_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, v_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, v_odd_value, 1);

			// Save the eight u chroma values for packing later
			v2_output_epi16 = out_epi16;

			// Save the remaining two output values
			v_even_value = (short)temp;
			v_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			v_low1_epi16 = v_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			v_high1_epi16 = v_high2_epi16;


			/***** Interleave the luma and chroma values *****/

			// Interleave the first four values from each chroma channel
			uv_epi16 = _mm_unpacklo_epi16(u1_output_epi16, v1_output_epi16);

			// Interleave the first eight chroma values with the first eight luma values
			yuv1_epi16 = _mm_unpacklo_epi16(uv_epi16, y1_output_epi16);
			yuv2_epi16 = _mm_unpackhi_epi16(uv_epi16, y1_output_epi16);

			// Pack the first sixteen bytes of luma and chroma
			yuv1_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

			// Store the first sixteen bytes of output values
			_mm_store_si128(outptr++, yuv1_epi8);

			// Interleave the second four values from each chroma channel
			uv_epi16 = _mm_unpackhi_epi16(u1_output_epi16, v1_output_epi16);

			// Interleave the second eight chroma values with the second eight luma values
			yuv1_epi16 = _mm_unpacklo_epi16(uv_epi16, y2_output_epi16);
			yuv2_epi16 = _mm_unpackhi_epi16(uv_epi16, y2_output_epi16);

			// Pack the second sixteen bytes of luma and chroma
			yuv2_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

			// Store the second sixteen bytes of output values
			_mm_store_si128(outptr++, yuv2_epi8);

			// Interleave the third four values from each chroma channel
			uv_epi16 = _mm_unpacklo_epi16(u2_output_epi16, v2_output_epi16);

			// Interleave the third eight chroma values with the third eight luma values
			yuv1_epi16 = _mm_unpacklo_epi16(uv_epi16, y3_output_epi16);
			yuv2_epi16 = _mm_unpackhi_epi16(uv_epi16, y3_output_epi16);

			// Pack the first sixteen bytes of luma and chroma
			yuv3_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

			// Store the first sixteen bytes of output values
			_mm_store_si128(outptr++, yuv3_epi8);

			// Interleave the fourth four values from each chroma channel
			uv_epi16 = _mm_unpackhi_epi16(u2_output_epi16, v2_output_epi16);

			// Interleave the second eight chroma values with the second eight luma values
			yuv1_epi16 = _mm_unpacklo_epi16(uv_epi16, y4_output_epi16);
			yuv2_epi16 = _mm_unpackhi_epi16(uv_epi16, y4_output_epi16);

			// Pack the second sixteen bytes of luma and chroma
			yuv4_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

			// Store the second sixteen bytes of output values
			_mm_store_si128(outptr++, yuv4_epi8);
		}

		// Should have exited the loop with the column equal to the post processing column
		//	assert(column == post_column);

		colptr = (uint8_t *)outptr;
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column += 2)
		{
			int y1_even_value;
			int y2_even_value;
			int y1_odd_value;
			int y2_odd_value;

			chroma_column = column/2;


			/***** First pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += y_lowpass_ptr[column - 1];
			even -= y_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += y_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += y_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the luma result for later output in the correct order
			y1_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= y_lowpass_ptr[column - 1];
			odd += y_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += y_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= y_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the luma result for later output in the correct order
			y1_odd_value = odd;


			/***** Pair of u chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += u_lowpass_ptr[chroma_column - 1];
			even -= u_lowpass_ptr[chroma_column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += u_lowpass_ptr[chroma_column + 0];

			// Add the highpass correction
			even += u_highpass_ptr[chroma_column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the u chroma result for later output in the correct order
			u_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= u_lowpass_ptr[chroma_column - 1];
			odd += u_lowpass_ptr[chroma_column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += u_lowpass_ptr[chroma_column + 0];

			// Subtract the highpass correction
			odd -= u_highpass_ptr[chroma_column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the u chroma result for later output in the correct order
			u_odd_value = odd;


			/***** Second pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += y_lowpass_ptr[column + 0];
			even -= y_lowpass_ptr[column + 2];
			even += 4; //DAN20050921
			even >>= 3;
			even += y_lowpass_ptr[column + 1];

			// Add the highpass correction
			even += y_highpass_ptr[column + 1];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the luma result for later output in the correct order
			y2_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= y_lowpass_ptr[column + 0];
			odd += y_lowpass_ptr[column + 2];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += y_lowpass_ptr[column + 1];

			// Subtract the highpass correction
			odd -= y_highpass_ptr[column + 1];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the luma result for later output in the correct order
			y2_odd_value = odd;


			/***** Pair of v chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += v_lowpass_ptr[chroma_column - 1];
			even -= v_lowpass_ptr[chroma_column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += v_lowpass_ptr[chroma_column + 0];

			// Add the highpass correction
			even += v_highpass_ptr[chroma_column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the v chroma result for later output in the correct order
			v_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= v_lowpass_ptr[chroma_column - 1];
			odd += v_lowpass_ptr[chroma_column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += v_lowpass_ptr[chroma_column + 0];

			// Subtract the highpass correction
			odd -= v_highpass_ptr[chroma_column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the v chroma result for later output in the correct order
			v_odd_value = odd;


			// Output the luma and chroma values in the correct order
			*(colptr++) = SATURATE_8U(u_even_value);
			*(colptr++) = SATURATE_8U(y1_even_value);
			*(colptr++) = SATURATE_8U(v_even_value);
			*(colptr++) = SATURATE_8U(y1_odd_value);

			// Need to output the second set of values?
			if ((column + 1) < last_column)
			{
				*(colptr++) = SATURATE_8U(u_odd_value);
				*(colptr++) = SATURATE_8U(y2_even_value);
				*(colptr++) = SATURATE_8U(v_odd_value);
				*(colptr++) = SATURATE_8U(y2_odd_value);
			}
			else
			{
				column++;
				break;
			}
		}

		// Should have exited the loop at the column for right border processing
		//	assert(column == last_column);

		column = last_column - 1;
		colptr -= 4;

		// Compute the last chroma column
		chroma_column = column/2;

		// Process the last luma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * y_lowpass_ptr[column + 0];
		even += 4 * y_lowpass_ptr[column - 1];
		even -= 1 * y_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += y_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the luma result for later output in the correct order
		y_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * y_lowpass_ptr[column + 0];
		odd -=  4 * y_lowpass_ptr[column - 1];
		odd +=  1 * y_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= y_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the luma result for later output in the correct order
		y_odd_value = odd;

		// Process the last u chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * u_lowpass_ptr[chroma_column + 0];
		even += 4 * u_lowpass_ptr[chroma_column - 1];
		even -= 1 * u_lowpass_ptr[chroma_column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += u_highpass_ptr[chroma_column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		u_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * u_lowpass_ptr[chroma_column + 0];
		odd -=  4 * u_lowpass_ptr[chroma_column - 1];
		odd +=  1 * u_lowpass_ptr[chroma_column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= u_highpass_ptr[chroma_column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		u_odd_value = odd;

		// Process the last v chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * v_lowpass_ptr[chroma_column + 0];
		even += 4 * v_lowpass_ptr[chroma_column - 1];
		even -= 1 * v_lowpass_ptr[chroma_column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += v_highpass_ptr[chroma_column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		v_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * v_lowpass_ptr[chroma_column + 0];
		odd -=  4 * v_lowpass_ptr[chroma_column - 1];
		odd +=  1 * v_lowpass_ptr[chroma_column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= v_highpass_ptr[chroma_column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		v_odd_value = odd;

		//DAN06052005 - Fix for PSNR errors in UV on right edge
		colptr-=4;
		*(colptr++) = SATURATE_8U(u_even_value);
		colptr++; // Y fine
		*(colptr++) = SATURATE_8U(v_even_value);
		colptr++; // Y2 fine

		// Output the last luma and chroma values in the correct order
		*(colptr++) = SATURATE_8U(u_odd_value);
		*(colptr++) = SATURATE_8U(y_even_value);
		*(colptr++) = SATURATE_8U(v_odd_value);
		*(colptr++) = SATURATE_8U(y_odd_value);

		// Advance to the next row of coefficients in each channel
		y_lowpass_ptr += lowpass_pitch[0];
		u_lowpass_ptr += lowpass_pitch[1];
		v_lowpass_ptr += lowpass_pitch[2];
		y_highpass_ptr += highpass_pitch[0];
		u_highpass_ptr += highpass_pitch[1];
		v_highpass_ptr += highpass_pitch[2];

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}


void HalfHorizontalStrip16sToYUYV(PIXEL *lowpass_band[],	// Horizontal lowpass coefficients
									int lowpass_pitch[],	// Distance between rows in bytes
									PIXEL *highpass_band[],	// Horizontal highpass coefficients
									int highpass_pitch[],	// Distance between rows in bytes
									uint8_t *output_image,		// Row of reconstructed results
									int output_pitch,		// Distance between rows in bytes
									ROI roi,				// Height and width of the strip
									int precision,			// Precision of the original video
									int format)				// COLOR_FORMAT_YUYV or COLOR_FORMAT_UYVY
{
	int num_channels = CODEC_NUM_CHANNELS;
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *y_lowpass_ptr = lowpass_band[0];
	PIXEL *u_lowpass_ptr = lowpass_band[2];
	PIXEL *v_lowpass_ptr = lowpass_band[1];
	PIXEL *y_highpass_ptr = highpass_band[0];
	PIXEL *u_highpass_ptr = highpass_band[2];
	PIXEL *v_highpass_ptr = highpass_band[1];

	uint8_t *output = output_image;
	uint8_t *colptr;

	// Process sixteen luma coefficients per loop iteration
//	const int column_step = 16;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;

	// Need at least four luma values of border processing up to the last column
//	const int post_border = 2;

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	int descale_shift = (precision - 8); //DAN060725 -- dither is > 8-bit

	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);


	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		int column;
		int shift = descale_shift+1;
		colptr = (uint8_t *)output;
		// Process the rest of the columns up to the last column in the row

		if(format == COLOR_FORMAT_YUYV)
		{
			for (column = 0; column < last_column; column += 2)
			{
				int chroma_column = column>>1;

				*(colptr++) = SATURATE_8U(y_lowpass_ptr[column]>>shift);
				*(colptr++) = SATURATE_8U(u_lowpass_ptr[chroma_column]>>shift);
				*(colptr++) = SATURATE_8U(y_lowpass_ptr[column+1]>>shift);
				*(colptr++) = SATURATE_8U(v_lowpass_ptr[chroma_column]>>shift);
			}
		}
		else
		{
			for (column = 0; column < last_column; column += 2)
			{
				int chroma_column = column>>1;

				*(colptr++) = SATURATE_8U(u_lowpass_ptr[chroma_column]>>shift);
				*(colptr++) = SATURATE_8U(y_lowpass_ptr[column]>>shift);
				*(colptr++) = SATURATE_8U(v_lowpass_ptr[chroma_column]>>shift);
				*(colptr++) = SATURATE_8U(y_lowpass_ptr[column+1]>>shift);
			}
		}

		// Advance to the next row of coefficients in each channel
		y_lowpass_ptr += lowpass_pitch[0];
		u_lowpass_ptr += lowpass_pitch[1];
		v_lowpass_ptr += lowpass_pitch[2];
		y_highpass_ptr += highpass_pitch[0];
		u_highpass_ptr += highpass_pitch[1];
		v_highpass_ptr += highpass_pitch[2];

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}

// Apply the inverse horizontal transform to reconstruct a strip of rows into packed YUV pixels
void InvertHorizontalStrip16sToYUV(HorizontalFilterParams)
{
	if((decoder->channel_blend_type == 2 || decoder->channel_blend_type == 7) && decoder->frame.format == DECODED_FORMAT_YUYV) //3d work
	{
		HalfHorizontalStrip16sToYUYV(lowpass_band, lowpass_pitch,
										highpass_band, highpass_pitch,
										output_image, output_pitch,
										roi, precision, format);
	}
	else
	{
		if (format == COLOR_FORMAT_YUYV)
		{
			InvertHorizontalStrip16sToYUYV(lowpass_band, lowpass_pitch,
										highpass_band, highpass_pitch,
										output_image, output_pitch,
										roi, precision);
		}
		else
		{
			assert(format == COLOR_FORMAT_UYVY);

			InvertHorizontalStrip16sToUYVY(lowpass_band, lowpass_pitch,
										highpass_band, highpass_pitch,
										output_image, output_pitch,
										roi, precision);
		}
	}
}

// Apply the inverse horizontal transform to reconstruct a strip of rows into packed YUV pixels
void InvertHorizontalStrip16sToOutput(HorizontalFilterParams)
{
	int i;
	int channels = decoder->codec.num_channels;
	//uint8_t *chroma_buffer = output_buffer;
	uint8_t *plane_array[TRANSFORM_MAX_CHANNELS] = {0};
	int plane_pitch[TRANSFORM_MAX_CHANNELS] = {0};

	uint8_t *output_row_ptr = output_image;

	// Scratch buffer for the reconstructed rows allocated on the stack
	//unsigned short scanline2[6 * 2048];

	void *scratch = NULL;
	size_t scratchsize = 0;
	int local_pitch = roi.width * 2 * 2 * 2;
	uint8_t *sptr;
	uint8_t *sptr2;
	ROI output_strip = roi;
	int color_space = decoder->frame.colorspace;

	scratch = decoder->threads_buffer[thread_index];
	scratchsize = decoder->threads_buffer_size;
	if((int)scratchsize < local_pitch)
	{
		assert(0);
		return;
	}


	// Two rows of 4:2:2 YUV components, two components per pixel, each component is 16 bits
	output_strip.width *= 2;

	// Pointer for allocating row buffers that are 16 byte aligned
	//sptr = (uint8_t *)&scanline2[0];
	sptr = (uint8_t *)scratch;
	sptr = sptr2 = (uint8_t *)((((uintptr_t)sptr)+15) & ~0x0F);
	sptr2 += output_strip.width * 6;

	// Invert the horizontal strip in each channel
	for (i = 0; i < channels; i++)
	{
		ROI temp_strip = roi;

		if (i > 0)
		{
			temp_strip.width >>= 1;
		}

		InvertHorizontalStrip16sToRow16u(lowpass_band[i], lowpass_pitch[i],
										 highpass_band[i], highpass_pitch[i],
										 (PIXEL16U *)sptr2, local_pitch, temp_strip,
										 precision);
		plane_array[i] = (uint8_t *)sptr2;
		plane_pitch[i] = local_pitch;

		// Move the buffer allocation pointer to the next row
		sptr2 += temp_strip.width*2*2;
	}

	// Convert the strip to the output format
	for (i = 0; i < roi.height; i++)
	{
		int flags = ACTIVEMETADATA_PLANAR;		// Unpacked rows of YUV422
		int white_bit_depth = 16;

		//convert to YUV444
		ChannelYUYV16toPlanarYUV16((unsigned short **)plane_array, (PIXEL16U *)sptr, output_strip.width, color_space);
		if(LUTYUV(decoder->frame.format))
			flags |= ACTIVEMETADATA_COLORFORMATDONE;  // leave in YUV
		else
			PlanarYUV16toPlanarRGB16((PIXEL16U *)sptr, (PIXEL16U *)sptr, output_strip.width, color_space);

		ConvertLinesToOutput(decoder, output_strip.width, 1, 1, (PIXEL16U *)sptr,
					output_row_ptr, output_pitch, decoder->frame.format,
					white_bit_depth, flags);

		plane_array[0] += plane_pitch[0];
		plane_array[1] += plane_pitch[1];
		plane_array[2] += plane_pitch[2];

		output_row_ptr += output_pitch;
	}
}

// Apply the inverse horizontal transform to reconstruct a strip of rows into packed YUV pixels
void InvertHorizontalYUVStrip16sToYUVOutput(HorizontalFilterParams)
{
	int i;
	int channels = decoder->codec.num_channels;
	//uint8_t *chroma_buffer = output_buffer;
	PIXEL *plane_array[TRANSFORM_MAX_CHANNELS] = {0};
	int plane_pitch[TRANSFORM_MAX_CHANNELS] = {0};

	uint8_t *output_row_ptr = output_image;

	// Scratch buffer for the reconstructed rows allocated on the stack
	//unsigned short scanline2[6 * 2048];
	
	void *scratch = NULL;
	size_t scratchsize = 0;
	int local_pitch = roi.width * 2 * 2 * 2;
	uint8_t *sptr;
	uint8_t *sptr2;
	ROI output_strip = roi;

	scratch = decoder->threads_buffer[thread_index];
	scratchsize = decoder->threads_buffer_size;
	if((int)scratchsize < local_pitch)
	{
		assert(0);
		return;
	}


	// Two rows of 4:2:2 YUV components, two components per pixel, each component is 16 bits
	output_strip.width *= 2;

	// Pointer for allocating row buffers that are 16 byte aligned
	//sptr = (uint8_t *)&scanline2[0];
	sptr = (uint8_t *)scratch;
	sptr = sptr2 = (uint8_t *)((((uintptr_t)sptr)+15) & ~0x0F);

	if(format == COLOR_FORMAT_V210 || format == COLOR_FORMAT_YU64) //DAN20081222 //works
	{

		// Invert the horizontal strip in each channel
		for (i = 0; i < channels; i++)
		{
			ROI temp_strip = roi;

			if (i > 0)
			{
				temp_strip.width >>= 1;
			}

			InvertHorizontalStrip16sToRow16u(lowpass_band[i], lowpass_pitch[i],
											highpass_band[i], highpass_pitch[i],
											(PIXEL16U *)sptr2, local_pitch, temp_strip,
											precision);
			plane_array[i] = (PIXEL *)sptr2;
			plane_pitch[i] = local_pitch;

			// Move the buffer allocation pointer to the next row
			sptr2 += temp_strip.width*2*2;
		}

		// Convert the strip to the output format
		for (i = 0; i < roi.height; i++)
		{
//			int flags = ACTIVEMETADATA_PLANAR;		// Unpacked rows of YUV
//			int whitebitdepth = 16;

			ROI new_strip = output_strip;
			new_strip.height = 1;

			ConvertYUVStripPlanarToV210(plane_array, plane_pitch, new_strip,
				output_row_ptr, output_pitch, new_strip.width, format, decoder->frame.colorspace, 16);

			plane_array[0] += plane_pitch[0]/sizeof(PIXEL);
			plane_array[1] += plane_pitch[1]/sizeof(PIXEL);
			plane_array[2] += plane_pitch[2]/sizeof(PIXEL);

			output_row_ptr += output_pitch;
		}
	}
	else
	{
		assert(0);
	}
}

// Used in RT YUYV playback
// Apply the inverse horizontal transform to reconstruct a strip of rows into packed YUV pixels
void InvertHorizontalStrip16sToBayerYUV(HorizontalFilterParams)
{
	int num_channels = CODEC_NUM_CHANNELS;
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *gg_lowpass_ptr = lowpass_band[0];
	PIXEL *rg_lowpass_ptr = lowpass_band[1];
	PIXEL *bg_lowpass_ptr = lowpass_band[2];
	PIXEL *gg_highpass_ptr = highpass_band[0];
	PIXEL *rg_highpass_ptr = highpass_band[1];
	PIXEL *bg_highpass_ptr = highpass_band[2];

	uint8_t *output = output_image;

	// Process 8 luma coefficients per loop iteration
	const int column_step = 8;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;
	int post_column = last_column - (last_column % column_step);

	// Need at least four luma values of border processing up to the last column
	//const int post_border = 2;

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	int descale_shift = (precision - 8);

	int shift = 8;

	//int y_rmult,y_gmult,y_bmult,y_offset;
	//int u_rmult,u_gmult,u_bmult,u_offset;
	//int v_rmult,v_gmult,v_bmult,v_offset;

	float fy_rmult,fy_gmult,fy_bmult,fy_offset;
	float fu_rmult,fu_gmult,fu_bmult,fu_offset;
	float fv_rmult,fv_gmult,fv_bmult,fv_offset;

	float fr_rmult,fr_gmult,fr_bmult;
	float fg_rmult,fg_gmult,fg_bmult;
	float fb_rmult,fb_gmult,fb_bmult;

	//int y_rmult_sign,y_gmult_sign,y_bmult_sign;
	//int u_rmult_sign,u_gmult_sign,u_bmult_sign;
	//int v_rmult_sign,v_gmult_sign,v_bmult_sign;

	//int color_space = decoder->frame.colorspace;
	int matrix_non_unity = 0;

	float rgb2yuv[3][4] =
	{
        {0.183f, 0.614f, 0.062f, 16.0f/255.0f},
        {-0.101f,-0.338f, 0.439f, 128.0f/255.0f},
        {0.439f,-0.399f,-0.040f, 128.0f/255.0f}
	};

	// this can't be a static (lines of cross talk between two decoders using different color matrices)
	float mtrx[3][4] =
	{
        {1.0f,  0,   0,   0},
        {0,  1.0f,   0,   0},
        {0,    0, 1.0f,   0}
	};

	//3560
	/*	float mtrx[3][4] =
	{
		 1.60,   -0.38,   -0.22,    0,
		-0.45,    2.31,   -0.86,    0,
		-0.35,   -0.24,    1.59,    0
	};*/

/*	//3570
	float mtrx[3][4] =
	{
		 1.095,   0.405,    -0.500,    0,
		-0.491,   2.087,    -0.596,    0,
		-0.179,  -0.994,     2.173,    0
	};
*/


	float scale;

	// We use 16-bit fixed-point arithmetic to approximate the color conversion coefficients
	//
	// Floating point arithmetic is
	//
	// Y  = 0.257 * R + 0.504 * G + 0.098 * B + 16.5;
	// Cb =-0.148 * R - 0.291 * G + 0.439 * B + 128.5;
	// Cr = 0.439 * R - 0.368 * G - 0.071 * B + 128.5;
	//
	// Fixed point approximation (8-bit) is
	//
	// Y  = ( 66 * R + 129 * G +  25 * B +  4224) >> 8;
	// Cb = (-38 * R -  74 * G + 112 * B + 32896) >> 8;
	// Cr = (112 * R -  94 * G -  18 * B + 32896) >> 8;

	// 601 video systems
	// Y = 0.299R + 0.587G + 0.114B
	// Cb = -0.172R - 0.339G + 0.511B + 128
	// Cr = 0.511R - 0.428G - 0.083B + 128

	// 709 video systems
	// Y = 0.213R + 0.715G + 0.072B
	// Cb = -0.117R - 0.394G + 0.511B + 128
	// Cr = 0.511R - 0.464G - 0.047B + 128

	// 709 sRGB
	// Y = 0.183R + 0.614G + 0.062B + 16
	// Cb = -0.101R - 0.338G + 0.439B + 128
	// Cr = 0.439R - 0.399G - 0.040B + 128

#if MMXSUPPORTED //TODO DANREMOVE
	//_mm_empty();
#endif
/*
	scale = 256.0; //TODO We can;t multiple the matrices together as the R' = aR+bG+cB muast be saturated, so all this must change.
	y_rmult = (int)((rgb2yuv[0][0]*mtrx[0][0] + rgb2yuv[1][0]*mtrx[0][1] + rgb2yuv[2][0]*mtrx[0][2]) * scale);
	y_gmult = (int)((rgb2yuv[0][1]*mtrx[0][0] + rgb2yuv[1][1]*mtrx[0][1] + rgb2yuv[2][1]*mtrx[0][2]) * scale);
	y_bmult = (int)((rgb2yuv[0][2]*mtrx[0][0] + rgb2yuv[1][2]*mtrx[0][1] + rgb2yuv[2][2]*mtrx[0][2]) * scale);
	y_offset= (int)((rgb2yuv[0][3]*mtrx[0][0] + rgb2yuv[1][3]*mtrx[0][1] + rgb2yuv[2][3]*mtrx[0][2]) * 65536.0);

	u_rmult = (int)((rgb2yuv[0][0]*mtrx[1][0] + rgb2yuv[1][0]*mtrx[1][1] + rgb2yuv[2][0]*mtrx[1][2]) * scale);
	u_gmult = (int)((rgb2yuv[0][1]*mtrx[1][0] + rgb2yuv[1][1]*mtrx[1][1] + rgb2yuv[2][1]*mtrx[1][2]) * scale);
	u_bmult = (int)((rgb2yuv[0][2]*mtrx[1][0] + rgb2yuv[1][2]*mtrx[1][1] + rgb2yuv[2][2]*mtrx[1][2]) * scale);
	u_offset= (int)((rgb2yuv[0][3]*mtrx[1][0] + rgb2yuv[1][3]*mtrx[1][1] + rgb2yuv[2][3]*mtrx[1][2]) * 65536.0);

	v_rmult = (int)((rgb2yuv[0][0]*mtrx[2][0] + rgb2yuv[1][0]*mtrx[2][1] + rgb2yuv[2][0]*mtrx[2][2]) * scale);
	v_gmult = (int)((rgb2yuv[0][1]*mtrx[2][0] + rgb2yuv[1][1]*mtrx[2][1] + rgb2yuv[2][1]*mtrx[2][2]) * scale);
	v_bmult = (int)((rgb2yuv[0][2]*mtrx[2][0] + rgb2yuv[1][2]*mtrx[2][1] + rgb2yuv[2][2]*mtrx[2][2]) * scale);
	v_offset= (int)((rgb2yuv[0][3]*mtrx[2][0] + rgb2yuv[1][3]*mtrx[2][1] + rgb2yuv[2][3]*mtrx[2][2]) * 65536.0);
	*/
	scale = 64.0;
/*	fy_rmult = ((rgb2yuv[0][0]*mtrx[0][0] + rgb2yuv[1][0]*mtrx[0][1] + rgb2yuv[2][0]*mtrx[0][2]) * scale);
	fy_gmult = ((rgb2yuv[0][1]*mtrx[0][0] + rgb2yuv[1][1]*mtrx[0][1] + rgb2yuv[2][1]*mtrx[0][2]) * scale);
	fy_bmult = ((rgb2yuv[0][2]*mtrx[0][0] + rgb2yuv[1][2]*mtrx[0][1] + rgb2yuv[2][2]*mtrx[0][2]) * scale);
	fy_offset= ((rgb2yuv[0][3]*mtrx[0][0] + rgb2yuv[1][3]*mtrx[0][1] + rgb2yuv[2][3]*mtrx[0][2]) * 16384.0);

	fu_rmult = ((rgb2yuv[0][0]*mtrx[1][0] + rgb2yuv[1][0]*mtrx[1][1] + rgb2yuv[2][0]*mtrx[1][2]) * scale);
	fu_gmult = ((rgb2yuv[0][1]*mtrx[1][0] + rgb2yuv[1][1]*mtrx[1][1] + rgb2yuv[2][1]*mtrx[1][2]) * scale);
	fu_bmult = ((rgb2yuv[0][2]*mtrx[1][0] + rgb2yuv[1][2]*mtrx[1][1] + rgb2yuv[2][2]*mtrx[1][2]) * scale);
	fu_offset= ((rgb2yuv[0][3]*mtrx[1][0] + rgb2yuv[1][3]*mtrx[1][1] + rgb2yuv[2][3]*mtrx[1][2]) * 16384.0);

	fv_rmult = ((rgb2yuv[0][0]*mtrx[2][0] + rgb2yuv[1][0]*mtrx[2][1] + rgb2yuv[2][0]*mtrx[2][2]) * scale);
	fv_gmult = ((rgb2yuv[0][1]*mtrx[2][0] + rgb2yuv[1][1]*mtrx[2][1] + rgb2yuv[2][1]*mtrx[2][2]) * scale);
	fv_bmult = ((rgb2yuv[0][2]*mtrx[2][0] + rgb2yuv[1][2]*mtrx[2][1] + rgb2yuv[2][2]*mtrx[2][2]) * scale);
	fv_offset= ((rgb2yuv[0][3]*mtrx[2][0] + rgb2yuv[1][3]*mtrx[2][1] + rgb2yuv[2][3]*mtrx[2][2]) * 16384.0);
*/

/*
	if(decoder->cfhddata.MagicNumber == CFHDDATA_MAGIC_NUMBER && decoder->cfhddata.version >= 2)
	{
		float fval = 0.0;
		int i;

#if 0 // Matrix disabled as it can only be correct handled by the 3D LUT due to the required linear conversions
		for(i=0; i<12; i++)
		{
			mtrx[i>>2][i&3] = fval = decoder->cfhddata.colormatrix[i>>2][i&3];

			if((i>>2) == (i&3))
			{
				if(fval != 1.0)
				{
					matrix_non_unity = 1;
				}
			}
			else
			{
				if(fval != 0.0)
				{
					matrix_non_unity = 1;
				}
			}
		}
#endif
	}
	*/

	fy_rmult = ((rgb2yuv[0][0]) * scale);
	fy_gmult = ((rgb2yuv[0][1]) * scale);
	fy_bmult = ((rgb2yuv[0][2]) * scale);
	fy_offset= ((rgb2yuv[0][3]) * 16384.0f);

	fu_rmult = ((rgb2yuv[1][0]) * scale);
	fu_gmult = ((rgb2yuv[1][1]) * scale);
	fu_bmult = ((rgb2yuv[1][2]) * scale);
	fu_offset= ((rgb2yuv[1][3]) * 16384.0f);

	fv_rmult = ((rgb2yuv[2][0]) * scale);
	fv_gmult = ((rgb2yuv[2][1]) * scale);
	fv_bmult = ((rgb2yuv[2][2]) * scale);
	fv_offset= ((rgb2yuv[2][3]) * 16384.0f);


	fr_rmult= (mtrx[0][0]);
	fr_gmult= (mtrx[0][1]);
	fr_bmult= (mtrx[0][2]);

	fg_rmult= (mtrx[1][0]);
	fg_gmult= (mtrx[1][1]);
	fg_bmult= (mtrx[1][2]);

	fb_rmult= (mtrx[2][0]);
	fb_gmult= (mtrx[2][1]);
	fb_bmult= (mtrx[2][2]);

/*	y_rmult_sign = (abs(y_rmult) != y_rmult);
	y_gmult_sign = (abs(y_gmult) != y_gmult);
	y_bmult_sign = (abs(y_bmult) != y_bmult);

	u_rmult_sign = (abs(u_rmult) != u_rmult);
	u_gmult_sign = (abs(u_gmult) != u_gmult);
	u_bmult_sign = (abs(u_bmult) != u_bmult);

	v_rmult_sign = (abs(v_rmult) != v_rmult);
	v_gmult_sign = (abs(v_gmult) != v_gmult);
	v_bmult_sign = (abs(v_bmult) != v_bmult);

	y_rmult = abs(y_rmult);
	y_gmult = abs(y_gmult);
	y_bmult = abs(y_bmult);
	y_offset= abs(y_offset);

	u_rmult = abs(u_rmult);
	u_gmult = abs(u_gmult);
	u_bmult = abs(u_bmult);
	u_offset= abs(u_offset);

	v_rmult = abs(v_rmult);
	v_gmult = abs(v_gmult);
	v_bmult = abs(v_bmult);
	v_offset= abs(v_offset);

	y_offset>>=2;
	u_offset>>=2;
	v_offset>>=2;*/
	shift-=2;


	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);

	// Adjust the end of the fast loop if necessary for border processing
	//if (post_column > (last_column - post_border))  // DAN08112004 - ignore end row calc -- use SSE to edge.
	//	post_column -= column_step;

	// Check that there is enough margin to accommodate border processing
//	assert(post_column <= (last_column - post_border));

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
#if (1 && XMMOPT)
		__m128i gg_low1_epi16;		// Lowpass coefficients
		__m128i gg_low2_epi16;
		__m128i bg_low1_epi16;
		__m128i bg_low2_epi16;
		__m128i rg_low1_epi16;
		__m128i rg_low2_epi16;

		__m128i gg_high1_epi16;		// Highpass coefficients
		__m128i gg_high2_epi16;
		__m128i bg_high1_epi16;
		__m128i bg_high2_epi16;
		__m128i rg_high1_epi16;
		__m128i rg_high2_epi16;

		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];

		const __m128i mask_epi32 = _mm_set1_epi32(0xffff);
		const __m128i value128_epi32 = _mm_set1_epi16(128);

		__m128i limiterRGB = _mm_set1_epi16(0x7fff - 0x00ff);
		__m128i limiter = _mm_set1_epi16(0x7fff - 0x3fff);

#endif
		uint8_t *colptr = (uint8_t *)&output[0];

		int32_t gg_even_value;
		int32_t bg_even_value;
		int32_t rg_even_value;
		int32_t gg_odd_value;
		int32_t bg_odd_value;
		int32_t rg_odd_value;


		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two luma output points with special filters for the left border
		int32_t even = 0;
		int32_t odd = 0;

		__m128i rounding1_epi16 = _mm_set1_epi16(0); // for 6bit matm
		__m128i rounding2_epi16 = _mm_set1_epi16(0); // for 6bit matm

		if(descale_shift>=2)
		{
			int mask = (1<<(descale_shift-1))-1; //DAN20090601
			rounding1_epi16 = _mm_insert_epi16(rounding1_epi16, rand()&mask, 0);
			rounding1_epi16 = _mm_insert_epi16(rounding1_epi16, rand()&mask, 1);
			rounding1_epi16 = _mm_insert_epi16(rounding1_epi16, rand()&mask, 2);
			rounding1_epi16 = _mm_insert_epi16(rounding1_epi16, rand()&mask, 3);
			rounding1_epi16 = _mm_insert_epi16(rounding1_epi16, rand()&mask, 4);
			rounding1_epi16 = _mm_insert_epi16(rounding1_epi16, rand()&mask, 5);
			rounding1_epi16 = _mm_insert_epi16(rounding1_epi16, rand()&mask, 6);
			rounding1_epi16 = _mm_insert_epi16(rounding1_epi16, rand()&mask, 7);

			rounding2_epi16 = _mm_insert_epi16(rounding2_epi16, rand()&mask, 0);
			rounding2_epi16 = _mm_insert_epi16(rounding2_epi16, rand()&mask, 1);
			rounding2_epi16 = _mm_insert_epi16(rounding2_epi16, rand()&mask, 2);
			rounding2_epi16 = _mm_insert_epi16(rounding2_epi16, rand()&mask, 3);
			rounding2_epi16 = _mm_insert_epi16(rounding2_epi16, rand()&mask, 4);
			rounding2_epi16 = _mm_insert_epi16(rounding2_epi16, rand()&mask, 5);
			rounding2_epi16 = _mm_insert_epi16(rounding2_epi16, rand()&mask, 6);
			rounding2_epi16 = _mm_insert_epi16(rounding2_epi16, rand()&mask, 7);

			rounding1_epi16 = _mm_adds_epi16(rounding1_epi16, _mm_set1_epi16(10*mask/32)); //DAN20090601
			rounding2_epi16 = _mm_adds_epi16(rounding2_epi16, _mm_set1_epi16(10*mask/32));
		}




		// Apply the even reconstruction filter to the lowpass band
		even += 11 * gg_lowpass_ptr[column + 0];
		even -=  4 * gg_lowpass_ptr[column + 1];
		even +=  1 * gg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += gg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		gg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * gg_lowpass_ptr[column + 0];
		odd += 4 * gg_lowpass_ptr[column + 1];
		odd -= 1 * gg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= gg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		gg_odd_value = odd;


		// Process the first two u chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * bg_lowpass_ptr[column + 0];
		even -=  4 * bg_lowpass_ptr[column + 1];
		even +=  1 * bg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += bg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		bg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * bg_lowpass_ptr[column + 0];
		odd += 4 * bg_lowpass_ptr[column + 1];
		odd -= 1 * bg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= bg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		bg_odd_value = odd;


		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * rg_lowpass_ptr[column + 0];
		even -=  4 * rg_lowpass_ptr[column + 1];
		even +=  1 * rg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += rg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		rg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * rg_lowpass_ptr[column + 0];
		odd += 4 * rg_lowpass_ptr[column + 1];
		odd -= 1 * rg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= rg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		rg_odd_value = odd;

#if (1 && XMMOPT)

		// Preload the first eight lowpass luma coefficients
		gg_low1_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[0]);

		// Preload the first eight highpass luma coefficients
		gg_high1_epi16 = _mm_load_si128((__m128i *)&gg_highpass_ptr[0]);

		// Preload the first eight lowpass u chroma coefficients
		bg_low1_epi16 = _mm_load_si128((__m128i *)&bg_lowpass_ptr[0]);

		// Preload the first eight highpass u chroma coefficients
		bg_high1_epi16 = _mm_load_si128((__m128i *)&bg_highpass_ptr[0]);

		// Preload the first eight lowpass v chroma coefficients
		rg_low1_epi16 = _mm_load_si128((__m128i *)&rg_lowpass_ptr[0]);

		// Preload the first eight highpass v chroma coefficients
 		rg_high1_epi16 = _mm_load_si128((__m128i *)&rg_highpass_ptr[0]);


		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i g1_output_epi16;
			__m128i g2_output_epi16;
			__m128i r1_output_epi16;
			__m128i r2_output_epi16;
			__m128i b1_output_epi16;
			__m128i b2_output_epi16;

			__m128i gg1_output_epi16;
			__m128i gg2_output_epi16;
			__m128i rg1_output_epi16;
			__m128i rg2_output_epi16;
			__m128i bg1_output_epi16;
			__m128i bg2_output_epi16;

			__m128i y1_output_epi16;
			__m128i y2_output_epi16;
			__m128i u1_output_epi16;
			__m128i u2_output_epi16;
			__m128i v1_output_epi16;
			__m128i v2_output_epi16;

			__m128i urg_epi16;
			__m128i yuv1_epi16;
			__m128i yuv2_epi16;
			__m128i yuv1_epi8;
			__m128i yuv2_epi8;

			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i temp_epi32;
			__m128i tempB_epi32;
			__m128i rgb_epi32;
			__m128i zero_epi128;
			__m128  temp_ps;
			__m128  rgb_ps;
			__m128	y1a_ps;
			__m128	y1b_ps;
			__m128	u1a_ps;
			__m128	u1b_ps;
			__m128	v1a_ps;
			__m128	v1b_ps;


			__m128i out_epi16;		// Reconstructed data
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			__m128i low1_epi16;
			__m128i low2_epi16;
			__m128i high1_epi16;
			__m128i high2_epi16;

			//DAN031304 -- correct inverse filter
			__m128i half_epi16 = _mm_set1_epi16(4);
			__m128i offset_epi16 = _mm_set1_epi16(2048);

			uint32_t temp;		// Temporary register for last two values


			/***** Compute the first eight luma output values *****/

			// Preload the second eight lowpass coefficients
			gg_low2_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			gg_high2_epi16 = _mm_load_si128((__m128i *)&gg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = gg_low1_epi16;
			high1_epi16 = gg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);


			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_epi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_odd_value, 1);

			// Save the eight luma values for packing later
			gg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			gg_even_value = (short)temp;
			gg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = gg_low2_epi16;
			high2_epi16 = gg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_epi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_odd_value, 1);

			// Save the eight luma values for packing later
			gg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			gg_even_value = (short)temp;
			gg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are used later in the loop
			gg_low1_epi16 = gg_low2_epi16;

			// The second eight highpass coefficients are used later in the loop
			gg_high1_epi16 = gg_high2_epi16;


			/***** Compute the first eight u chroma output values *****/

			// Preload the second eight lowpass coefficients
			bg_low2_epi16 = _mm_load_si128((__m128i *)&bg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			bg_high2_epi16 = _mm_load_si128((__m128i *)&bg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = bg_low1_epi16;
			high1_epi16 = bg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			bg_even_value = (short)temp;
			bg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight u chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = bg_low2_epi16;
			high2_epi16 = bg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			bg_even_value = (short)temp;
			bg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			bg_low1_epi16 = bg_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			bg_high1_epi16 = bg_high2_epi16;



			/***** Compute the first eight v chroma output values *****/

			// Preload the second eight lowpass coefficients
			rg_low2_epi16 = _mm_load_si128((__m128i *)&rg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			rg_high2_epi16 = _mm_load_si128((__m128i *)&rg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = rg_low1_epi16;
			high1_epi16 = rg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			rg_even_value = (short)temp;
			rg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight v chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = rg_low2_epi16;
			high2_epi16 = rg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			rg_even_value = (short)temp;
			rg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			rg_low1_epi16 = rg_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			rg_high1_epi16 = rg_high2_epi16;












			//r_output_epi16  = ((rg_output_epi16 - 32768)<<1)+gg_output_epi16
			//r_output_epi16  = ((rg_output_epi16>>3 - 32768>>3))+gg_output_epi16>>4


			g1_output_epi16 = gg1_output_epi16;

			r1_output_epi16 = rg1_output_epi16;//_mm_srli_epi16(rg1_output_epi16,2);
			r1_output_epi16 = _mm_subs_epi16(r1_output_epi16, value128_epi32);
			r1_output_epi16 = _mm_slli_epi16(r1_output_epi16,1);
			r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, g1_output_epi16);

			b1_output_epi16 = bg1_output_epi16;//_mm_srli_epi16(bg1_output_epi16,2);
			b1_output_epi16 = _mm_subs_epi16(b1_output_epi16, value128_epi32);
			b1_output_epi16 = _mm_slli_epi16(b1_output_epi16,1);
			b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, g1_output_epi16);


			 r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, limiterRGB);
			 r1_output_epi16 = _mm_subs_epu16(r1_output_epi16, limiterRGB);

			 g1_output_epi16 = _mm_adds_epi16(g1_output_epi16, limiterRGB);
			 g1_output_epi16 = _mm_subs_epu16(g1_output_epi16, limiterRGB);

			 b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, limiterRGB);
			 b1_output_epi16 = _mm_subs_epu16(b1_output_epi16, limiterRGB);


			zero_epi128 = _mm_setzero_si128();



			// Compute R'G'B'
			if(matrix_non_unity)
			{
				rgb_epi32 = _mm_unpacklo_epi16(r1_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				y1a_ps = _mm_mul_ps(_mm_set_ps1(fr_rmult), rgb_ps);
				u1a_ps = _mm_mul_ps(_mm_set_ps1(fg_rmult), rgb_ps);
				v1a_ps = _mm_mul_ps(_mm_set_ps1(fb_rmult), rgb_ps);
				rgb_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				y1b_ps = _mm_mul_ps(_mm_set_ps1(fr_rmult), rgb_ps);
				u1b_ps = _mm_mul_ps(_mm_set_ps1(fg_rmult), rgb_ps);
				v1b_ps = _mm_mul_ps(_mm_set_ps1(fb_rmult), rgb_ps);

				rgb_epi32 = _mm_unpacklo_epi16(g1_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fr_gmult), rgb_ps);
				y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fg_gmult), rgb_ps);
				u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fb_gmult), rgb_ps);
				v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
				rgb_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fr_gmult), rgb_ps);
				y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fg_gmult), rgb_ps);
				u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fb_gmult), rgb_ps);
				v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

				rgb_epi32 = _mm_unpacklo_epi16(b1_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fr_bmult), rgb_ps);
				y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fg_bmult), rgb_ps);
				u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fb_bmult), rgb_ps);
				v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
				rgb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fr_bmult), rgb_ps);
				y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fg_bmult), rgb_ps);
				u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fb_bmult), rgb_ps);
				v1b_ps = _mm_add_ps(v1b_ps, temp_ps);


				temp_epi32 = _mm_cvtps_epi32(y1a_ps);
				tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
				r1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
				r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, limiterRGB);
				r1_output_epi16 = _mm_subs_epu16(r1_output_epi16, limiterRGB);

				temp_epi32 = _mm_cvtps_epi32(u1a_ps);
				tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
				g1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
				g1_output_epi16 = _mm_adds_epi16(g1_output_epi16, limiterRGB);
				g1_output_epi16 = _mm_subs_epu16(g1_output_epi16, limiterRGB);

				temp_epi32 = _mm_cvtps_epi32(v1a_ps);
				tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
				b1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
				b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, limiterRGB);
				b1_output_epi16 = _mm_subs_epu16(b1_output_epi16, limiterRGB);
			}

			// Compute Y,U,V
			rgb_epi32 = _mm_unpacklo_epi16(r1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1a_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1a_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1a_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);
			rgb_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1b_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1b_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1b_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);

			rgb_epi32 = _mm_unpacklo_epi16(g1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			rgb_epi32 = _mm_unpacklo_epi16(b1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_ps = _mm_set_ps1(fy_offset);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fu_offset);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fv_offset);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_epi32 = _mm_cvtps_epi32(y1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
			y1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			y1_output_epi16 = _mm_adds_epi16(y1_output_epi16, limiter);
			y1_output_epi16 = _mm_subs_epu16(y1_output_epi16, limiter);
			y1_output_epi16 = _mm_srli_epi16(y1_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(u1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
			u1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			u1_output_epi16 = _mm_adds_epi16(u1_output_epi16, limiter);
			u1_output_epi16 = _mm_subs_epu16(u1_output_epi16, limiter);
			u1_output_epi16 = _mm_srli_epi16(u1_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(v1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
			v1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			v1_output_epi16 = _mm_adds_epi16(v1_output_epi16, limiter);
			v1_output_epi16 = _mm_subs_epu16(v1_output_epi16, limiter);
			v1_output_epi16 = _mm_srli_epi16(v1_output_epi16, shift);






			g2_output_epi16 = gg2_output_epi16;//_mm_srli_epi16(gg2_output_epi16,2);

			r2_output_epi16 = rg2_output_epi16;//_mm_srli_epi16(rg2_output_epi16,2);
			r2_output_epi16 = _mm_subs_epi16(r2_output_epi16, value128_epi32);
			r2_output_epi16 = _mm_slli_epi16(r2_output_epi16,1);
			r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, g2_output_epi16);

			b2_output_epi16 = bg2_output_epi16;//_mm_srli_epi16(bg2_output_epi16,2);
			b2_output_epi16 = _mm_subs_epi16(b2_output_epi16, value128_epi32);
			b2_output_epi16 = _mm_slli_epi16(b2_output_epi16,1);
			b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, g2_output_epi16);



			 r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, limiterRGB);
			 r2_output_epi16 = _mm_subs_epu16(r2_output_epi16, limiterRGB);

			 g2_output_epi16 = _mm_adds_epi16(g2_output_epi16, limiterRGB);
			 g2_output_epi16 = _mm_subs_epu16(g2_output_epi16, limiterRGB);

			 b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, limiterRGB);
			 b2_output_epi16 = _mm_subs_epu16(b2_output_epi16, limiterRGB);


			 // Compute R'G'B'
			if(matrix_non_unity)
			{
				rgb_epi32 = _mm_unpacklo_epi16(r2_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				y1a_ps = _mm_mul_ps(_mm_set_ps1(fr_rmult), rgb_ps);
				u1a_ps = _mm_mul_ps(_mm_set_ps1(fg_rmult), rgb_ps);
				v1a_ps = _mm_mul_ps(_mm_set_ps1(fb_rmult), rgb_ps);
				rgb_epi32 = _mm_unpackhi_epi16(r2_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				y1b_ps = _mm_mul_ps(_mm_set_ps1(fr_rmult), rgb_ps);
				u1b_ps = _mm_mul_ps(_mm_set_ps1(fg_rmult), rgb_ps);
				v1b_ps = _mm_mul_ps(_mm_set_ps1(fb_rmult), rgb_ps);

				rgb_epi32 = _mm_unpacklo_epi16(g2_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fr_gmult), rgb_ps);
				y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fg_gmult), rgb_ps);
				u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fb_gmult), rgb_ps);
				v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
				rgb_epi32 = _mm_unpackhi_epi16(g2_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fr_gmult), rgb_ps);
				y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fg_gmult), rgb_ps);
				u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fb_gmult), rgb_ps);
				v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

				rgb_epi32 = _mm_unpacklo_epi16(b2_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fr_bmult), rgb_ps);
				y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fg_bmult), rgb_ps);
				u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fb_bmult), rgb_ps);
				v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
				rgb_epi32 = _mm_unpackhi_epi16(b2_output_epi16, zero_epi128);
				rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fr_bmult), rgb_ps);
				y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fg_bmult), rgb_ps);
				u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
				temp_ps = _mm_mul_ps(_mm_set_ps1(fb_bmult), rgb_ps);
				v1b_ps = _mm_add_ps(v1b_ps, temp_ps);


				temp_epi32 = _mm_cvtps_epi32(y1a_ps);
				tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
				r2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
				r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, limiterRGB);
				r2_output_epi16 = _mm_subs_epu16(r2_output_epi16, limiterRGB);

				temp_epi32 = _mm_cvtps_epi32(u1a_ps);
				tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
				g2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
				g2_output_epi16 = _mm_adds_epi16(g2_output_epi16, limiterRGB);
				g2_output_epi16 = _mm_subs_epu16(g2_output_epi16, limiterRGB);

				temp_epi32 = _mm_cvtps_epi32(v1a_ps);
				tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
				b2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
				b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, limiterRGB);
				b2_output_epi16 = _mm_subs_epu16(b2_output_epi16, limiterRGB);
			}


			// Compute Y,U,V
			rgb_epi32 = _mm_unpacklo_epi16(r2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1a_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1a_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1a_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);
			rgb_epi32 = _mm_unpackhi_epi16(r2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1b_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1b_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1b_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);

			rgb_epi32 = _mm_unpacklo_epi16(g2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(g2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			rgb_epi32 = _mm_unpacklo_epi16(b2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(b2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_ps = _mm_set_ps1(fy_offset);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fu_offset);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fv_offset);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_epi32 = _mm_cvtps_epi32(y1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
			y2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			y2_output_epi16 = _mm_adds_epi16(y2_output_epi16, limiter);
			y2_output_epi16 = _mm_subs_epu16(y2_output_epi16, limiter);
			y2_output_epi16 = _mm_srli_epi16(y2_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(u1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
			u2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			u2_output_epi16 = _mm_adds_epi16(u2_output_epi16, limiter);
			u2_output_epi16 = _mm_subs_epu16(u2_output_epi16, limiter);
			u2_output_epi16 = _mm_srli_epi16(u2_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(v1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
			v2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			v2_output_epi16 = _mm_adds_epi16(v2_output_epi16, limiter);
			v2_output_epi16 = _mm_subs_epu16(v2_output_epi16, limiter);
			v2_output_epi16 = _mm_srli_epi16(v2_output_epi16, shift);


			// 4:4:4 to 4:2:2
			temp_epi16 = _mm_srli_si128(u1_output_epi16, 2);
			u1_output_epi16 = _mm_adds_epi16(u1_output_epi16, temp_epi16);
			u1_output_epi16 = _mm_srli_epi16(u1_output_epi16, 1);
			temp_epi16 = _mm_srli_si128(u2_output_epi16, 2);
			u2_output_epi16 = _mm_adds_epi16(u2_output_epi16, temp_epi16);
			u2_output_epi16 = _mm_srli_epi16(u2_output_epi16, 1);
			temp_epi16 = _mm_srli_si128(v1_output_epi16, 2);
			v1_output_epi16 = _mm_adds_epi16(v1_output_epi16, temp_epi16);
			v1_output_epi16 = _mm_srli_epi16(v1_output_epi16, 1);
			temp_epi16 = _mm_srli_si128(v2_output_epi16, 2);
			v2_output_epi16 = _mm_adds_epi16(v2_output_epi16, temp_epi16);
			v2_output_epi16 = _mm_srli_epi16(v2_output_epi16, 1);
			u1_output_epi16 = _mm_and_si128(u1_output_epi16, mask_epi32);
			u2_output_epi16 = _mm_and_si128(u2_output_epi16, mask_epi32);
			v1_output_epi16 = _mm_and_si128(v1_output_epi16, mask_epi32);
			v2_output_epi16 = _mm_and_si128(v2_output_epi16, mask_epi32);

			u1_output_epi16 = _mm_packs_epi32 (u1_output_epi16, u2_output_epi16);
			v1_output_epi16 = _mm_packs_epi32 (v1_output_epi16, v2_output_epi16);


			/***** Interleave the luma and chroma values *****/

			if(format == DECODED_FORMAT_YUYV)
			{
				// Interleave the first four values from each chroma channel
				urg_epi16 = _mm_unpacklo_epi16(u1_output_epi16, v1_output_epi16);
				// Interleave the first eight chroma values with the first eight luma values
				yuv1_epi16 = _mm_unpacklo_epi16(y1_output_epi16, urg_epi16);
				yuv2_epi16 = _mm_unpackhi_epi16(y1_output_epi16, urg_epi16);

				// Pack the first sixteen bytes of luma and chroma
				yuv1_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

				// Store the first sixteen bytes of output values
				_mm_store_si128(outptr++, yuv1_epi8);

				// Interleave the second four values from each chroma channel
				urg_epi16 = _mm_unpackhi_epi16(u1_output_epi16, v1_output_epi16);

				// Interleave the second eight chroma values with the second eight luma values
				yuv1_epi16 = _mm_unpacklo_epi16(y2_output_epi16, urg_epi16);
				yuv2_epi16 = _mm_unpackhi_epi16(y2_output_epi16, urg_epi16);

				// Pack the second sixteen bytes of luma and chroma
				yuv2_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

				// Store the second sixteen bytes of output values
				_mm_store_si128(outptr++, yuv2_epi8);
			}
			else
			{
				// Interleave the first four values from each chroma channel
				urg_epi16 = _mm_unpacklo_epi16(u1_output_epi16, v1_output_epi16);
				// Interleave the first eight chroma values with the first eight luma values
				yuv1_epi16 = _mm_unpacklo_epi16(urg_epi16, y1_output_epi16);
				yuv2_epi16 = _mm_unpackhi_epi16(urg_epi16, y1_output_epi16);

				// Pack the first sixteen bytes of luma and chroma
				yuv1_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

				// Store the first sixteen bytes of output values
				_mm_store_si128(outptr++, yuv1_epi8);

				// Interleave the second four values from each chroma channel
				urg_epi16 = _mm_unpackhi_epi16(u1_output_epi16, v1_output_epi16);

				// Interleave the second eight chroma values with the second eight luma values
				yuv1_epi16 = _mm_unpacklo_epi16(urg_epi16, y2_output_epi16);
				yuv2_epi16 = _mm_unpackhi_epi16(urg_epi16, y2_output_epi16);

				// Pack the second sixteen bytes of luma and chroma
				yuv2_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

				// Store the second sixteen bytes of output values
				_mm_store_si128(outptr++, yuv2_epi8);
			}
		}

		// Should have exited the loop with the column equal to the post processing column
	//	assert(column == post_column);

		colptr = (uint8_t *)outptr;
#endif

//////////////////////////////////////////// *******************************************************************************************************************
//TODO: Non-SSE code not upgraded for Bayer. *******************************************************************************************************************
//////////////////////////////////////////// *******************************************************************************************************************

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column += 2)
		{
			int y1_even_value;
			int y2_even_value;
			int y1_odd_value;
			int y2_odd_value;


			/***** First pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += gg_lowpass_ptr[column - 1];
			even -= gg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += gg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += gg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the luma result for later output in the correct order
			y1_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= gg_lowpass_ptr[column - 1];
			odd += gg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += gg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= gg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the luma result for later output in the correct order
			y1_odd_value = odd;


			/***** Pair of u chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += bg_lowpass_ptr[column - 1];
			even -= bg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += bg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += bg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the u chroma result for later output in the correct order
			bg_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= bg_lowpass_ptr[column - 1];
			odd += bg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += bg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= bg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the u chroma result for later output in the correct order
			bg_odd_value = odd;


			/***** Second pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += gg_lowpass_ptr[column + 0];
			even -= gg_lowpass_ptr[column + 2];
			even += 4; //DAN20050921
			even >>= 3;
			even += gg_lowpass_ptr[column + 1];

			// Add the highpass correction
			even += gg_highpass_ptr[column + 1];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the luma result for later output in the correct order
			y2_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= gg_lowpass_ptr[column + 0];
			odd += gg_lowpass_ptr[column + 2];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += gg_lowpass_ptr[column + 1];

			// Subtract the highpass correction
			odd -= gg_highpass_ptr[column + 1];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the luma result for later output in the correct order
			y2_odd_value = odd;


			/***** Pair of v chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += rg_lowpass_ptr[column - 1];
			even -= rg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += rg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += rg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the v chroma result for later output in the correct order
			rg_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= rg_lowpass_ptr[column - 1];
			odd += rg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += rg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= rg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the v chroma result for later output in the correct order
			rg_odd_value = odd;

			
			if(format == DECODED_FORMAT_YUYV)
			{
				// Output the luma and chroma values in the correct order
				*(colptr++) = SATURATE_8U(y1_even_value);
				*(colptr++) = SATURATE_8U(bg_even_value);
				*(colptr++) = SATURATE_8U(y1_odd_value);
				*(colptr++) = SATURATE_8U(rg_even_value);

				// Need to output the second set of values?
				if ((column + 1) < last_column)
				{
					*(colptr++) = SATURATE_8U(y2_even_value);
					*(colptr++) = SATURATE_8U(bg_odd_value);
					*(colptr++) = SATURATE_8U(y2_odd_value);
					*(colptr++) = SATURATE_8U(rg_odd_value);
				}
				else
				{
					column++;
					break;
				}
			}
			else
			{
				// Output the luma and chroma values in the correct order
				*(colptr++) = SATURATE_8U(bg_even_value);
				*(colptr++) = SATURATE_8U(y1_even_value);
				*(colptr++) = SATURATE_8U(rg_even_value);
				*(colptr++) = SATURATE_8U(y1_odd_value);

				// Need to output the second set of values?
				if ((column + 1) < last_column)
				{
					*(colptr++) = SATURATE_8U(bg_odd_value);
					*(colptr++) = SATURATE_8U(y2_even_value);
					*(colptr++) = SATURATE_8U(rg_odd_value);
					*(colptr++) = SATURATE_8U(y2_odd_value);
				}
				else
				{
					column++;
					break;
				}
			}
		}

		// Should have exited the loop at the column for right border processing
	//	assert(column == last_column);


		column = last_column - 1;
		colptr -= 4;

		// Process the last luma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * gg_lowpass_ptr[column + 0];
		even += 4 * gg_lowpass_ptr[column - 1];
		even -= 1 * gg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += gg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the luma result for later output in the correct order
		gg_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * gg_lowpass_ptr[column + 0];
		odd -=  4 * gg_lowpass_ptr[column - 1];
		odd +=  1 * gg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= gg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the luma result for later output in the correct order
		gg_odd_value = odd;

		// Process the last u chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * bg_lowpass_ptr[column + 0];
		even += 4 * bg_lowpass_ptr[column - 1];
		even -= 1 * bg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += bg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		bg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * bg_lowpass_ptr[column + 0];
		odd -=  4 * bg_lowpass_ptr[column - 1];
		odd +=  1 * bg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= bg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		bg_odd_value = odd;

		// Process the last v chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * rg_lowpass_ptr[column + 0];
		even += 4 * rg_lowpass_ptr[column - 1];
		even -= 1 * rg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += rg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		rg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * rg_lowpass_ptr[column + 0];
		odd -=  4 * rg_lowpass_ptr[column - 1];
		odd +=  1 * rg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= rg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		rg_odd_value = odd;

			
		if(format == DECODED_FORMAT_YUYV)
		{
			//DAN06052005 - Fix for PSNR errors in UV on right edge
			colptr-=4;
			colptr++; // Y fine
			*(colptr++) = SATURATE_8U(bg_even_value);
			colptr++; // Y2 fine
			*(colptr++) = SATURATE_8U(rg_even_value);

			// Output the last luma and chroma values in the correct order
			*(colptr++) = SATURATE_8U(gg_even_value);
			*(colptr++) = SATURATE_8U(bg_odd_value);
			*(colptr++) = SATURATE_8U(gg_odd_value);
			*(colptr++) = SATURATE_8U(rg_odd_value);
		}
		else
		{
			//DAN06052005 - Fix for PSNR errors in UV on right edge
			colptr-=4;
			*(colptr++) = SATURATE_8U(bg_even_value);
			colptr++; // Y fine
			*(colptr++) = SATURATE_8U(rg_even_value);
			colptr++; // Y2 fine

			// Output the last luma and chroma values in the correct order
			*(colptr++) = SATURATE_8U(bg_odd_value);
			*(colptr++) = SATURATE_8U(gg_even_value);
			*(colptr++) = SATURATE_8U(rg_odd_value);
			*(colptr++) = SATURATE_8U(gg_odd_value);
		}

		// Advance to the next row of coefficients in each channel
		gg_lowpass_ptr += lowpass_pitch[0];
		bg_lowpass_ptr += lowpass_pitch[1];
		rg_lowpass_ptr += lowpass_pitch[2];
		gg_highpass_ptr += highpass_pitch[0];
		bg_highpass_ptr += highpass_pitch[1];
		rg_highpass_ptr += highpass_pitch[2];

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}



// Used in RT YUYV playback
// Apply the inverse horizontal transform to reconstruct a strip of rows into packed YUV pixels
void InvertHorizontalStrip16sRGB2YUV(HorizontalFilterParams)
{
	int num_channels = CODEC_NUM_CHANNELS;
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *gg_lowpass_ptr = lowpass_band[0];
	PIXEL *rg_lowpass_ptr = lowpass_band[1];
	PIXEL *bg_lowpass_ptr = lowpass_band[2];
	PIXEL *gg_highpass_ptr = highpass_band[0];
	PIXEL *rg_highpass_ptr = highpass_band[1];
	PIXEL *bg_highpass_ptr = highpass_band[2];

	uint8_t *output = output_image;

	// Process 8 luma coefficients per loop iteration
	const int column_step = 8;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;
	int post_column = last_column - (last_column % column_step);

	// Need at least four luma values of border processing up to the last column
	//const int post_border = 2;

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	int descale_shift = (precision - 8);

	int shift = 8;

	float fy_rmult,fy_gmult,fy_bmult,fy_offset;
	float fu_rmult,fu_gmult,fu_bmult,fu_offset;
	float fv_rmult,fv_gmult,fv_bmult,fv_offset;

	int color_space = decoder->frame.colorspace;

	float rgb2yuv[3][4];
	float scale;

	switch(color_space & COLORSPACE_MASK)
	{
		case COLOR_SPACE_CG_601: //601
			{
			float rgb2yuv601[3][4] =
			{	{0.257f, 0.504f, 0.098f, 16.0f/255.0f},
                {-0.148f,-0.291f, 0.439f, 128.0f/255.0f},
                {0.439f,-0.368f,-0.071f, 128.0f/255.0f}
			};
			memcpy(rgb2yuv,rgb2yuv601, sizeof(rgb2yuv));
			}
			break;
		default: assert(0);
		case COLOR_SPACE_CG_709:
			{
			float rgb2yuv709[3][4] =
			{
                {0.183f, 0.614f, 0.062f, 16.0f/255.0f},
                {-0.101f,-0.338f, 0.439f, 128.0f/255.0f},
                {0.439f,-0.399f,-0.040f, 128.0f/255.0f}
			};
			memcpy(rgb2yuv,rgb2yuv709, sizeof(rgb2yuv));
			}
			break;
		case COLOR_SPACE_VS_601: //VS 601
			{
			float rgb2yuvVS601[3][4] =
			{
                {0.299f,0.587f,0.114f,0},
                {-0.172f,-0.339f,0.511f,128.0f/255.0f},
                {0.511f,-0.428f,-0.083f,128.0f/255.0f}
			};
			memcpy(rgb2yuv,rgb2yuvVS601, sizeof(rgb2yuv));
			}
			break;
		case COLOR_SPACE_VS_709:
			{
			float rgb2yuvVS709[3][4] =
			{
                {0.213f,0.715f,0.072f,0},
                {-0.117f,-0.394f,0.511f,128.0f/255.0f},
                {0.511f,-0.464f,-0.047f,128.0f/255.0f}
			};
			memcpy(rgb2yuv,rgb2yuvVS709, sizeof(rgb2yuv));
			}
			break;
	}



	// We use 16-bit fixed-point arithmetic to approximate the color conversion coefficients
	//
	// Floating point arithmetic is
	//
	// Y  = 0.257 * R + 0.504 * G + 0.098 * B + 16.5;
	// Cb =-0.148 * R - 0.291 * G + 0.439 * B + 128.5;
	// Cr = 0.439 * R - 0.368 * G - 0.071 * B + 128.5;
	//
	// Fixed point approximation (8-bit) is
	//
	// Y  = ( 66 * R + 129 * G +  25 * B +  4224) >> 8;
	// Cb = (-38 * R -  74 * G + 112 * B + 32896) >> 8;
	// Cr = (112 * R -  94 * G -  18 * B + 32896) >> 8;

	// 601 video systems
	// Y = 0.299R + 0.587G + 0.114B
	// Cb = -0.172R - 0.339G + 0.511B + 128
	// Cr = 0.511R - 0.428G - 0.083B + 128

	// 709 video systems
	// Y = 0.213R + 0.715G + 0.072B
	// Cb = -0.117R - 0.394G + 0.511B + 128
	// Cr = 0.511R - 0.464G - 0.047B + 128

	// 709 sRGB
	// Y = 0.183R + 0.614G + 0.062B + 16
	// Cb = -0.101R - 0.338G + 0.439B + 128
	// Cr = 0.439R - 0.399G - 0.040B + 128

	scale = 64.0;

	fy_rmult = ((rgb2yuv[0][0]) * scale);
	fy_gmult = ((rgb2yuv[0][1]) * scale);
	fy_bmult = ((rgb2yuv[0][2]) * scale);
	fy_offset= ((rgb2yuv[0][3]) * 16384.0f);

	fu_rmult = ((rgb2yuv[1][0]) * scale);
	fu_gmult = ((rgb2yuv[1][1]) * scale);
	fu_bmult = ((rgb2yuv[1][2]) * scale);
	fu_offset= ((rgb2yuv[1][3]) * 16384.0f);

	fv_rmult = ((rgb2yuv[2][0]) * scale);
	fv_gmult = ((rgb2yuv[2][1]) * scale);
	fv_bmult = ((rgb2yuv[2][2]) * scale);
	fv_offset= ((rgb2yuv[2][3]) * 16384.0f);

	shift-=2;


	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);

	// Adjust the end of the fast loop if necessary for border processing
	//if (post_column > (last_column - post_border))  // DAN08112004 - ignore end row calc -- use SSE to edge.
	//	post_column -= column_step;

	// Check that there is enough margin to accommodate border processing
//	assert(post_column <= (last_column - post_border));

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
#if (1 && XMMOPT)
		__m128i gg_low1_epi16;		// Lowpass coefficients
		__m128i gg_low2_epi16;
		__m128i bg_low1_epi16;
		__m128i bg_low2_epi16;
		__m128i rg_low1_epi16;
		__m128i rg_low2_epi16;

		__m128i gg_high1_epi16;		// Highpass coefficients
		__m128i gg_high2_epi16;
		__m128i bg_high1_epi16;
		__m128i bg_high2_epi16;
		__m128i rg_high1_epi16;
		__m128i rg_high2_epi16;

		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];

		const __m128i mask_epi32 = _mm_set1_epi32(0xffff);

		__m128i limiterRGB = _mm_set1_epi16(0x7fff - 0x00ff);
		__m128i limiter = _mm_set1_epi16(0x7fff - 0x3fff);

#endif
		uint8_t *colptr = (uint8_t *)&output[0];

		int32_t gg_even_value;
		int32_t bg_even_value;
		int32_t rg_even_value;
		int32_t gg_odd_value;
		int32_t bg_odd_value;
		int32_t rg_odd_value;


		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two luma output points with special filters for the left border
		int32_t even = 0;
		int32_t odd = 0;


		__m128i rounding1_pi16 = _mm_set1_epi16(0); // for 6bit matm
		__m128i rounding2_pi16 = _mm_set1_epi16(0); // for 6bit matm

		if(descale_shift>=2)
		{
			int mask = (1<<(descale_shift-1))-1;//DAN20090601
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 0);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 1);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 2);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 3);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 4);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 5);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 6);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 7);

			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 0);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 1);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 2);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 3);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 4);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 5);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 6);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 7);

			rounding1_pi16 = _mm_adds_epi16(rounding1_pi16, _mm_set1_epi16(10*mask/32)); //DAN20090601
			rounding2_pi16 = _mm_adds_epi16(rounding2_pi16, _mm_set1_epi16(10*mask/32));
		}



		// Apply the even reconstruction filter to the lowpass band
		even += 11 * gg_lowpass_ptr[column + 0];
		even -=  4 * gg_lowpass_ptr[column + 1];
		even +=  1 * gg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += gg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		gg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * gg_lowpass_ptr[column + 0];
		odd += 4 * gg_lowpass_ptr[column + 1];
		odd -= 1 * gg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= gg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		gg_odd_value = odd;


		// Process the first two u chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * bg_lowpass_ptr[column + 0];
		even -=  4 * bg_lowpass_ptr[column + 1];
		even +=  1 * bg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += bg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		bg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * bg_lowpass_ptr[column + 0];
		odd += 4 * bg_lowpass_ptr[column + 1];
		odd -= 1 * bg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= bg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		bg_odd_value = odd;


		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * rg_lowpass_ptr[column + 0];
		even -=  4 * rg_lowpass_ptr[column + 1];
		even +=  1 * rg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += rg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		rg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * rg_lowpass_ptr[column + 0];
		odd += 4 * rg_lowpass_ptr[column + 1];
		odd -= 1 * rg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= rg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		rg_odd_value = odd;

#if (1 && XMMOPT)

		// Preload the first eight lowpass luma coefficients
		gg_low1_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[0]);

		// Preload the first eight highpass luma coefficients
		gg_high1_epi16 = _mm_load_si128((__m128i *)&gg_highpass_ptr[0]);

		// Preload the first eight lowpass u chroma coefficients
		bg_low1_epi16 = _mm_load_si128((__m128i *)&bg_lowpass_ptr[0]);

		// Preload the first eight highpass u chroma coefficients
		bg_high1_epi16 = _mm_load_si128((__m128i *)&bg_highpass_ptr[0]);

		// Preload the first eight lowpass v chroma coefficients
		rg_low1_epi16 = _mm_load_si128((__m128i *)&rg_lowpass_ptr[0]);

		// Preload the first eight highpass v chroma coefficients
 		rg_high1_epi16 = _mm_load_si128((__m128i *)&rg_highpass_ptr[0]);


		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i g1_output_epi16;
			__m128i g2_output_epi16;
			__m128i r1_output_epi16;
			__m128i r2_output_epi16;
			__m128i b1_output_epi16;
			__m128i b2_output_epi16;

			__m128i gg1_output_epi16;
			__m128i gg2_output_epi16;
			__m128i rg1_output_epi16;
			__m128i rg2_output_epi16;
			__m128i bg1_output_epi16;
			__m128i bg2_output_epi16;

			__m128i y1_output_epi16;
			__m128i y2_output_epi16;
			__m128i u1_output_epi16;
			__m128i u2_output_epi16;
			__m128i v1_output_epi16;
			__m128i v2_output_epi16;

			__m128i urg_epi16;
			__m128i yuv1_epi16;
			__m128i yuv2_epi16;
			__m128i yuv1_epi8;
			__m128i yuv2_epi8;

			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i temp_epi32;
			__m128i tempB_epi32;
			__m128i rgb_epi32;
			__m128i zero_epi128;
			__m128  temp_ps;
			__m128  rgb_ps;
			__m128	y1a_ps;
			__m128	y1b_ps;
			__m128	u1a_ps;
			__m128	u1b_ps;
			__m128	v1a_ps;
			__m128	v1b_ps;


			__m128i out_epi16;		// Reconstructed data
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			__m128i low1_epi16;
			__m128i low2_epi16;
			__m128i high1_epi16;
			__m128i high2_epi16;

			//DAN031304 -- correct inverse filter
			__m128i half_epi16 = _mm_set1_epi16(4);
			__m128i offset_epi16 = _mm_set1_epi16(2048);

			uint32_t temp;		// Temporary register for last two values


			/***** Compute the first eight luma output values *****/

			// Preload the second eight lowpass coefficients
			gg_low2_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			gg_high2_epi16 = _mm_load_si128((__m128i *)&gg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = gg_low1_epi16;
			high1_epi16 = gg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);


			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_odd_value, 1);

			// Save the eight luma values for packing later
			gg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			gg_even_value = (short)temp;
			gg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = gg_low2_epi16;
			high2_epi16 = gg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_odd_value, 1);

			// Save the eight luma values for packing later
			gg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			gg_even_value = (short)temp;
			gg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are used later in the loop
			gg_low1_epi16 = gg_low2_epi16;

			// The second eight highpass coefficients are used later in the loop
			gg_high1_epi16 = gg_high2_epi16;


			/***** Compute the first eight u chroma output values *****/

			// Preload the second eight lowpass coefficients
			bg_low2_epi16 = _mm_load_si128((__m128i *)&bg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			bg_high2_epi16 = _mm_load_si128((__m128i *)&bg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = bg_low1_epi16;
			high1_epi16 = bg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			bg_even_value = (short)temp;
			bg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight u chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = bg_low2_epi16;
			high2_epi16 = bg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			bg_even_value = (short)temp;
			bg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			bg_low1_epi16 = bg_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			bg_high1_epi16 = bg_high2_epi16;



			/***** Compute the first eight v chroma output values *****/

			// Preload the second eight lowpass coefficients
			rg_low2_epi16 = _mm_load_si128((__m128i *)&rg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			rg_high2_epi16 = _mm_load_si128((__m128i *)&rg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = rg_low1_epi16;
			high1_epi16 = rg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			rg_even_value = (short)temp;
			rg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight v chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = rg_low2_epi16;
			high2_epi16 = rg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			rg_even_value = (short)temp;
			rg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			rg_low1_epi16 = rg_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			rg_high1_epi16 = rg_high2_epi16;


			//r_output_epi16  = ((rg_output_epi16 - 32768)<<1)+gg_output_epi16
			//r_output_epi16  = ((rg_output_epi16>>3 - 32768>>3))+gg_output_epi16>>4


			g1_output_epi16 = gg1_output_epi16;

			r1_output_epi16 = rg1_output_epi16;//_mm_srli_epi16(rg1_output_epi16,2);
		//	r1_output_epi16 = _mm_subs_epi16(r1_output_epi16, value128_epi32);
		//	r1_output_epi16 = _mm_slli_epi16(r1_output_epi16,1);
		//	r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, g1_output_epi16);

			b1_output_epi16 = bg1_output_epi16;//_mm_srli_epi16(bg1_output_epi16,2);
		//	b1_output_epi16 = _mm_subs_epi16(b1_output_epi16, value128_epi32);
		//	b1_output_epi16 = _mm_slli_epi16(b1_output_epi16,1);
		//	b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, g1_output_epi16);


			 r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, limiterRGB);
			 r1_output_epi16 = _mm_subs_epu16(r1_output_epi16, limiterRGB);

			 g1_output_epi16 = _mm_adds_epi16(g1_output_epi16, limiterRGB);
			 g1_output_epi16 = _mm_subs_epu16(g1_output_epi16, limiterRGB);

			 b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, limiterRGB);
			 b1_output_epi16 = _mm_subs_epu16(b1_output_epi16, limiterRGB);


			zero_epi128 = _mm_setzero_si128();


			// Compute Y,U,V
			rgb_epi32 = _mm_unpacklo_epi16(r1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1a_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1a_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1a_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);
			rgb_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1b_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1b_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1b_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);

			rgb_epi32 = _mm_unpacklo_epi16(g1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			rgb_epi32 = _mm_unpacklo_epi16(b1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_ps = _mm_set_ps1(fy_offset);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fu_offset);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fv_offset);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_epi32 = _mm_cvtps_epi32(y1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
			y1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			y1_output_epi16 = _mm_adds_epi16(y1_output_epi16, limiter);
			y1_output_epi16 = _mm_subs_epu16(y1_output_epi16, limiter);
			y1_output_epi16 = _mm_srli_epi16(y1_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(u1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
			u1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			u1_output_epi16 = _mm_adds_epi16(u1_output_epi16, limiter);
			u1_output_epi16 = _mm_subs_epu16(u1_output_epi16, limiter);
			u1_output_epi16 = _mm_srli_epi16(u1_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(v1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
			v1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			v1_output_epi16 = _mm_adds_epi16(v1_output_epi16, limiter);
			v1_output_epi16 = _mm_subs_epu16(v1_output_epi16, limiter);
			v1_output_epi16 = _mm_srli_epi16(v1_output_epi16, shift);


			g2_output_epi16 = gg2_output_epi16;//_mm_srli_epi16(gg2_output_epi16,2);

			r2_output_epi16 = rg2_output_epi16;//_mm_srli_epi16(rg2_output_epi16,2);
		//	r2_output_epi16 = _mm_subs_epi16(r2_output_epi16, value128_epi32);
		//	r2_output_epi16 = _mm_slli_epi16(r2_output_epi16,1);
		//	r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, g2_output_epi16);

			b2_output_epi16 = bg2_output_epi16;//_mm_srli_epi16(bg2_output_epi16,2);
		//	b2_output_epi16 = _mm_subs_epi16(b2_output_epi16, value128_epi32);
		//	b2_output_epi16 = _mm_slli_epi16(b2_output_epi16,1);
		//	b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, g2_output_epi16);


			 r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, limiterRGB);
			 r2_output_epi16 = _mm_subs_epu16(r2_output_epi16, limiterRGB);

			 g2_output_epi16 = _mm_adds_epi16(g2_output_epi16, limiterRGB);
			 g2_output_epi16 = _mm_subs_epu16(g2_output_epi16, limiterRGB);

			 b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, limiterRGB);
			 b2_output_epi16 = _mm_subs_epu16(b2_output_epi16, limiterRGB);


			// Compute Y,U,V
			rgb_epi32 = _mm_unpacklo_epi16(r2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1a_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1a_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1a_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);
			rgb_epi32 = _mm_unpackhi_epi16(r2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1b_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1b_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1b_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);

			rgb_epi32 = _mm_unpacklo_epi16(g2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(g2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			rgb_epi32 = _mm_unpacklo_epi16(b2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(b2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_ps = _mm_set_ps1(fy_offset);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fu_offset);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fv_offset);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_epi32 = _mm_cvtps_epi32(y1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
			y2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			y2_output_epi16 = _mm_adds_epi16(y2_output_epi16, limiter);
			y2_output_epi16 = _mm_subs_epu16(y2_output_epi16, limiter);
			y2_output_epi16 = _mm_srli_epi16(y2_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(u1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
			u2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			u2_output_epi16 = _mm_adds_epi16(u2_output_epi16, limiter);
			u2_output_epi16 = _mm_subs_epu16(u2_output_epi16, limiter);
			u2_output_epi16 = _mm_srli_epi16(u2_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(v1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
			v2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			v2_output_epi16 = _mm_adds_epi16(v2_output_epi16, limiter);
			v2_output_epi16 = _mm_subs_epu16(v2_output_epi16, limiter);
			v2_output_epi16 = _mm_srli_epi16(v2_output_epi16, shift);

			/***** Interleave the luma and chroma values *****/
			if(decoder->frame.format == DECODED_FORMAT_R408 || decoder->frame.format == DECODED_FORMAT_V408)
			{
				__m128i y_epi8 = _mm_packus_epi16(y1_output_epi16, y2_output_epi16); //pack to 8-bit
				__m128i u_epi8 = _mm_packus_epi16(u1_output_epi16, u2_output_epi16); //pack to 8-bit
				__m128i v_epi8 = _mm_packus_epi16(v1_output_epi16, v2_output_epi16); //pack to 8-bit
				__m128i a_epi8 =  _mm_set1_epi8(0xff);


				if(decoder->frame.format == COLOR_FORMAT_V408) // UYVA
				{
					__m128i UY,VA,UYVA;

					UY = _mm_unpacklo_epi8(u_epi8, y_epi8);
					VA = _mm_unpacklo_epi8(v_epi8, a_epi8);
					UYVA = _mm_unpacklo_epi16(UY, VA);
					_mm_storeu_si128(outptr++, UYVA);

					UYVA = _mm_unpackhi_epi16(UY, VA);
					_mm_storeu_si128(outptr++, UYVA);

					UY = _mm_unpackhi_epi8(u_epi8, y_epi8);
					VA = _mm_unpackhi_epi8(v_epi8, a_epi8);
					UYVA = _mm_unpacklo_epi16(UY, VA);
					_mm_storeu_si128(outptr++, UYVA);

					UYVA = _mm_unpackhi_epi16(UY, VA);
					_mm_storeu_si128(outptr++, UYVA);
				}
				else //r408 AYUV
				{
					__m128i AY,UV,AYUV;
					__m128i offsetR408_epi8 =  _mm_set1_epi8(16);

					y_epi8 = _mm_subs_epu8(y_epi8, offsetR408_epi8);

					AY = _mm_unpacklo_epi8(a_epi8, y_epi8);
					UV = _mm_unpacklo_epi8(u_epi8, v_epi8);
					AYUV = _mm_unpacklo_epi16(AY, UV);
					_mm_storeu_si128(outptr++, AYUV);

					AYUV = _mm_unpackhi_epi16(AY, UV);
					_mm_storeu_si128(outptr++, AYUV);

					AY = _mm_unpackhi_epi8(a_epi8, y_epi8);
					UV = _mm_unpackhi_epi8(u_epi8, v_epi8);
					AYUV = _mm_unpacklo_epi16(AY, UV);
					_mm_storeu_si128(outptr++, AYUV);

					AYUV = _mm_unpackhi_epi16(AY, UV);
					_mm_storeu_si128(outptr++, AYUV);
				}
			}
			else
			{
				// 4:4:4 to 4:2:2
				temp_epi16 = _mm_srli_si128(u1_output_epi16, 2);
				u1_output_epi16 = _mm_adds_epi16(u1_output_epi16, temp_epi16);
				u1_output_epi16 = _mm_srli_epi16(u1_output_epi16, 1);
				temp_epi16 = _mm_srli_si128(u2_output_epi16, 2);
				u2_output_epi16 = _mm_adds_epi16(u2_output_epi16, temp_epi16);
				u2_output_epi16 = _mm_srli_epi16(u2_output_epi16, 1);
				temp_epi16 = _mm_srli_si128(v1_output_epi16, 2);
				v1_output_epi16 = _mm_adds_epi16(v1_output_epi16, temp_epi16);
				v1_output_epi16 = _mm_srli_epi16(v1_output_epi16, 1);
				temp_epi16 = _mm_srli_si128(v2_output_epi16, 2);
				v2_output_epi16 = _mm_adds_epi16(v2_output_epi16, temp_epi16);
				v2_output_epi16 = _mm_srli_epi16(v2_output_epi16, 1);
				u1_output_epi16 = _mm_and_si128(u1_output_epi16, mask_epi32);
				u2_output_epi16 = _mm_and_si128(u2_output_epi16, mask_epi32);
				v1_output_epi16 = _mm_and_si128(v1_output_epi16, mask_epi32);
				v2_output_epi16 = _mm_and_si128(v2_output_epi16, mask_epi32);

				u1_output_epi16 = _mm_packs_epi32 (u1_output_epi16, u2_output_epi16);
				v1_output_epi16 = _mm_packs_epi32 (v1_output_epi16, v2_output_epi16);

				if(decoder->frame.format == DECODED_FORMAT_YUYV) // UYVA
				{
					// Interleave the first four values from each chroma channel
					urg_epi16 = _mm_unpacklo_epi16(u1_output_epi16, v1_output_epi16);

					// Interleave the first eight chroma values with the first eight luma values
					yuv1_epi16 = _mm_unpacklo_epi16(y1_output_epi16, urg_epi16);
					yuv2_epi16 = _mm_unpackhi_epi16(y1_output_epi16, urg_epi16);

					// Pack the first sixteen bytes of luma and chroma
					yuv1_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

					// Store the first sixteen bytes of output values
					_mm_store_si128(outptr++, yuv1_epi8);

					// Interleave the second four values from each chroma channel
					urg_epi16 = _mm_unpackhi_epi16(u1_output_epi16, v1_output_epi16);

					// Interleave the second eight chroma values with the second eight luma values
					yuv1_epi16 = _mm_unpacklo_epi16(y2_output_epi16, urg_epi16);
					yuv2_epi16 = _mm_unpackhi_epi16(y2_output_epi16, urg_epi16);

					// Pack the second sixteen bytes of luma and chroma
					yuv2_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

					// Store the second sixteen bytes of output values
					_mm_store_si128(outptr++, yuv2_epi8);
				}
				else
				{
					// This block of code only works for 8-bit CbYCrY with 4:2:2 sampling
					//int format = decoder->frame.format;
					assert(decoder->frame.format == DECODED_FORMAT_UYVY || decoder->frame.format == DECODED_FORMAT_CbYCrY_8bit);

					// Interleave the first four values from each chroma channel
					urg_epi16 = _mm_unpacklo_epi16(u1_output_epi16, v1_output_epi16);

					// Interleave the first eight chroma values with the first eight luma values
					yuv1_epi16 = _mm_unpacklo_epi16(urg_epi16, y1_output_epi16);
					yuv2_epi16 = _mm_unpackhi_epi16(urg_epi16, y1_output_epi16);

					// Pack the first sixteen bytes of luma and chroma
					yuv1_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

					// Store the first sixteen bytes of output values
					_mm_store_si128(outptr++, yuv1_epi8);

					// Interleave the second four values from each chroma channel
					urg_epi16 = _mm_unpackhi_epi16(u1_output_epi16, v1_output_epi16);

					// Interleave the second eight chroma values with the second eight luma values
					yuv1_epi16 = _mm_unpacklo_epi16(urg_epi16, y2_output_epi16);
					yuv2_epi16 = _mm_unpackhi_epi16(urg_epi16, y2_output_epi16);

					// Pack the second sixteen bytes of luma and chroma
					yuv2_epi8 = _mm_packus_epi16(yuv1_epi16, yuv2_epi16);

					// Store the second sixteen bytes of output values
					_mm_store_si128(outptr++, yuv2_epi8);
				}
			}
		}

		// Should have exited the loop with the column equal to the post processing column
	//	assert(column == post_column);

		colptr = (uint8_t *)outptr;
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column ++)
		{
			int re,ge,be;
			int ro,go,bo;
			int ye,yo,u,v;
			int ue,uo,ve,vo;


			/***** First pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += gg_lowpass_ptr[column - 1];
			even -= gg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += gg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += gg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the luma result for later output in the correct order
		//	y1_even_value = even;
			ge = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= gg_lowpass_ptr[column - 1];
			odd += gg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += gg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= gg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the luma result for later output in the correct order
		//	y1_odd_value = odd;
			go = odd;


			/***** Pair of u chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += bg_lowpass_ptr[column - 1];
			even -= bg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += bg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += bg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the u chroma result for later output in the correct order
		//	bg_even_value = even;
			be = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= bg_lowpass_ptr[column - 1];
			odd += bg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += bg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= bg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the u chroma result for later output in the correct order
		//	bg_odd_value = odd;
			bo = odd;



			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += rg_lowpass_ptr[column - 1];
			even -= rg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += rg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += rg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the v chroma result for later output in the correct order
		//	rg_even_value = even;
			re = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= rg_lowpass_ptr[column - 1];
			odd += rg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += rg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= rg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the v chroma result for later output in the correct order
		//	rg_odd_value = odd;
			ro = odd;


		// We use 16-bit fixed-point arithmetic to approximate the color conversion coefficients
		//
		// Floating point arithmetic is
		//

			switch(decoder->frame.format)
			{
			case DECODED_FORMAT_R408: //AYUV
			case DECODED_FORMAT_V408: //UYVA
				ye = ((int)((fy_rmult * (float)re + fy_gmult * (float)ge + fy_bmult * (float)be)) >> (descale_shift + 6)) + 16;
				yo = ((int)((fy_rmult * (float)ro + fy_gmult * (float)go + fy_bmult * (float)bo)) >> (descale_shift + 6)) + 16;
				ue = ((int)((fu_rmult * (float)re + fu_gmult * (float)ge + fu_bmult * (float)be)) >> (descale_shift + 6)) + 128;
				uo = ((int)((fu_rmult * (float)ro + fu_gmult * (float)go + fu_bmult * (float)bo)) >> (descale_shift + 6)) + 128;
				ve = ((int)((fv_rmult * (float)re + fv_gmult * (float)ge + fv_bmult * (float)be)) >> (descale_shift + 6)) + 128;
				vo = ((int)((fv_rmult * (float)ro + fv_gmult * (float)go + fv_bmult * (float)bo)) >> (descale_shift + 6)) + 128;
			
				if(decoder->frame.format == DECODED_FORMAT_R408)//AYUV
				{
					*(colptr++) = 0xff;
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(ue);
					*(colptr++) = SATURATE_8U(ve);

					*(colptr++) = 0xff;
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(uo);
					*(colptr++) = SATURATE_8U(vo);
				}
				else	//UYVA
				{
					*(colptr++) = SATURATE_8U(ue);
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(ve);
					*(colptr++) = 0xff;

					*(colptr++) = SATURATE_8U(vo);
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(uo);
					*(colptr++) = 0xff;
				}
				break;
			case DECODED_FORMAT_YUYV:// Output the luma and chroma values in the correct order	
			case DECODED_FORMAT_UYVY:
				ye = ((int)((fy_rmult * (float)re + fy_gmult * (float)ge + fy_bmult * (float)be)) >> (descale_shift + 6)) + 16;
				yo = ((int)((fy_rmult * (float)ro + fy_gmult * (float)go + fy_bmult * (float)bo)) >> (descale_shift + 6)) + 16;
				u  = ((int)((fu_rmult * (float)(re+ro) + fu_gmult * (float)(ge+go) + fu_bmult * (float)(be+bo))) >> (1 + descale_shift + 6)) + 128;
				v  = ((int)((fv_rmult * (float)(re+ro) + fv_gmult * (float)(ge+go) + fv_bmult * (float)(be+bo))) >> (1 + descale_shift + 6)) + 128;

				if(decoder->frame.format == DECODED_FORMAT_YUYV)
				{
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(u);
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(v);
				}
				else
				{
					*(colptr++) = SATURATE_8U(u);
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(v);
					*(colptr++) = SATURATE_8U(yo);
				}
				break;
			}
		}

		// Should have exited the loop at the column for right border processing
	//	assert(column == last_column);


		// Redo the last two RGB444 pixels.
		column = last_column - 1;
		colptr -= 4; //two pixels

		// Process the last luma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * gg_lowpass_ptr[column + 0];
		even += 4 * gg_lowpass_ptr[column - 1];
		even -= 1 * gg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += gg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the luma result for later output in the correct order
		gg_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * gg_lowpass_ptr[column + 0];
		odd -=  4 * gg_lowpass_ptr[column - 1];
		odd +=  1 * gg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= gg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the luma result for later output in the correct order
		gg_odd_value = odd;

		// Process the last u chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * bg_lowpass_ptr[column + 0];
		even += 4 * bg_lowpass_ptr[column - 1];
		even -= 1 * bg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += bg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		bg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * bg_lowpass_ptr[column + 0];
		odd -=  4 * bg_lowpass_ptr[column - 1];
		odd +=  1 * bg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= bg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		bg_odd_value = odd;

		// Process the last v chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * rg_lowpass_ptr[column + 0];
		even += 4 * rg_lowpass_ptr[column - 1];
		even -= 1 * rg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += rg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		rg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * rg_lowpass_ptr[column + 0];
		odd -=  4 * rg_lowpass_ptr[column - 1];
		odd +=  1 * rg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= rg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		rg_odd_value = odd;

		{
			int ye,yo,ue,uo,ve,vo,u,v;
			int re = rg_even_value;
			int ro = rg_odd_value;
			int ge = gg_even_value;
			int go = gg_odd_value;
			int be = bg_even_value;
			int bo = bg_odd_value;

			switch(decoder->frame.format)
			{
			case DECODED_FORMAT_R408: //AYUV
			case DECODED_FORMAT_V408: //UYVA
				ye = ((int)((fy_rmult * (float)re + fy_gmult * (float)ge + fy_bmult * (float)be)) >> (descale_shift + 6)) + 16;
				yo = ((int)((fy_rmult * (float)ro + fy_gmult * (float)go + fy_bmult * (float)bo)) >> (descale_shift + 6)) + 16;
				ue = ((int)((fu_rmult * (float)re + fu_gmult * (float)ge + fu_bmult * (float)be)) >> (descale_shift + 6)) + 128;
				uo = ((int)((fu_rmult * (float)ro + fu_gmult * (float)go + fu_bmult * (float)bo)) >> (descale_shift + 6)) + 128;
				ve = ((int)((fv_rmult * (float)re + fv_gmult * (float)ge + fv_bmult * (float)be)) >> (descale_shift + 6)) + 128;
				vo = ((int)((fv_rmult * (float)ro + fv_gmult * (float)go + fv_bmult * (float)bo)) >> (descale_shift + 6)) + 128;
			
				if(decoder->frame.format == DECODED_FORMAT_R408)//AYUV
				{
					*(colptr++) = 0xff;
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(ue);
					*(colptr++) = SATURATE_8U(ve);

					*(colptr++) = 0xff;
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(uo);
					*(colptr++) = SATURATE_8U(vo);
				}
				else	//UYVA
				{
					*(colptr++) = SATURATE_8U(ue);
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(ve);
					*(colptr++) = 0xff;

					*(colptr++) = SATURATE_8U(vo);
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(uo);
					*(colptr++) = 0xff;
				}
				break;
			case DECODED_FORMAT_YUYV:// Output the luma and chroma values in the correct order	
			case DECODED_FORMAT_UYVY:
				ye = ((int)((fy_rmult * (float)re + fy_gmult * (float)ge + fy_bmult * (float)be)) >> (descale_shift + 6)) + 16;
				yo = ((int)((fy_rmult * (float)ro + fy_gmult * (float)go + fy_bmult * (float)bo)) >> (descale_shift + 6)) + 16;
				u  = ((int)((fu_rmult * (float)(re+ro) + fu_gmult * (float)(ge+go) + fu_bmult * (float)(be+bo))) >> (1 + descale_shift + 6)) + 128;
				v  = ((int)((fv_rmult * (float)(re+ro) + fv_gmult * (float)(ge+go) + fv_bmult * (float)(be+bo))) >> (1 + descale_shift + 6)) + 128;

				if(decoder->frame.format == DECODED_FORMAT_YUYV)
				{
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(u);
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(v);
				}
				else
				{
					*(colptr++) = SATURATE_8U(u);
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(v);
					*(colptr++) = SATURATE_8U(yo);
				}
				break;
			}
		}


		// Advance to the next row of coefficients in each channel
		gg_lowpass_ptr += lowpass_pitch[0];
		bg_lowpass_ptr += lowpass_pitch[1];
		rg_lowpass_ptr += lowpass_pitch[2];
		gg_highpass_ptr += highpass_pitch[0];
		bg_highpass_ptr += highpass_pitch[1];
		rg_highpass_ptr += highpass_pitch[2];

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}


void InvertHorizontalStrip16sRGBA2YUVA(HorizontalFilterParams)
{
	int num_channels = CODEC_MAX_CHANNELS; // need alpha
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *g_lowpass_ptr = lowpass_band[0];
	PIXEL *r_lowpass_ptr = lowpass_band[1];
	PIXEL *b_lowpass_ptr = lowpass_band[2];
	PIXEL *a_lowpass_ptr = lowpass_band[3];
	PIXEL *g_highpass_ptr = highpass_band[0];
	PIXEL *r_highpass_ptr = highpass_band[1];
	PIXEL *b_highpass_ptr = highpass_band[2];
	PIXEL *a_highpass_ptr = highpass_band[3];

	uint8_t *output = output_image;

	// Process 8 luma coefficients per loop iteration
	const int column_step = 8;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;
	int post_column = last_column - (last_column % column_step);

	// Need at least four luma values of border processing up to the last column
	//const int post_border = 2;

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	int descale_shift = (precision - 8);

	int shift = 8;

	float fy_rmult,fy_gmult,fy_bmult,fy_offset;
	float fu_rmult,fu_gmult,fu_bmult,fu_offset;
	float fv_rmult,fv_gmult,fv_bmult,fv_offset;

	int color_space = decoder->frame.colorspace;

	float rgb2yuv[3][4];
	float scale;

	
	decoder->frame.alpha_Companded = 1;

	switch(color_space & COLORSPACE_MASK)
	{
		case COLOR_SPACE_CG_601: //601
			{
			float rgb2yuv601[3][4] =
                {	{0.257f, 0.504f, 0.098f, 16.0f/255.0f},
                    {-0.148f,-0.291f, 0.439f, 128.0f/255.0f},
                    {0.439f,-0.368f,-0.071f, 128.0f/255.0f}
			};
			memcpy(rgb2yuv,rgb2yuv601, sizeof(rgb2yuv));
			}
			break;
		default: assert(0);
		case COLOR_SPACE_CG_709:
			{
			float rgb2yuv709[3][4] =
			{
                {0.183f, 0.614f, 0.062f, 16.0f/255.0f},
                {-0.101f,-0.338f, 0.439f, 128.0f/255.0f},
                {0.439f,-0.399f,-0.040f, 128.0f/255.0f}
			};
			memcpy(rgb2yuv,rgb2yuv709, sizeof(rgb2yuv));
			}
			break;
		case COLOR_SPACE_VS_601: //VS 601
			{
			float rgb2yuvVS601[3][4] =
			{
                {0.299f,0.587f,0.114f,0},
                {-0.172f,-0.339f,0.511f,128.0f/255.0f},
                {0.511f,-0.428f,-0.083f,128.0f/255.0f}
			};
			memcpy(rgb2yuv,rgb2yuvVS601, sizeof(rgb2yuv));
			}
			break;
		case COLOR_SPACE_VS_709:
			{
			float rgb2yuvVS709[3][4] =
			{
                {0.213f,0.715f,0.072f,0},
                {-0.117f,-0.394f,0.511f,128.0f/255.0f},
                {0.511f,-0.464f,-0.047f,128.0f/255.0f}
			};
			memcpy(rgb2yuv,rgb2yuvVS709, sizeof(rgb2yuv));
			}
			break;
	}



	// We use 16-bit fixed-point arithmetic to approximate the color conversion coefficients
	//
	// Floating point arithmetic is
	//
	// Y  = 0.257 * R + 0.504 * G + 0.098 * B + 16.5;
	// Cb =-0.148 * R - 0.291 * G + 0.439 * B + 128.5;
	// Cr = 0.439 * R - 0.368 * G - 0.071 * B + 128.5;
	//
	// Fixed point approximation (8-bit) is
	//
	// Y  = ( 66 * R + 129 * G +  25 * B +  4224) >> 8;
	// Cb = (-38 * R -  74 * G + 112 * B + 32896) >> 8;
	// Cr = (112 * R -  94 * G -  18 * B + 32896) >> 8;

	// 601 video systems
	// Y = 0.299R + 0.587G + 0.114B
	// Cb = -0.172R - 0.339G + 0.511B + 128
	// Cr = 0.511R - 0.428G - 0.083B + 128

	// 709 video systems
	// Y = 0.213R + 0.715G + 0.072B
	// Cb = -0.117R - 0.394G + 0.511B + 128
	// Cr = 0.511R - 0.464G - 0.047B + 128

	// 709 sRGB
	// Y = 0.183R + 0.614G + 0.062B + 16
	// Cb = -0.101R - 0.338G + 0.439B + 128
	// Cr = 0.439R - 0.399G - 0.040B + 128

	scale = 64.0;

	fy_rmult = ((rgb2yuv[0][0]) * scale);
	fy_gmult = ((rgb2yuv[0][1]) * scale);
	fy_bmult = ((rgb2yuv[0][2]) * scale);
	fy_offset= ((rgb2yuv[0][3]) * 16384.0f);

	fu_rmult = ((rgb2yuv[1][0]) * scale);
	fu_gmult = ((rgb2yuv[1][1]) * scale);
	fu_bmult = ((rgb2yuv[1][2]) * scale);
	fu_offset= ((rgb2yuv[1][3]) * 16384.0f);

	fv_rmult = ((rgb2yuv[2][0]) * scale);
	fv_gmult = ((rgb2yuv[2][1]) * scale);
	fv_bmult = ((rgb2yuv[2][2]) * scale);
	fv_offset= ((rgb2yuv[2][3]) * 16384.0f);

	shift-=2;


	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);

	// Adjust the end of the fast loop if necessary for border processing
	//if (post_column > (last_column - post_border))  // DAN08112004 - ignore end row calc -- use SSE to edge.
	//	post_column -= column_step;

	// Check that there is enough margin to accommodate border processing
//	assert(post_column <= (last_column - post_border));

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
#if (1 && XMMOPT)
		__m128i g_low1_epi16;		// Lowpass coefficients
		__m128i g_low2_epi16;
		__m128i b_low1_epi16;
		__m128i b_low2_epi16;
		__m128i r_low1_epi16;
		__m128i r_low2_epi16;
		__m128i a_low1_epi16;
		__m128i a_low2_epi16;

		__m128i g_high1_epi16;		// Highpass coefficients
		__m128i g_high2_epi16;
		__m128i b_high1_epi16;
		__m128i b_high2_epi16;
		__m128i r_high1_epi16;
		__m128i r_high2_epi16;
		__m128i a_high1_epi16;
		__m128i a_high2_epi16;

		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];

		__m128i limiterRGB = _mm_set1_epi16(0x7fff - 0x00ff);
		__m128i limiter = _mm_set1_epi16(0x7fff - 0x3fff);

#endif
		uint8_t *colptr = (uint8_t *)&output[0];

		int32_t g_even_value;
		int32_t b_even_value;
		int32_t r_even_value;
		int32_t a_even_value;
		int32_t g_odd_value;
		int32_t b_odd_value;
		int32_t r_odd_value;
		int32_t a_odd_value;


		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two luma output points with special filters for the left border
		int32_t even = 0;
		int32_t odd = 0;


		__m128i rounding1_pi16 = _mm_set1_epi16(0); // for 6bit matm
		__m128i rounding2_pi16 = _mm_set1_epi16(0); // for 6bit matm

		if(descale_shift>=2)
		{
			int mask = (1<<(descale_shift-1))-1;//DAN20090601
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 0);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 1);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 2);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 3);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 4);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 5);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 6);
			rounding1_pi16 = _mm_insert_epi16(rounding1_pi16, rand()&mask, 7);

			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 0);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 1);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 2);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 3);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 4);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 5);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 6);
			rounding2_pi16 = _mm_insert_epi16(rounding2_pi16, rand()&mask, 7);

			rounding1_pi16 = _mm_adds_epi16(rounding1_pi16, _mm_set1_epi16(10*mask/32)); //DAN20090601
			rounding2_pi16 = _mm_adds_epi16(rounding2_pi16, _mm_set1_epi16(10*mask/32));
		}



		// Apply the even reconstruction filter to the lowpass band
		even += 11 * g_lowpass_ptr[column + 0];
		even -=  4 * g_lowpass_ptr[column + 1];
		even +=  1 * g_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += g_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		g_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * g_lowpass_ptr[column + 0];
		odd += 4 * g_lowpass_ptr[column + 1];
		odd -= 1 * g_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= g_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		g_odd_value = odd;


		// Process the first two u chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * b_lowpass_ptr[column + 0];
		even -=  4 * b_lowpass_ptr[column + 1];
		even +=  1 * b_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += b_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		b_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * b_lowpass_ptr[column + 0];
		odd += 4 * b_lowpass_ptr[column + 1];
		odd -= 1 * b_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= b_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		b_odd_value = odd;


		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * r_lowpass_ptr[column + 0];
		even -=  4 * r_lowpass_ptr[column + 1];
		even +=  1 * r_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += r_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		even >>= descale_shift;

		// Save the value for use in the fast loop
		r_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * r_lowpass_ptr[column + 0];
		odd += 4 * r_lowpass_ptr[column + 1];
		odd -= 1 * r_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= r_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		odd >>= descale_shift;

		// Save the value for use in the fast loop
		r_odd_value = odd;
		
		
		
		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		if(decoder->codec.num_channels == 4)
		{
			even += 11 * a_lowpass_ptr[column + 0];
			even -=  4 * a_lowpass_ptr[column + 1];
			even +=  1 * a_lowpass_ptr[column + 2];
			even += ROUNDING(even,8);
			even = DivideByShift(even, 3);

			// Add the highpass correction
			even += a_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even >>= descale_shift;

			// Save the value for use in the fast loop
			a_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd += 5 * a_lowpass_ptr[column + 0];
			odd += 4 * a_lowpass_ptr[column + 1];
			odd -= 1 * a_lowpass_ptr[column + 2];
			odd += ROUNDING(odd,8);
			odd = DivideByShift(odd, 3);

			// Subtract the highpass correction
			odd -= a_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd >>= descale_shift;

			// Save the value for use in the fast loop
			a_odd_value = odd;
		}
		else
		{
			a_odd_value = a_even_value = 255;
		}



#if (1 && XMMOPT)

		// Preload the first eight lowpass luma coefficients
		g_low1_epi16 = _mm_load_si128((__m128i *)&g_lowpass_ptr[0]);

		// Preload the first eight highpass luma coefficients
		g_high1_epi16 = _mm_load_si128((__m128i *)&g_highpass_ptr[0]);

		// Preload the first eight lowpass u chroma coefficients
		b_low1_epi16 = _mm_load_si128((__m128i *)&b_lowpass_ptr[0]);

		// Preload the first eight highpass u chroma coefficients
		b_high1_epi16 = _mm_load_si128((__m128i *)&b_highpass_ptr[0]);

		// Preload the first eight lowpass v chroma coefficients
		r_low1_epi16 = _mm_load_si128((__m128i *)&r_lowpass_ptr[0]);

		// Preload the first eight highpass v chroma coefficients
 		r_high1_epi16 = _mm_load_si128((__m128i *)&r_highpass_ptr[0]);
 		
		if(decoder->codec.num_channels == 4)
		{
			// Preload the first eight lowpass v chroma coefficients
			a_low1_epi16 = _mm_load_si128((__m128i *)&a_lowpass_ptr[0]);

			// Preload the first eight highpass v chroma coefficients
 			a_high1_epi16 = _mm_load_si128((__m128i *)&a_highpass_ptr[0]);
		}


		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i g1_output_epi16;
			__m128i g2_output_epi16;
			__m128i r1_output_epi16;
			__m128i r2_output_epi16;
			__m128i b1_output_epi16;
			__m128i b2_output_epi16;
			__m128i a1_output_epi16;
			__m128i a2_output_epi16;

			__m128i gg1_output_epi16;
			__m128i gg2_output_epi16;
			__m128i rg1_output_epi16;
			__m128i rg2_output_epi16;
			__m128i bg1_output_epi16;
			__m128i bg2_output_epi16;
			__m128i ag1_output_epi16;
			__m128i ag2_output_epi16;

			__m128i y1_output_epi16;
			__m128i y2_output_epi16;
			__m128i u1_output_epi16;
			__m128i u2_output_epi16;
			__m128i v1_output_epi16;
			__m128i v2_output_epi16;
			
			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i temp_epi32;
			__m128i tempB_epi32;
			__m128i rgb_epi32;
			__m128i zero_epi128;
			__m128  temp_ps;
			__m128  rgb_ps;
			__m128	y1a_ps;
			__m128	y1b_ps;
			__m128	u1a_ps;
			__m128	u1b_ps;
			__m128	v1a_ps;
			__m128	v1b_ps;


			__m128i out_epi16;		// Reconstructed data
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			__m128i low1_epi16;
			__m128i low2_epi16;
			__m128i high1_epi16;
			__m128i high2_epi16;

			//DAN031304 -- correct inverse filter
			__m128i half_epi16 = _mm_set1_epi16(4);
			__m128i offset_epi16 = _mm_set1_epi16(2048);

			uint32_t temp;		// Temporary register for last two values


			/***** Compute the first eight luma output values *****/

			// Preload the second eight lowpass coefficients
			g_low2_epi16 = _mm_load_si128((__m128i *)&g_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			g_high2_epi16 = _mm_load_si128((__m128i *)&g_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = g_low1_epi16;
			high1_epi16 = g_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);


			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, g_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, g_odd_value, 1);

			// Save the eight luma values for packing later
			gg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			g_even_value = (short)temp;
			g_odd_value = (short)(temp >> 16);


			/***** Compute the second eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = g_low2_epi16;
			high2_epi16 = g_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, g_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, g_odd_value, 1);

			// Save the eight luma values for packing later
			gg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			g_even_value = (short)temp;
			g_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are used later in the loop
			g_low1_epi16 = g_low2_epi16;

			// The second eight highpass coefficients are used later in the loop
			g_high1_epi16 = g_high2_epi16;


			/***** Compute the first eight u chroma output values *****/

			// Preload the second eight lowpass coefficients
			b_low2_epi16 = _mm_load_si128((__m128i *)&b_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			b_high2_epi16 = _mm_load_si128((__m128i *)&b_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = b_low1_epi16;
			high1_epi16 = b_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, b_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, b_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			b_even_value = (short)temp;
			b_odd_value = (short)(temp >> 16);


			/***** Compute the second eight u chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = b_low2_epi16;
			high2_epi16 = b_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, b_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, b_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			b_even_value = (short)temp;
			b_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			b_low1_epi16 = b_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			b_high1_epi16 = b_high2_epi16;



			/***** Compute the first eight v chroma output values *****/

			// Preload the second eight lowpass coefficients
			r_low2_epi16 = _mm_load_si128((__m128i *)&r_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			r_high2_epi16 = _mm_load_si128((__m128i *)&r_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = r_low1_epi16;
			high1_epi16 = r_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, r_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, r_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			r_even_value = (short)temp;
			r_odd_value = (short)(temp >> 16);


			/***** Compute the second eight v chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = r_low2_epi16;
			high2_epi16 = r_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
			out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
			out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, r_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, r_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			r_even_value = (short)temp;
			r_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			r_low1_epi16 = r_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			r_high1_epi16 = r_high2_epi16;

			
			
			
			
			
			/***** Compute the first eight v chroma output values *****/
			if(decoder->codec.num_channels == 4)
			{
				// Preload the second eight lowpass coefficients
				a_low2_epi16 = _mm_load_si128((__m128i *)&a_lowpass_ptr[column + 8]);

				// Preload the second eight highpass coefficients
				a_high2_epi16 = _mm_load_si128((__m128i *)&a_highpass_ptr[column + 8]);

				// Move the current set of coefficients to working registers
				low1_epi16 = a_low1_epi16;
				high1_epi16 = a_high1_epi16;

				// Apply the even reconstruction filter to the lowpass band
				even_epi16 = low1_epi16;
				temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
				even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
				even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
				even_epi16 = _mm_srai_epi16(even_epi16, 3);
				temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
				even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

				// Shift the highpass correction by one column
				high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

				// Add the highpass correction and divide by two
				even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
				even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
				even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
				even_epi16 = _mm_srai_epi16(even_epi16, 1);

				// Apply the odd reconstruction filter to the lowpass band
				odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
				temp_epi16 = low1_epi16;
				odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
				odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
				odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
				temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
				odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

				// Subtract the highpass correction and divide by two
				odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
				odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
				odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
				odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

				// Interleave the four even and four odd results
				out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

				// Reduce the precision to eight bits
				out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
				out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

				// Combine the new output values with the two values from the previous phase
				out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
				temp = _mm_cvtsi128_si32(out_epi16);
				out_epi16 = _mm_insert_epi16(out_epi16, a_even_value, 0);
				out_epi16 = _mm_insert_epi16(out_epi16, a_odd_value, 1);

				// Save the eight u chroma values for packing later
				ag1_output_epi16 = out_epi16;

				// Save the remaining two output values
				a_even_value = (short)temp;
				a_odd_value = (short)(temp >> 16);


				/***** Compute the second eight v chroma output values *****/

				// Move the next set of coefficients to working registers
				low2_epi16 = a_low2_epi16;
				high2_epi16 = a_high2_epi16;

				// Shift in the new pixels for the next stage of the loop
				low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
				temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
				low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

				// Apply the even reconstruction filter to the lowpass band
				even_epi16 = low1_epi16;
				temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
				even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
				even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
				even_epi16 = _mm_srai_epi16(even_epi16, 3);
				temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
				even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

				// Shift in the next four highpass coefficients
				high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
				temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
				high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

				// Add the highpass correction and divide by two
				even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
				even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
				even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
				even_epi16 = _mm_srai_epi16(even_epi16, 1);

				// Apply the odd reconstruction filter to the lowpass band
				odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
				temp_epi16 = low1_epi16;
				odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
				odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
				odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
				temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
				odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

				// Subtract the highpass correction and divide by two
				odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
				odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
				odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
				odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

				// Interleave the four even and four odd results
				out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

				// Reduce the precision to eight bits
				out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
				out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

				// Combine the new output values with the two values from the previous phase
				out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
				temp = _mm_cvtsi128_si32(out_epi16);
				out_epi16 = _mm_insert_epi16(out_epi16, a_even_value, 0);
				out_epi16 = _mm_insert_epi16(out_epi16, a_odd_value, 1);

				// Save the eight u chroma values for packing later
				ag2_output_epi16 = out_epi16;

				// Save the remaining two output values
				a_even_value = (short)temp;
				a_odd_value = (short)(temp >> 16);

				// The second eight lowpass coefficients are the current values in the next iteration
				a_low1_epi16 = a_low2_epi16;

				// The second eight highpass coefficients are the current values in the next iteration
				a_high1_epi16 = a_high2_epi16;

			
				a1_output_epi16 = ag1_output_epi16;

				a1_output_epi16 = _mm_adds_epi16(a1_output_epi16, limiterRGB); 
				a1_output_epi16 = _mm_subs_epu16(a1_output_epi16, limiterRGB);

				a1_output_epi16 = _mm_slli_epi16(a1_output_epi16, 4);  //12-bit
				a1_output_epi16 = _mm_subs_epu16(a1_output_epi16, _mm_set1_epi16(alphacompandDCoffset)); //12-bit  -16
				a1_output_epi16 = _mm_slli_epi16(a1_output_epi16, 3);  //15-bit
				a1_output_epi16 = _mm_mulhi_epi16(a1_output_epi16, _mm_set1_epi16(alphacompandGain));

				a1_output_epi16 = _mm_adds_epi16(a1_output_epi16, limiterRGB); //8-bit limit
				a1_output_epi16 = _mm_subs_epu16(a1_output_epi16, limiterRGB);
			}
			
			

			//r_output_epi16  = ((rg_output_epi16 - 32768)<<1)+gg_output_epi16
			//r_output_epi16  = ((rg_output_epi16>>3 - 32768>>3))+gg_output_epi16>>4


			g1_output_epi16 = gg1_output_epi16;

			r1_output_epi16 = rg1_output_epi16;			

			b1_output_epi16 = bg1_output_epi16;


			r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, limiterRGB);
			r1_output_epi16 = _mm_subs_epu16(r1_output_epi16, limiterRGB);

			g1_output_epi16 = _mm_adds_epi16(g1_output_epi16, limiterRGB);
			g1_output_epi16 = _mm_subs_epu16(g1_output_epi16, limiterRGB);

			b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, limiterRGB);
			b1_output_epi16 = _mm_subs_epu16(b1_output_epi16, limiterRGB);
			
			
			 

			zero_epi128 = _mm_setzero_si128();


			// Compute Y,U,V
			rgb_epi32 = _mm_unpacklo_epi16(r1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1a_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1a_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1a_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);
			rgb_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1b_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1b_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1b_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);

			rgb_epi32 = _mm_unpacklo_epi16(g1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			rgb_epi32 = _mm_unpacklo_epi16(b1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_ps = _mm_set_ps1(fy_offset);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fu_offset);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fv_offset);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_epi32 = _mm_cvtps_epi32(y1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
			y1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			y1_output_epi16 = _mm_adds_epi16(y1_output_epi16, limiter);
			y1_output_epi16 = _mm_subs_epu16(y1_output_epi16, limiter);
			y1_output_epi16 = _mm_srli_epi16(y1_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(u1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
			u1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			u1_output_epi16 = _mm_adds_epi16(u1_output_epi16, limiter);
			u1_output_epi16 = _mm_subs_epu16(u1_output_epi16, limiter);
			u1_output_epi16 = _mm_srli_epi16(u1_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(v1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
			v1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			v1_output_epi16 = _mm_adds_epi16(v1_output_epi16, limiter);
			v1_output_epi16 = _mm_subs_epu16(v1_output_epi16, limiter);
			v1_output_epi16 = _mm_srli_epi16(v1_output_epi16, shift);


			g2_output_epi16 = gg2_output_epi16;

			r2_output_epi16 = rg2_output_epi16;

			b2_output_epi16 = bg2_output_epi16;
			


			 r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, limiterRGB);
			 r2_output_epi16 = _mm_subs_epu16(r2_output_epi16, limiterRGB);

			 g2_output_epi16 = _mm_adds_epi16(g2_output_epi16, limiterRGB);
			 g2_output_epi16 = _mm_subs_epu16(g2_output_epi16, limiterRGB);

			 b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, limiterRGB);
			 b2_output_epi16 = _mm_subs_epu16(b2_output_epi16, limiterRGB);

			 if(decoder->codec.num_channels == 4)
			 {
				a2_output_epi16 = ag2_output_epi16;
				 
				a2_output_epi16 = _mm_adds_epi16(a2_output_epi16, limiterRGB); //12-bit limit
				a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, limiterRGB);

				a2_output_epi16 = _mm_slli_epi16(a2_output_epi16, 4);  //12-bit
				a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, _mm_set1_epi16(alphacompandDCoffset)); //12-bit  -16
				a2_output_epi16 = _mm_slli_epi16(a2_output_epi16, 3);  //15-bit
				a2_output_epi16 = _mm_mulhi_epi16(a2_output_epi16, _mm_set1_epi16(alphacompandGain));

				a2_output_epi16 = _mm_adds_epi16(a2_output_epi16, limiterRGB); //8-bit limit
				a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, limiterRGB);
			 }	

			// Compute Y,U,V
			rgb_epi32 = _mm_unpacklo_epi16(r2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1a_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1a_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1a_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);
			rgb_epi32 = _mm_unpackhi_epi16(r2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1b_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1b_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1b_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);

			rgb_epi32 = _mm_unpacklo_epi16(g2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(g2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			rgb_epi32 = _mm_unpacklo_epi16(b2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(b2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_ps = _mm_set_ps1(fy_offset);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fu_offset);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fv_offset);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_epi32 = _mm_cvtps_epi32(y1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
			y2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			y2_output_epi16 = _mm_adds_epi16(y2_output_epi16, limiter);
			y2_output_epi16 = _mm_subs_epu16(y2_output_epi16, limiter);
			y2_output_epi16 = _mm_srli_epi16(y2_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(u1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
			u2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			u2_output_epi16 = _mm_adds_epi16(u2_output_epi16, limiter);
			u2_output_epi16 = _mm_subs_epu16(u2_output_epi16, limiter);
			u2_output_epi16 = _mm_srli_epi16(u2_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(v1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
			v2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
			v2_output_epi16 = _mm_adds_epi16(v2_output_epi16, limiter);
			v2_output_epi16 = _mm_subs_epu16(v2_output_epi16, limiter);
			v2_output_epi16 = _mm_srli_epi16(v2_output_epi16, shift);

			/***** Interleave the luma and chroma values *****/
			if(decoder->frame.format == DECODED_FORMAT_R408 || decoder->frame.format == DECODED_FORMAT_V408)
			{
				__m128i y_epi8 = _mm_packus_epi16(y1_output_epi16, y2_output_epi16); //pack to 8-bit
				__m128i u_epi8 = _mm_packus_epi16(u1_output_epi16, u2_output_epi16); //pack to 8-bit
				__m128i v_epi8 = _mm_packus_epi16(v1_output_epi16, v2_output_epi16); //pack to 8-bit
				__m128i a_epi8 = _mm_packus_epi16(a1_output_epi16, a2_output_epi16); //pack to 8-bit


				if(decoder->codec.num_channels == 3)
					a_epi8 = _mm_set1_epi8(0xff);
			 

				if(decoder->frame.format == COLOR_FORMAT_V408) // UYVA
				{
					__m128i UY,VA,UYVA;

					UY = _mm_unpacklo_epi8(u_epi8, y_epi8);
					VA = _mm_unpacklo_epi8(v_epi8, a_epi8);
					UYVA = _mm_unpacklo_epi16(UY, VA);
					_mm_storeu_si128(outptr++, UYVA);

					UYVA = _mm_unpackhi_epi16(UY, VA);
					_mm_storeu_si128(outptr++, UYVA);

					UY = _mm_unpackhi_epi8(u_epi8, y_epi8);
					VA = _mm_unpackhi_epi8(v_epi8, a_epi8);
					UYVA = _mm_unpacklo_epi16(UY, VA);
					_mm_storeu_si128(outptr++, UYVA);

					UYVA = _mm_unpackhi_epi16(UY, VA);
					_mm_storeu_si128(outptr++, UYVA);
				}
				else //r408 AYUV
				{
					__m128i AY,UV,AYUV;
					__m128i offsetR408_epi8 =  _mm_set1_epi8(16);

					y_epi8 = _mm_subs_epu8(y_epi8, offsetR408_epi8);

					AY = _mm_unpacklo_epi8(a_epi8, y_epi8);
					UV = _mm_unpacklo_epi8(u_epi8, v_epi8);
					AYUV = _mm_unpacklo_epi16(AY, UV);
					_mm_storeu_si128(outptr++, AYUV);

					AYUV = _mm_unpackhi_epi16(AY, UV);
					_mm_storeu_si128(outptr++, AYUV);

					AY = _mm_unpackhi_epi8(a_epi8, y_epi8);
					UV = _mm_unpackhi_epi8(u_epi8, v_epi8);
					AYUV = _mm_unpacklo_epi16(AY, UV);
					_mm_storeu_si128(outptr++, AYUV);

					AYUV = _mm_unpackhi_epi16(AY, UV);
					_mm_storeu_si128(outptr++, AYUV);
				}
			}
		}

		// Should have exited the loop with the column equal to the post processing column
	//	assert(column == post_column);

		colptr = (uint8_t *)outptr;
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column ++)
		{
			int re,ge,be,ae;
			int ro,go,bo,ao;
			int ye,yo;
			int ue,uo,ve,vo;


			/***** First pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += g_lowpass_ptr[column - 1];
			even -= g_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += g_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += g_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the luma result for later output in the correct order
		//	y1_even_value = even;
			ge = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= g_lowpass_ptr[column - 1];
			odd += g_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += g_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= g_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the luma result for later output in the correct order
		//	y1_odd_value = odd;
			go = odd;


			/***** Pair of u chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += b_lowpass_ptr[column - 1];
			even -= b_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += b_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += b_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the u chroma result for later output in the correct order
		//	b_even_value = even;
			be = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= b_lowpass_ptr[column - 1];
			odd += b_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += b_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= b_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the u chroma result for later output in the correct order
		//	b_odd_value = odd;
			bo = odd;



			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += r_lowpass_ptr[column - 1];
			even -= r_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += r_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += r_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the v chroma result for later output in the correct order
		//	r_even_value = even;
			re = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= r_lowpass_ptr[column - 1];
			odd += r_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += r_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= r_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the v chroma result for later output in the correct order
		//	r_odd_value = odd;
			ro = odd;


			// Apply the even reconstruction filter to the lowpass band
			if(decoder->codec.num_channels == 4)
			{
				even = 0;
				even += a_lowpass_ptr[column - 1];
				even -= a_lowpass_ptr[column + 1];
				even += 4; //DAN20050921
				even >>= 3;
				even += a_lowpass_ptr[column + 0];

				// Add the highpass correction
				even += a_highpass_ptr[column];
				even = DivideByShift(even, 1);

				// Reduce the precision to eight bits
			//	even >>= descale_shift;

				// Save the v chroma result for later output in the correct order
			//	a_even_value = even;
				ae = even << 4; //12-bit
				
				ae -= alphacompandDCoffset;
				ae <<= 3; //15-bit
				ae *= alphacompandGain;
				ae >>= 16; //12-bit
				ae >>= 4; //8-bit
				if (ae < 0) ae = 0; else if (ae > 255) ae = 255;

				// Apply the odd reconstruction filter to the lowpass band
				odd = 0;
				odd -= a_lowpass_ptr[column - 1];
				odd += a_lowpass_ptr[column + 1];
				odd += 4; //DAN20050921
				odd >>= 3;
				odd += a_lowpass_ptr[column + 0];

				// Subtract the highpass correction
				odd -= a_highpass_ptr[column];
				odd = DivideByShift(odd, 1);

				// Reduce the precision to eight bits
			//	odd >>= descale_shift;

				// Save the v chroma result for later output in the correct order
			//	a_odd_value = odd;
				ao = odd << 4; //12-bit
				
				ao -= alphacompandDCoffset;
				ao <<= 3; //15-bit
				ao *= alphacompandGain;
				ao >>= 16; //12-bit
				ao >>= 4; //8-bit
				if (ao < 0) ao = 0; else if (ao > 255) ao = 255;
			}
			else
			{
				ae = ao = 255;
			}


		// We use 16-bit fixed-point arithmetic to approximate the color conversion coefficients
		//
		// Floating point arithmetic is
		//

			switch(decoder->frame.format)
			{
			case DECODED_FORMAT_R408: //AYUV
			case DECODED_FORMAT_V408: //UYVA
				ye = ((int)((fy_rmult * (float)re + fy_gmult * (float)ge + fy_bmult * (float)be)) >> (descale_shift + 6)) + 16;
				yo = ((int)((fy_rmult * (float)ro + fy_gmult * (float)go + fy_bmult * (float)bo)) >> (descale_shift + 6)) + 16;
				ue = ((int)((fu_rmult * (float)re + fu_gmult * (float)ge + fu_bmult * (float)be)) >> (descale_shift + 6)) + 128;
				uo = ((int)((fu_rmult * (float)ro + fu_gmult * (float)go + fu_bmult * (float)bo)) >> (descale_shift + 6)) + 128;
				ve = ((int)((fv_rmult * (float)re + fv_gmult * (float)ge + fv_bmult * (float)be)) >> (descale_shift + 6)) + 128;
				vo = ((int)((fv_rmult * (float)ro + fv_gmult * (float)go + fv_bmult * (float)bo)) >> (descale_shift + 6)) + 128;
			
				if(decoder->frame.format == DECODED_FORMAT_R408)//AYUV
				{
					*(colptr++) = SATURATE_8U(ae);
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(ue);
					*(colptr++) = SATURATE_8U(ve);

					*(colptr++) = SATURATE_8U(ao);
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(uo);
					*(colptr++) = SATURATE_8U(vo);
				}
				else	//UYVA
				{
					*(colptr++) = SATURATE_8U(ue);
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(ve);
					*(colptr++) = SATURATE_8U(ae);

					*(colptr++) = SATURATE_8U(vo);
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(uo);
					*(colptr++) = SATURATE_8U(ao);
				}
				break;
			}
		}

		// Should have exited the loop at the column for right border processing
	//	assert(column == last_column);


		// Redo the last two RGB444 pixels.
		column = last_column - 1;
		colptr -= 4; //two pixels

		// Process the last luma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * g_lowpass_ptr[column + 0];
		even += 4 * g_lowpass_ptr[column - 1];
		even -= 1 * g_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += g_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the luma result for later output in the correct order
		g_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * g_lowpass_ptr[column + 0];
		odd -=  4 * g_lowpass_ptr[column - 1];
		odd +=  1 * g_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= g_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the luma result for later output in the correct order
		g_odd_value = odd;

		// Process the last u chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * b_lowpass_ptr[column + 0];
		even += 4 * b_lowpass_ptr[column - 1];
		even -= 1 * b_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += b_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		b_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * b_lowpass_ptr[column + 0];
		odd -=  4 * b_lowpass_ptr[column - 1];
		odd +=  1 * b_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= b_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		b_odd_value = odd;

		
		// Process the last v chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * r_lowpass_ptr[column + 0];
		even += 4 * r_lowpass_ptr[column - 1];
		even -= 1 * r_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += r_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		r_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * r_lowpass_ptr[column + 0];
		odd -=  4 * r_lowpass_ptr[column - 1];
		odd +=  1 * r_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= r_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		r_odd_value = odd;
		
		
		// Process the last v chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		if(decoder->codec.num_channels == 4)
		{
			// Apply the even border filter to the lowpass band
			even += 5 * a_lowpass_ptr[column + 0];
			even += 4 * a_lowpass_ptr[column - 1];
			even -= 1 * a_lowpass_ptr[column - 2];
			even += ROUNDING(even,8);
			even = DivideByShift(even, 3);

			// Add the highpass correction
			even += a_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			//even >>= descale_shift;

			// Save the v chroma result for later output in the correct order
			a_even_value = even; //12-bit
			
			a_even_value -= alphacompandDCoffset;
			a_even_value <<= 3; //15-bit
			a_even_value *= alphacompandGain;
			a_even_value >>= 16; //12-bit
			a_even_value >>= 4; //8-bit
			if (a_even_value < 0) a_even_value = 0; else if (a_even_value > 255) a_even_value = 255;

			// Apply the odd reconstruction filter to the lowpass band
			odd += 11 * a_lowpass_ptr[column + 0];
			odd -=  4 * a_lowpass_ptr[column - 1];
			odd +=  1 * a_lowpass_ptr[column - 2];
			odd += ROUNDING(odd,8);
			odd = DivideByShift(odd, 3);

			// Subtract the highpass correction
			odd -= a_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			//odd >>= descale_shift;

			// Save the v chroma result for later output in the correct order
			a_odd_value = odd; //12-bit
					
			a_odd_value -= alphacompandDCoffset;
			a_odd_value <<= 3; //15-bit
			a_odd_value *= alphacompandGain;
			a_odd_value >>= 16; //12-bit
			a_odd_value >>= 4; //8-bit
			if (a_odd_value < 0) a_odd_value = 0; else if (a_odd_value > 255) a_odd_value = 255;
		}
		else
		{
			a_even_value = a_odd_value = 255;
		}
	
		{
			int ye,yo,ue,uo,ve,vo;
			int re = r_even_value;
			int ro = r_odd_value;
			int ge = g_even_value;
			int go = g_odd_value;
			int be = b_even_value;
			int bo = b_odd_value;
			int ae = a_even_value;
			int ao = a_odd_value;

			switch(decoder->frame.format)
			{
			case DECODED_FORMAT_R408: //AYUV
			case DECODED_FORMAT_V408: //UYVA
				ye = ((int)((fy_rmult * (float)re + fy_gmult * (float)ge + fy_bmult * (float)be)) >> (descale_shift + 6)) + 16;
				yo = ((int)((fy_rmult * (float)ro + fy_gmult * (float)go + fy_bmult * (float)bo)) >> (descale_shift + 6)) + 16;
				ue = ((int)((fu_rmult * (float)re + fu_gmult * (float)ge + fu_bmult * (float)be)) >> (descale_shift + 6)) + 128;
				uo = ((int)((fu_rmult * (float)ro + fu_gmult * (float)go + fu_bmult * (float)bo)) >> (descale_shift + 6)) + 128;
				ve = ((int)((fv_rmult * (float)re + fv_gmult * (float)ge + fv_bmult * (float)be)) >> (descale_shift + 6)) + 128;
				vo = ((int)((fv_rmult * (float)ro + fv_gmult * (float)go + fv_bmult * (float)bo)) >> (descale_shift + 6)) + 128;
			
				if(decoder->frame.format == DECODED_FORMAT_R408)//AYUV
				{
					*(colptr++) = SATURATE_8U(ae);
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(ue);
					*(colptr++) = SATURATE_8U(ve);

					*(colptr++) = SATURATE_8U(ao);
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(uo);
					*(colptr++) = SATURATE_8U(vo);
				}
				else	//UYVA
				{
					*(colptr++) = SATURATE_8U(ue);
					*(colptr++) = SATURATE_8U(ye);
					*(colptr++) = SATURATE_8U(ve);
					*(colptr++) = SATURATE_8U(ae);

					*(colptr++) = SATURATE_8U(vo);
					*(colptr++) = SATURATE_8U(yo);
					*(colptr++) = SATURATE_8U(uo);
					*(colptr++) = SATURATE_8U(ao);
				}
				break;
			}
		}


		// Advance to the next row of coefficients in each channel
		g_lowpass_ptr += lowpass_pitch[0];
		b_lowpass_ptr += lowpass_pitch[1];
		r_lowpass_ptr += lowpass_pitch[2];
		g_highpass_ptr += highpass_pitch[0];
		b_highpass_ptr += highpass_pitch[1];
		r_highpass_ptr += highpass_pitch[2];
		if(decoder->codec.num_channels == 4)
		{
			a_lowpass_ptr += lowpass_pitch[3];
			a_highpass_ptr += highpass_pitch[3];
		}

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}


// Used in RT YR16 playback
// Apply the inverse horizontal transform to reconstruct a strip of rows into packed YUV pixels
void InvertHorizontalStrip16sRGB2YR16(HorizontalFilterParams)
{
	int num_channels = CODEC_NUM_CHANNELS;
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *gg_lowpass_ptr = lowpass_band[0];
	PIXEL *rg_lowpass_ptr = lowpass_band[1];
	PIXEL *bg_lowpass_ptr = lowpass_band[2];
	PIXEL *gg_highpass_ptr = highpass_band[0];
	PIXEL *rg_highpass_ptr = highpass_band[1];
	PIXEL *bg_highpass_ptr = highpass_band[2];

	uint8_t *output = output_image;

	// Process 8 luma coefficients per loop iteration
	const int column_step = 8;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;
	int post_column = last_column - (last_column % column_step);

	// Need at least four luma values of border processing up to the last column
	//const int post_border = 2;

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	//int descale_shift = (precision - 8);

	int shift = 8;
	float scale;

	float fy_rmult,fy_gmult,fy_bmult,fy_offset;
	float fu_rmult,fu_gmult,fu_bmult,fu_offset;
	float fv_rmult,fv_gmult,fv_bmult,fv_offset;

	int color_space = decoder->frame.colorspace;

	const float rgb2yuv709[3][4] =
	{
        {0.183f, 0.614f, 0.062f, 16.0f/255.0f},
        {-0.101f,-0.338f, 0.439f, 128.0f/255.0f},
        {0.439f,-0.399f,-0.040f, 128.0f/255.0f}
	};
	const float rgb2yuv601[3][4] =
	{
        {0.257f, 0.504f, 0.098f, 16.0f/255.0f},
        {-0.148f,-0.291f, 0.439f, 128.0f/255.0f},
        {0.439f,-0.368f,-0.071f, 128.0f/255.0f}
	};
	const float rgb2yuvVS601[3][4] =
	{
        {0.299f,0.587f,0.114f,0},
        {-0.172f,-0.339f,0.511f,128.0f/255.0f},
        {0.511f,-0.428f,-0.083f,128.0f/255.0f}
	};
	const float rgb2yuvVS709[3][4] =
	{
        {0.213f,0.715f,0.072f,0},
        {-0.117f,-0.394f,0.511f,128.0f/255.0f},
        {0.511f,-0.464f,-0.047f,128.0f/255.0f}
	};
	float rgb2yuv[3][4];
	//int yoffset = 16;

	switch(color_space & COLORSPACE_MASK)
	{
	case COLOR_SPACE_CG_601:
		memcpy(rgb2yuv, rgb2yuv601, 12*sizeof(float));
		break;
	default: assert(0);
	case COLOR_SPACE_CG_709:
		memcpy(rgb2yuv, rgb2yuv709, 12*sizeof(float));
		break;
	case COLOR_SPACE_VS_601:
		memcpy(rgb2yuv, rgb2yuvVS601, 12*sizeof(float));
		break;
	case COLOR_SPACE_VS_709:
		memcpy(rgb2yuv, rgb2yuvVS709, 12*sizeof(float));
		break;
	}

	scale = 4.0;

	fy_rmult = ((rgb2yuv[0][0]) * scale);
	fy_gmult = ((rgb2yuv[0][1]) * scale);
	fy_bmult = ((rgb2yuv[0][2]) * scale);
	fy_offset= ((rgb2yuv[0][3]) * 16384.0f);

	fu_rmult = ((rgb2yuv[1][0]) * scale);
	fu_gmult = ((rgb2yuv[1][1]) * scale);
	fu_bmult = ((rgb2yuv[1][2]) * scale);
	fu_offset= ((rgb2yuv[1][3]) * 16384.0f);

	fv_rmult = ((rgb2yuv[2][0]) * scale);
	fv_gmult = ((rgb2yuv[2][1]) * scale);
	fv_bmult = ((rgb2yuv[2][2]) * scale);
	fv_offset= ((rgb2yuv[2][3]) * 16384.0f);

	shift-=2;


	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);

	// Adjust the end of the fast loop if necessary for border processing
	//if (post_column > (last_column - post_border))  // DAN08112004 - ignore end row calc -- use SSE to edge.
	//	post_column -= column_step;

	// Check that there is enough margin to accommodate border processing
//	assert(post_column <= (last_column - post_border));

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
#if (1 && XMMOPT)
		__m128i gg_low1_epi16;		// Lowpass coefficients
		__m128i gg_low2_epi16;
		__m128i bg_low1_epi16;
		__m128i bg_low2_epi16;
		__m128i rg_low1_epi16;
		__m128i rg_low2_epi16;

		__m128i gg_high1_epi16;		// Highpass coefficients
		__m128i gg_high2_epi16;
		__m128i bg_high1_epi16;
		__m128i bg_high2_epi16;
		__m128i rg_high1_epi16;
		__m128i rg_high2_epi16;

		// The fast loop merges values from different phases to allow aligned stores
		__m128i *outptr = (__m128i *)&output[0];
		__m128i *Yptr128 = (__m128i *)&output[0];
		__m128i *Vptr128 = (__m128i *)&output[width*4];
		__m128i *Uptr128 = (__m128i *)&output[width*6];

		const __m128i mask_epi32 = _mm_set1_epi32(0xffff);

		__m128i limiterRGB = _mm_set1_epi16(0x7fff - 0x0fff);

#endif
		uint8_t *colptr = (uint8_t *)&output[0];
		PIXEL16U *Yptr = (PIXEL16U *)&output[0];
		PIXEL16U *Vptr = Yptr + width*2;
		PIXEL16U *Uptr = Vptr + (width);
		int32_t lastU0=0,lastV0=0;

		int32_t gg_even_value;
		int32_t bg_even_value;
		int32_t rg_even_value;
		int32_t gg_odd_value;
		int32_t bg_odd_value;
		int32_t rg_odd_value;


		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two luma output points with special filters for the left border
		int32_t even = 0;
		int32_t odd = 0;


		// Apply the even reconstruction filter to the lowpass band
		even += 11 * gg_lowpass_ptr[column + 0];
		even -=  4 * gg_lowpass_ptr[column + 1];
		even +=  1 * gg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += gg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		gg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * gg_lowpass_ptr[column + 0];
		odd += 4 * gg_lowpass_ptr[column + 1];
		odd -= 1 * gg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= gg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
	//	odd >>= descale_shift;

		// Save the value for use in the fast loop
		gg_odd_value = odd;


		// Process the first two u chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * bg_lowpass_ptr[column + 0];
		even -=  4 * bg_lowpass_ptr[column + 1];
		even +=  1 * bg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += bg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		bg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * bg_lowpass_ptr[column + 0];
		odd += 4 * bg_lowpass_ptr[column + 1];
		odd -= 1 * bg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= bg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
	//	odd >>= descale_shift;

		// Save the value for use in the fast loop
		bg_odd_value = odd;


		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * rg_lowpass_ptr[column + 0];
		even -=  4 * rg_lowpass_ptr[column + 1];
		even +=  1 * rg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += rg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		rg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * rg_lowpass_ptr[column + 0];
		odd += 4 * rg_lowpass_ptr[column + 1];
		odd -= 1 * rg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= rg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
	//	odd >>= descale_shift;

		// Save the value for use in the fast loop
		rg_odd_value = odd;

#if (1 && XMMOPT)

		// Preload the first eight lowpass luma coefficients
		gg_low1_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[0]);

		// Preload the first eight highpass luma coefficients
		gg_high1_epi16 = _mm_load_si128((__m128i *)&gg_highpass_ptr[0]);

		// Preload the first eight lowpass u chroma coefficients
		bg_low1_epi16 = _mm_load_si128((__m128i *)&bg_lowpass_ptr[0]);

		// Preload the first eight highpass u chroma coefficients
		bg_high1_epi16 = _mm_load_si128((__m128i *)&bg_highpass_ptr[0]);

		// Preload the first eight lowpass v chroma coefficients
		rg_low1_epi16 = _mm_load_si128((__m128i *)&rg_lowpass_ptr[0]);

		// Preload the first eight highpass v chroma coefficients
 		rg_high1_epi16 = _mm_load_si128((__m128i *)&rg_highpass_ptr[0]);


		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i g1_output_epi16;
			__m128i g2_output_epi16;
			__m128i r1_output_epi16;
			__m128i r2_output_epi16;
			__m128i b1_output_epi16;
			__m128i b2_output_epi16;

			__m128i gg1_output_epi16;
			__m128i gg2_output_epi16;
			__m128i rg1_output_epi16;
			__m128i rg2_output_epi16;
			__m128i bg1_output_epi16;
			__m128i bg2_output_epi16;

			__m128i y1_output_epi16;
			__m128i y2_output_epi16;
			__m128i u1_output_epi16;
			__m128i u2_output_epi16;
			__m128i v1_output_epi16;
			__m128i v2_output_epi16;

			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i temp_epi32;
			__m128i tempB_epi32;
			__m128i rgb_epi32;
			__m128i zero_epi128;
			__m128  temp_ps;
			__m128  rgb_ps;
			__m128	y1a_ps;
			__m128	y1b_ps;
			__m128	u1a_ps;
			__m128	u1b_ps;
			__m128	v1a_ps;
			__m128	v1b_ps;


			__m128i out_epi16;		// Reconstructed data
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			__m128i low1_epi16;
			__m128i low2_epi16;
			__m128i high1_epi16;
			__m128i high2_epi16;

			//DAN031304 -- correct inverse filter
			__m128i half_epi16 = _mm_set1_epi16(4);
			__m128i offset_epi16 = _mm_set1_epi16(2048);

			uint32_t temp;		// Temporary register for last two values


			/***** Compute the first eight luma output values *****/

			// Preload the second eight lowpass coefficients
			gg_low2_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			gg_high2_epi16 = _mm_load_si128((__m128i *)&gg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = gg_low1_epi16;
			high1_epi16 = gg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);


			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_odd_value, 1);

			// Save the eight luma values for packing later
			gg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			gg_even_value = (short)temp;
			gg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = gg_low2_epi16;
			high2_epi16 = gg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_odd_value, 1);

			// Save the eight luma values for packing later
			gg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			gg_even_value = (short)temp;
			gg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are used later in the loop
			gg_low1_epi16 = gg_low2_epi16;

			// The second eight highpass coefficients are used later in the loop
			gg_high1_epi16 = gg_high2_epi16;


			/***** Compute the first eight u chroma output values *****/

			// Preload the second eight lowpass coefficients
			bg_low2_epi16 = _mm_load_si128((__m128i *)&bg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			bg_high2_epi16 = _mm_load_si128((__m128i *)&bg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = bg_low1_epi16;
			high1_epi16 = bg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			bg_even_value = (short)temp;
			bg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight u chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = bg_low2_epi16;
			high2_epi16 = bg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			bg_even_value = (short)temp;
			bg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			bg_low1_epi16 = bg_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			bg_high1_epi16 = bg_high2_epi16;



			/***** Compute the first eight v chroma output values *****/

			// Preload the second eight lowpass coefficients
			rg_low2_epi16 = _mm_load_si128((__m128i *)&rg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			rg_high2_epi16 = _mm_load_si128((__m128i *)&rg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = rg_low1_epi16;
			high1_epi16 = rg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			rg_even_value = (short)temp;
			rg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight v chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = rg_low2_epi16;
			high2_epi16 = rg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			rg_even_value = (short)temp;
			rg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			rg_low1_epi16 = rg_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			rg_high1_epi16 = rg_high2_epi16;












			//r_output_epi16  = ((rg_output_epi16 - 32768)<<1)+gg_output_epi16
			//r_output_epi16  = ((rg_output_epi16>>3 - 32768>>3))+gg_output_epi16>>4


			g1_output_epi16 = gg1_output_epi16;

			r1_output_epi16 = rg1_output_epi16;//_mm_srli_epi16(rg1_output_epi16,2);
		//	r1_output_epi16 = _mm_subs_epi16(r1_output_epi16, value128_epi32);
		//	r1_output_epi16 = _mm_slli_epi16(r1_output_epi16,1);
		//	r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, g1_output_epi16);

			b1_output_epi16 = bg1_output_epi16;//_mm_srli_epi16(bg1_output_epi16,2);
		//	b1_output_epi16 = _mm_subs_epi16(b1_output_epi16, value128_epi32);
		//	b1_output_epi16 = _mm_slli_epi16(b1_output_epi16,1);
		//	b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, g1_output_epi16);


			 r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, limiterRGB);
			 r1_output_epi16 = _mm_subs_epu16(r1_output_epi16, limiterRGB);

			 g1_output_epi16 = _mm_adds_epi16(g1_output_epi16, limiterRGB);
			 g1_output_epi16 = _mm_subs_epu16(g1_output_epi16, limiterRGB);

			 b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, limiterRGB);
			 b1_output_epi16 = _mm_subs_epu16(b1_output_epi16, limiterRGB);


			zero_epi128 = _mm_setzero_si128();


			// Compute Y,U,V
			rgb_epi32 = _mm_unpacklo_epi16(r1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1a_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1a_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1a_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);
			rgb_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1b_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1b_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1b_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);

			rgb_epi32 = _mm_unpacklo_epi16(g1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			rgb_epi32 = _mm_unpacklo_epi16(b1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_ps = _mm_set_ps1(fy_offset);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fu_offset);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fv_offset);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_epi32 = _mm_cvtps_epi32(y1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
			y1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);

			y1_output_epi16 = _mm_slli_epi16(y1_output_epi16, 2);
			_mm_store_si128(Yptr128++, y1_output_epi16);
	//		y1_output_epi16 = _mm_adds_epi16(y1_output_epi16, limiter);
	//		y1_output_epi16 = _mm_subs_epu16(y1_output_epi16, limiter);
	//		y1_output_epi16 = _mm_srli_epi16(y1_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(u1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
			u1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
	//		u1_output_epi16 = _mm_adds_epi16(u1_output_epi16, limiter);
	//		u1_output_epi16 = _mm_subs_epu16(u1_output_epi16, limiter);
	//		u1_output_epi16 = _mm_srli_epi16(u1_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(v1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
			v1_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
	//		v1_output_epi16 = _mm_adds_epi16(v1_output_epi16, limiter);
	//		v1_output_epi16 = _mm_subs_epu16(v1_output_epi16, limiter);
	//		v1_output_epi16 = _mm_srli_epi16(v1_output_epi16, shift);






			g2_output_epi16 = gg2_output_epi16;//_mm_srli_epi16(gg2_output_epi16,2);

			r2_output_epi16 = rg2_output_epi16;//_mm_srli_epi16(rg2_output_epi16,2);
		//	r2_output_epi16 = _mm_subs_epi16(r2_output_epi16, value128_epi32);
		//	r2_output_epi16 = _mm_slli_epi16(r2_output_epi16,1);
		//	r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, g2_output_epi16);

			b2_output_epi16 = bg2_output_epi16;//_mm_srli_epi16(bg2_output_epi16,2);
		//	b2_output_epi16 = _mm_subs_epi16(b2_output_epi16, value128_epi32);
		//	b2_output_epi16 = _mm_slli_epi16(b2_output_epi16,1);
		//	b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, g2_output_epi16);



			 r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, limiterRGB);
			 r2_output_epi16 = _mm_subs_epu16(r2_output_epi16, limiterRGB);

			 g2_output_epi16 = _mm_adds_epi16(g2_output_epi16, limiterRGB);
			 g2_output_epi16 = _mm_subs_epu16(g2_output_epi16, limiterRGB);

			 b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, limiterRGB);
			 b2_output_epi16 = _mm_subs_epu16(b2_output_epi16, limiterRGB);


			// Compute Y,U,V
			rgb_epi32 = _mm_unpacklo_epi16(r2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1a_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1a_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1a_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);
			rgb_epi32 = _mm_unpackhi_epi16(r2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			y1b_ps = _mm_mul_ps(_mm_set_ps1(fy_rmult), rgb_ps);
			u1b_ps = _mm_mul_ps(_mm_set_ps1(fu_rmult), rgb_ps);
			v1b_ps = _mm_mul_ps(_mm_set_ps1(fv_rmult), rgb_ps);

			rgb_epi32 = _mm_unpacklo_epi16(g2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(g2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_gmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_gmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_gmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			rgb_epi32 = _mm_unpacklo_epi16(b2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			rgb_epi32 = _mm_unpackhi_epi16(b2_output_epi16, zero_epi128);
			rgb_ps = _mm_cvtepi32_ps(rgb_epi32);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fy_bmult), rgb_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fu_bmult), rgb_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_mul_ps(_mm_set_ps1(fv_bmult), rgb_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_ps = _mm_set_ps1(fy_offset);
			y1a_ps = _mm_add_ps(y1a_ps, temp_ps);
			y1b_ps = _mm_add_ps(y1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fu_offset);
			u1a_ps = _mm_add_ps(u1a_ps, temp_ps);
			u1b_ps = _mm_add_ps(u1b_ps, temp_ps);
			temp_ps = _mm_set_ps1(fv_offset);
			v1a_ps = _mm_add_ps(v1a_ps, temp_ps);
			v1b_ps = _mm_add_ps(v1b_ps, temp_ps);

			temp_epi32 = _mm_cvtps_epi32(y1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(y1b_ps);
			y2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);

			y2_output_epi16 = _mm_slli_epi16(y2_output_epi16, 2);
			_mm_store_si128(Yptr128++, y2_output_epi16);
	//		y2_output_epi16 = _mm_adds_epi16(y2_output_epi16, limiter);
	//		y2_output_epi16 = _mm_subs_epu16(y2_output_epi16, limiter);
	//		y2_output_epi16 = _mm_srli_epi16(y2_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(u1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(u1b_ps);
			u2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
	//		u2_output_epi16 = _mm_adds_epi16(u2_output_epi16, limiter);
	//		u2_output_epi16 = _mm_subs_epu16(u2_output_epi16, limiter);
	//		u2_output_epi16 = _mm_srli_epi16(u2_output_epi16, shift);

			temp_epi32 = _mm_cvtps_epi32(v1a_ps);
			tempB_epi32 = _mm_cvtps_epi32(v1b_ps);
			v2_output_epi16 = _mm_packs_epi32(temp_epi32, tempB_epi32);
	//		v2_output_epi16 = _mm_adds_epi16(v2_output_epi16, limiter);
	//		v2_output_epi16 = _mm_subs_epu16(v2_output_epi16, limiter);
	//		v2_output_epi16 = _mm_srli_epi16(v2_output_epi16, shift);


			// 4:4:4 to 4:2:2 // U = (U1+U2)/2
#if 0
			temp_epi16 = _mm_srli_si128(u1_output_epi16, 2);
			u1_output_epi16 = _mm_adds_epi16(u1_output_epi16, temp_epi16);
			u1_output_epi16 = _mm_srli_epi16(u1_output_epi16, 1);
			temp_epi16 = _mm_srli_si128(u2_output_epi16, 2);
			u2_output_epi16 = _mm_adds_epi16(u2_output_epi16, temp_epi16);
			u2_output_epi16 = _mm_srli_epi16(u2_output_epi16, 1);
			temp_epi16 = _mm_srli_si128(v1_output_epi16, 2);
			v1_output_epi16 = _mm_adds_epi16(v1_output_epi16, temp_epi16);
			v1_output_epi16 = _mm_srli_epi16(v1_output_epi16, 1);
			temp_epi16 = _mm_srli_si128(v2_output_epi16, 2);
			v2_output_epi16 = _mm_adds_epi16(v2_output_epi16, temp_epi16);
			v2_output_epi16 = _mm_srli_epi16(v2_output_epi16, 1);
			u1_output_epi16 = _mm_and_si128(u1_output_epi16, mask_epi32);
			u2_output_epi16 = _mm_and_si128(u2_output_epi16, mask_epi32);
			v1_output_epi16 = _mm_and_si128(v1_output_epi16, mask_epi32);
			v2_output_epi16 = _mm_and_si128(v2_output_epi16, mask_epi32);
#else
			// 4:4:4 to 4:2:2 // U = (U1+2.U2+U3)/4 (correct centre weighting)
			{
				__m128i double1_epi16;
				__m128i double2_epi16;
				__m128i left1_epi16;
				__m128i left2_epi16;
				__m128i right1_epi16;
				__m128i right2_epi16;
				/*int i;
				for(i=0;i<16;i++)
                    gg_lowpass_ptr[i] = i+1;
				u1_output_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[0]);
				u2_output_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[8]);
				v1_output_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[0]);
				v2_output_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[8]);*/

				if(column == 0)
				{
					lastU0 = _mm_extract_epi16(u1_output_epi16, 0);
					lastV0 = _mm_extract_epi16(v1_output_epi16, 0);
				}

				double1_epi16 = _mm_adds_epu16(u1_output_epi16, u1_output_epi16);
				double2_epi16 = _mm_adds_epu16(u2_output_epi16, u2_output_epi16);
				left1_epi16 = _mm_slli_si128(u1_output_epi16, 2);
				left2_epi16 = _mm_slli_si128(u2_output_epi16, 2);
				left1_epi16 = _mm_insert_epi16(left1_epi16, lastU0, 0);
				left2_epi16 = _mm_insert_epi16(left2_epi16, _mm_extract_epi16(u1_output_epi16, 7), 0);
				right1_epi16 = _mm_srli_si128(u1_output_epi16, 2);
				right2_epi16 = _mm_srli_si128(u2_output_epi16, 2);
				lastU0 = _mm_extract_epi16(u2_output_epi16, 7);

				u1_output_epi16 = _mm_adds_epu16(double1_epi16, left1_epi16);
				u1_output_epi16 = _mm_adds_epu16(u1_output_epi16, right1_epi16);
				u1_output_epi16 = _mm_srli_epi16(u1_output_epi16, 2);
				u2_output_epi16 = _mm_adds_epu16(double2_epi16, left2_epi16);
				u2_output_epi16 = _mm_adds_epu16(u2_output_epi16, right2_epi16);
				u2_output_epi16 = _mm_srli_epi16(u2_output_epi16, 2);

				u1_output_epi16 = _mm_and_si128(u1_output_epi16, mask_epi32);
				u2_output_epi16 = _mm_and_si128(u2_output_epi16, mask_epi32);

				double1_epi16 = _mm_adds_epu16(v1_output_epi16, v1_output_epi16);
				double2_epi16 = _mm_adds_epu16(v2_output_epi16, v2_output_epi16);
				left1_epi16 = _mm_slli_si128(v1_output_epi16, 2);
				left2_epi16 = _mm_slli_si128(v2_output_epi16, 2);
				left1_epi16 = _mm_insert_epi16(left1_epi16, lastV0, 0);
				left2_epi16 = _mm_insert_epi16(left2_epi16, _mm_extract_epi16(v1_output_epi16, 7), 0);
				right1_epi16 = _mm_srli_si128(v1_output_epi16, 2);
				right2_epi16 = _mm_srli_si128(v2_output_epi16, 2);
				lastV0 = _mm_extract_epi16(v2_output_epi16, 7);

				v1_output_epi16 = _mm_adds_epu16(double1_epi16, left1_epi16);
				v1_output_epi16 = _mm_adds_epu16(v1_output_epi16, right1_epi16);
				v1_output_epi16 = _mm_srli_epi16(v1_output_epi16, 2);
				v2_output_epi16 = _mm_adds_epu16(double2_epi16, left2_epi16);
				v2_output_epi16 = _mm_adds_epu16(v2_output_epi16, right2_epi16);
				v2_output_epi16 = _mm_srli_epi16(v2_output_epi16, 2);

				v1_output_epi16 = _mm_and_si128(v1_output_epi16, mask_epi32);
				v2_output_epi16 = _mm_and_si128(v2_output_epi16, mask_epi32);



			}
#endif
			u1_output_epi16 = _mm_packs_epi32 (u1_output_epi16, u2_output_epi16);
			v1_output_epi16 = _mm_packs_epi32 (v1_output_epi16, v2_output_epi16);


			u1_output_epi16 = _mm_slli_epi16(u1_output_epi16, 2);
			v1_output_epi16 = _mm_slli_epi16(v1_output_epi16, 2);

			_mm_store_si128(Vptr128++, v1_output_epi16);
			_mm_store_si128(Uptr128++, u1_output_epi16);

		}

		// Should have exited the loop with the column equal to the post processing column
	//	assert(column == post_column);

		colptr = (uint8_t *)outptr;

		Yptr = (PIXEL16U *)outptr;
		Vptr = Yptr + width*2;
		Uptr = Vptr + (width);

#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column ++)
		{
			int re,ge,be;
			int ro,go,bo;
			int ye,yo,u,v;


			/***** First pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += gg_lowpass_ptr[column - 1];
			even -= gg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += gg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += gg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the luma result for later output in the correct order
		//	y1_even_value = even;
			ge = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= gg_lowpass_ptr[column - 1];
			odd += gg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += gg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= gg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the luma result for later output in the correct order
		//	y1_odd_value = odd;
			go = odd;


			/***** Pair of u chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += bg_lowpass_ptr[column - 1];
			even -= bg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += bg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += bg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the u chroma result for later output in the correct order
		//	bg_even_value = even;
			be = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= bg_lowpass_ptr[column - 1];
			odd += bg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += bg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= bg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the u chroma result for later output in the correct order
		//	bg_odd_value = odd;
			bo = odd;



			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += rg_lowpass_ptr[column - 1];
			even -= rg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += rg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += rg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
		//	even >>= descale_shift;

			// Save the v chroma result for later output in the correct order
		//	rg_even_value = even;
			re = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= rg_lowpass_ptr[column - 1];
			odd += rg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += rg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= rg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the v chroma result for later output in the correct order
		//	rg_odd_value = odd;
			ro = odd;


		// We use 16-bit fixed-point arithmetic to approximate the color conversion coefficients
		//
		// Floating point arithmetic is
		//
			ye = ((int)((fy_rmult * (float)re + fy_gmult * (float)ge + fy_bmult * (float)be))) + (int)fy_offset;
			yo = ((int)((fy_rmult * (float)ro + fy_gmult * (float)go + fy_bmult * (float)bo))) + (int)fy_offset;



#if 1	// 4:4:4 to 4:2:2 // U = (U1+U2)/2
			u  = ((int)((fu_rmult * (float)(re+ro) + fu_gmult * (float)(ge+go) + fu_bmult * (float)(be+bo))) >> (1)) + (int)fu_offset;
			v  = ((int)((fv_rmult * (float)(re+ro) + fv_gmult * (float)(ge+go) + fv_bmult * (float)(be+bo))) >> (1)) + (int)fv_offset;
#else	// 4:4:4 to 4:2:2 // U = (U1+2.U2+U3)/4 (correct centre weighting)
			//TODO non-SSE2 version of this down sample
#endif
			// Output the luma and chroma values in the correct order
			*(Yptr++) = SATURATE_16U(ye<<2);
			*(Yptr++) = SATURATE_16U(yo<<2);
			*(Vptr++) = SATURATE_16U(v<<2);
			*(Uptr++) = SATURATE_16U(u<<2);
		}




		// Redo the last two RGB444 pixels.
		column = last_column - 1;
		Yptr -= 2; //two pixels
		Uptr--;
		Vptr--;

		// Process the last luma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * gg_lowpass_ptr[column + 0];
		even += 4 * gg_lowpass_ptr[column - 1];
		even -= 1 * gg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += gg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the luma result for later output in the correct order
		gg_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * gg_lowpass_ptr[column + 0];
		odd -=  4 * gg_lowpass_ptr[column - 1];
		odd +=  1 * gg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= gg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the luma result for later output in the correct order
		gg_odd_value = odd;

		// Process the last u chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * bg_lowpass_ptr[column + 0];
		even += 4 * bg_lowpass_ptr[column - 1];
		even -= 1 * bg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += bg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		bg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * bg_lowpass_ptr[column + 0];
		odd -=  4 * bg_lowpass_ptr[column - 1];
		odd +=  1 * bg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= bg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the u chroma result for later output in the correct order
		bg_odd_value = odd;

		// Process the last v chroma output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even border filter to the lowpass band
		even += 5 * rg_lowpass_ptr[column + 0];
		even += 4 * rg_lowpass_ptr[column - 1];
		even -= 1 * rg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += rg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
		//even >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		rg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * rg_lowpass_ptr[column + 0];
		odd -=  4 * rg_lowpass_ptr[column - 1];
		odd +=  1 * rg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= rg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the v chroma result for later output in the correct order
		rg_odd_value = odd;

		{
			int ye,yo,u,v;
			int re = rg_even_value;
			int ro = rg_odd_value;
			int ge = gg_even_value;
			int go = gg_odd_value;
			int be = bg_even_value;
			int bo = bg_odd_value;

			// We use 16-bit fixed-point arithmetic to approximate the color conversion coefficients
			//
			// Floating point arithmetic is
		//
			ye = ((int)((fy_rmult * (float)re + fy_gmult * (float)ge + fy_bmult * (float)be))) + (int)fy_offset;
			yo = ((int)((fy_rmult * (float)ro + fy_gmult * (float)go + fy_bmult * (float)bo))) + (int)fy_offset;


#if 1	// 4:4:4 to 4:2:2 // U = (U1+U2)/2
			u  = ((int)((fu_rmult * (float)(re+ro) + fu_gmult * (float)(ge+go) + fu_bmult * (float)(be+bo))) >> (1)) + (int)fu_offset;
			v  = ((int)((fv_rmult * (float)(re+ro) + fv_gmult * (float)(ge+go) + fv_bmult * (float)(be+bo))) >> (1)) + (int)fv_offset;
#else	// 4:4:4 to 4:2:2 // U = (U1+2.U2+U3)/4 (correct centre weighting)
			//TODO non-SSE2 version of this down sample
#endif
			// Output the luma and chroma values in the correct order
			*(Yptr++) = SATURATE_16U(ye<<2);
			*(Yptr++) = SATURATE_16U(yo<<2);
			*(Vptr++) = SATURATE_16U(v<<2);
			*(Uptr++) = SATURATE_16U(u<<2);
		}


		// Should have exited the loop at the column for right border processing
	//	assert(column == last_column);
		// Advance to the next row of coefficients in each channel
		gg_lowpass_ptr += lowpass_pitch[0];
		bg_lowpass_ptr += lowpass_pitch[1];
		rg_lowpass_ptr += lowpass_pitch[2];
		gg_highpass_ptr += highpass_pitch[0];
		bg_highpass_ptr += highpass_pitch[1];
		rg_highpass_ptr += highpass_pitch[2];

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}

void InvertHorizontalStrip16sRGB2v210(HorizontalFilterParams)
{
	uint8_t buffer[8200*4],*bptr = buffer;
	int width = roi.width;
	//int height = roi.height;

	//align for SSE2
	bptr += 15;
	bptr = (uint8_t*)(((uintptr_t)bptr) & ~15);

	InvertHorizontalStrip16sRGB2YR16(decoder, thread_index,
			lowpass_band, lowpass_pitch,
			highpass_band, highpass_pitch,
			bptr,
			width * 2 * 4,
			roi,
			precision,
			format);

	{
		PIXEL *plane_array[3];
		int plane_pitch[3];
		ROI newroi;

		plane_array[0] = (PIXEL *)bptr;
		plane_array[1] = (PIXEL *)(bptr + width * 4);
		plane_array[2] = (PIXEL *)(bptr + width * 6);

		plane_pitch[0] = width * 4 * 2;
		plane_pitch[1] = width * 4 * 2;
		plane_pitch[2] = width * 4 * 2;

		newroi.width = width*2;
		newroi.height = 2;

		//TODO support YU64 as well, so we could YU64 this way
		ConvertYUVStripPlanarToV210(plane_array, plane_pitch, newroi, output_image,
			output_pitch, width * 2, format, decoder->frame.colorspace, 16);

	}
}

// Used in RT B64A playback
// Apply the inverse horizontal transform to reconstruct a strip of rows into packed RG30 pixels
void InvertHorizontalStrip16sRGB2B64A(HorizontalFilterParams)
{
	int num_channels = CODEC_NUM_CHANNELS;
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *g_lowpass_ptr = lowpass_band[0];
	PIXEL *r_lowpass_ptr = lowpass_band[1];
	PIXEL *b_lowpass_ptr = lowpass_band[2];
	PIXEL *a_lowpass_ptr = lowpass_band[3];
	PIXEL *g_highpass_ptr = highpass_band[0];
	PIXEL *r_highpass_ptr = highpass_band[1];
	PIXEL *b_highpass_ptr = highpass_band[2];
	PIXEL *a_highpass_ptr = highpass_band[3];

	uint8_t *output = output_image;

	// Process 8 luma coefficients per loop iteration
	const int column_step = 8;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;
	int post_column = last_column - (last_column % column_step);

	// Need at least four luma values of border processing up to the last column
	//const int post_border = 2;

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	//int descale_shift = (precision - 8);

	int shift = 8;

	float scale;

	
	decoder->frame.alpha_Companded = 1;

	num_channels = decoder->codec.num_channels;


	scale = 4.0;

	shift-=2;


	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);

	// Adjust the end of the fast loop if necessary for border processing
	//if (post_column > (last_column - post_border))  // DAN08112004 - ignore end row calc -- use SSE to edge.
	//	post_column -= column_step;

	// Check that there is enough margin to accommodate border processing
//	assert(post_column <= (last_column - post_border));

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
#if (1 && XMMOPT)
		__m128i g_low1_epi16;		// Lowpass coefficients
		__m128i g_low2_epi16;
		__m128i b_low1_epi16;
		__m128i b_low2_epi16;
		__m128i r_low1_epi16;
		__m128i r_low2_epi16;
		__m128i a_low1_epi16;
		__m128i a_low2_epi16;

		__m128i g_high1_epi16;		// Highpass coefficients
		__m128i g_high2_epi16;
		__m128i b_high1_epi16;
		__m128i b_high2_epi16;
		__m128i r_high1_epi16;
		__m128i r_high2_epi16;
		__m128i a_high1_epi16;
		__m128i a_high2_epi16;

		// The fast loop merges values from different phases to allow aligned stores
		//__m128i *outptr = (__m128i *)&output[0];
		__m128i *B64Aptr128 = (__m128i *)&output[0];

		const __m128i a_epi16 = _mm_set1_epi16(0xfff);

		__m128i limiterRGB = _mm_set1_epi16(0x7fff - 0x0fff);

#endif
		PIXEL16U *colptr = (PIXEL16U *)&output[0];
		//uint32_t *RG30ptr = (uint32_t *)&output[0];

		int32_t g_even_value;
		int32_t b_even_value;
		int32_t r_even_value;
		int32_t a_even_value;
		int32_t g_odd_value;
		int32_t b_odd_value;
		int32_t r_odd_value;
		int32_t a_odd_value;


		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two luma output points with special filters for the left border
		int32_t even = 0;
		int32_t odd = 0;


		// Apply the even reconstruction filter to the lowpass band
		even += 11 * g_lowpass_ptr[column + 0];
		even -=  4 * g_lowpass_ptr[column + 1];
		even +=  1 * g_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += g_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		g_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * g_lowpass_ptr[column + 0];
		odd += 4 * g_lowpass_ptr[column + 1];
		odd -= 1 * g_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= g_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
	//	odd >>= descale_shift;

		// Save the value for use in the fast loop
		g_odd_value = odd;


		// Process the first two u chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * b_lowpass_ptr[column + 0];
		even -=  4 * b_lowpass_ptr[column + 1];
		even +=  1 * b_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += b_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		b_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * b_lowpass_ptr[column + 0];
		odd += 4 * b_lowpass_ptr[column + 1];
		odd -= 1 * b_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= b_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
	//	odd >>= descale_shift;

		// Save the value for use in the fast loop
		b_odd_value = odd;


		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * r_lowpass_ptr[column + 0];
		even -=  4 * r_lowpass_ptr[column + 1];
		even +=  1 * r_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += r_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		r_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * r_lowpass_ptr[column + 0];
		odd += 4 * r_lowpass_ptr[column + 1];
		odd -= 1 * r_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= r_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
		//odd >>= descale_shift;

		// Save the value for use in the fast loop
		r_odd_value = odd;


		if(num_channels == 4)
		{
			// Process the first two v chroma output points with special filters for the left border
			even = 0;
			odd = 0;

			// Apply the even reconstruction filter to the lowpass band
			even += 11 * a_lowpass_ptr[column + 0];
			even -=  4 * a_lowpass_ptr[column + 1];
			even +=  1 * a_lowpass_ptr[column + 2];
			even += ROUNDING(even,8);
			even = DivideByShift(even, 3);

			// Add the highpass correction
			even += a_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			//even >>= descale_shift;

			// Save the value for use in the fast loop
			a_even_value = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd += 5 * a_lowpass_ptr[column + 0];
			odd += 4 * a_lowpass_ptr[column + 1];
			odd -= 1 * a_lowpass_ptr[column + 2];
			odd += ROUNDING(odd,8);
			odd = DivideByShift(odd, 3);

			// Subtract the highpass correction
			odd -= a_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
		//	odd >>= descale_shift;

			// Save the value for use in the fast l}oop
			a_odd_value = odd;
		}

#if (1 && XMMOPT)

		// Preload the first eight lowpass luma coefficients
		g_low1_epi16 = _mm_load_si128((__m128i *)&g_lowpass_ptr[0]);

		// Preload the first eight highpass luma coefficients
		g_high1_epi16 = _mm_load_si128((__m128i *)&g_highpass_ptr[0]);

		// Preload the first eight lowpass u chroma coefficients
		b_low1_epi16 = _mm_load_si128((__m128i *)&b_lowpass_ptr[0]);

		// Preload the first eight highpass u chroma coefficients
		b_high1_epi16 = _mm_load_si128((__m128i *)&b_highpass_ptr[0]);

		// Preload the first eight lowpass v chroma coefficients
		r_low1_epi16 = _mm_load_si128((__m128i *)&r_lowpass_ptr[0]);

		// Preload the first eight highpass v chroma coefficients
 		r_high1_epi16 = _mm_load_si128((__m128i *)&r_highpass_ptr[0]);

		if(num_channels == 4)
		{
			// Preload the first eight lowpass v chroma coefficients
			a_low1_epi16 = _mm_load_si128((__m128i *)&a_lowpass_ptr[0]);

			// Preload the first eight highpass v chroma coefficients
 			a_high1_epi16 = _mm_load_si128((__m128i *)&a_highpass_ptr[0]);
		}


		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i g1_output_epi16;
			__m128i g2_output_epi16;
			__m128i r1_output_epi16;
			__m128i r2_output_epi16;
			__m128i b1_output_epi16;
			__m128i b2_output_epi16;
			__m128i a1_output_epi16;
			__m128i a2_output_epi16;

			__m128i gg1_output_epi16;
			__m128i gg2_output_epi16;
			__m128i rg1_output_epi16;
			__m128i rg2_output_epi16;
			__m128i bg1_output_epi16;
			__m128i bg2_output_epi16;
			__m128i ag1_output_epi16;
			__m128i ag2_output_epi16;

			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i zero_epi128;

			//__m128i rr_epi32;
			//__m128i g_epi32;
			//__m128i bb_epi32;

			__m128i out_epi16;		// Reconstructed data
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			__m128i low1_epi16;
			__m128i low2_epi16;
			__m128i high1_epi16;
			__m128i high2_epi16;

			//DAN031304 -- correct inverse filter
			__m128i half_epi16 = _mm_set1_epi16(4);
			__m128i offset_epi16 = _mm_set1_epi16(2048);

			uint32_t temp;		// Temporary register for last two values

			/***** Compute the first eight luma output values *****/

			// Preload the second eight lowpass coefficients
			g_low2_epi16 = _mm_load_si128((__m128i *)&g_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			g_high2_epi16 = _mm_load_si128((__m128i *)&g_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = g_low1_epi16;
			high1_epi16 = g_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);


			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, g_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, g_odd_value, 1);

			// Save the eight luma values for packing later
			gg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			g_even_value = (short)temp;
			g_odd_value = (short)(temp >> 16);


			/***** Compute the second eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = g_low2_epi16;
			high2_epi16 = g_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, g_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, g_odd_value, 1);

			// Save the eight luma values for packing later
			gg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			g_even_value = (short)temp;
			g_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are used later in the loop
			g_low1_epi16 = g_low2_epi16;

			// The second eight highpass coefficients are used later in the loop
			g_high1_epi16 = g_high2_epi16;




			/***** Compute the first eight u chroma output values *****/

			// Preload the second eight lowpass coefficients
			b_low2_epi16 = _mm_load_si128((__m128i *)&b_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			b_high2_epi16 = _mm_load_si128((__m128i *)&b_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = b_low1_epi16;
			high1_epi16 = b_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, b_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, b_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			b_even_value = (short)temp;
			b_odd_value = (short)(temp >> 16);


			/***** Compute the second eight u chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = b_low2_epi16;
			high2_epi16 = b_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, b_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, b_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			b_even_value = (short)temp;
			b_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			b_low1_epi16 = b_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			b_high1_epi16 = b_high2_epi16;



			
			
			/***** Compute the first eight v chroma output values *****/

			// Preload the second eight lowpass coefficients
			r_low2_epi16 = _mm_load_si128((__m128i *)&r_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			r_high2_epi16 = _mm_load_si128((__m128i *)&r_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = r_low1_epi16;
			high1_epi16 = r_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, r_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, r_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			r_even_value = (short)temp;
			r_odd_value = (short)(temp >> 16);


			/***** Compute the second eight v chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = r_low2_epi16;
			high2_epi16 = r_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, r_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, r_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			r_even_value = (short)temp;
			r_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			r_low1_epi16 = r_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			r_high1_epi16 = r_high2_epi16;




			if(num_channels == 4)
			{
				/***** Compute the first eight v chroma output values *****/

				// Preload the second eight lowpass coefficients
				a_low2_epi16 = _mm_load_si128((__m128i *)&a_lowpass_ptr[column + 8]);

				// Preload the second eight highpass coefficients
				a_high2_epi16 = _mm_load_si128((__m128i *)&a_highpass_ptr[column + 8]);

				// Move the current set of coefficients to working registers
				low1_epi16 = a_low1_epi16;
				high1_epi16 = a_high1_epi16;

				// Apply the even reconstruction filter to the lowpass band
				even_epi16 = low1_epi16;
				temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
				even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
				even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
				even_epi16 = _mm_srai_epi16(even_epi16, 3);
				temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
				even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

				// Shift the highpass correction by one column
				high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

				// Add the highpass correction and divide by two
				even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
				even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
				even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
				even_epi16 = _mm_srai_epi16(even_epi16, 1);

				// Apply the odd reconstruction filter to the lowpass band
				odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
				temp_epi16 = low1_epi16;
				odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
				odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
				odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
				temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
				odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

				// Subtract the highpass correction and divide by two
				odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
				odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
				odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
				odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

				// Interleave the four even and four odd results
				out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

				// Reduce the precision to eight bits
		//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
		//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

				// Combine the new output values with the two values from the previous phase
				out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
				temp = _mm_cvtsi128_si32(out_epi16);
				out_epi16 = _mm_insert_epi16(out_epi16, a_even_value, 0);
				out_epi16 = _mm_insert_epi16(out_epi16, a_odd_value, 1);

				// Save the eight u chroma values for packing later
				ag1_output_epi16 = out_epi16;

				// Save the remaining two output values
				a_even_value = (short)temp;
				a_odd_value = (short)(temp >> 16);


				/***** Compute the second eight v chroma output values *****/

				// Move the next set of coefficients to working registers
				low2_epi16 = a_low2_epi16;
				high2_epi16 = a_high2_epi16;

				// Shift in the new pixels for the next stage of the loop
				low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
				temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
				low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

				// Apply the even reconstruction filter to the lowpass band
				even_epi16 = low1_epi16;
				temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
				even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
				even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
				even_epi16 = _mm_srai_epi16(even_epi16, 3);
				temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
				even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

				// Shift in the next four highpass coefficients
				high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
				temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
				high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

				// Add the highpass correction and divide by two
				even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
				even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
				even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
				even_epi16 = _mm_srai_epi16(even_epi16, 1);

				// Apply the odd reconstruction filter to the lowpass band
				odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
				temp_epi16 = low1_epi16;
				odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
				odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
				odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
				temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
				odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

				// Subtract the highpass correction and divide by two
				odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
				odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
				odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
				odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

				// Interleave the four even and four odd results
				out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

				// Reduce the precision to eight bits
		//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
		//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

				// Combine the new output values with the two values from the previous phase
				out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
				temp = _mm_cvtsi128_si32(out_epi16);
				out_epi16 = _mm_insert_epi16(out_epi16, a_even_value, 0);
				out_epi16 = _mm_insert_epi16(out_epi16, a_odd_value, 1);

				// Save the eight u chroma values for packing later
				ag2_output_epi16 = out_epi16;

				// Save the remaining two output values
				a_even_value = (short)temp;
				a_odd_value = (short)(temp >> 16);

				// The second eight lowpass coefficients are the current values in the next iteration
				a_low1_epi16 = a_low2_epi16;

				// The second eight highpass coefficients are the current values in the next iteration
				a_high1_epi16 = a_high2_epi16;
			}








			//r_output_epi16  = ((r_output_epi16 - 32768)<<1)+g_output_epi16
			//r_output_epi16  = ((r_output_epi16>>3 - 32768>>3))+g_output_epi16>>4


			g1_output_epi16 = gg1_output_epi16;

			r1_output_epi16 = rg1_output_epi16;//_mm_srli_epi16(rg1_output_epi16,2);
		//	r1_output_epi16 = _mm_subs_epi16(r1_output_epi16, value128_epi32);
		//	r1_output_epi16 = _mm_slli_epi16(r1_output_epi16,1);
		//	r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, g1_output_epi16);

			b1_output_epi16 = bg1_output_epi16;//_mm_srli_epi16(bg1_output_epi16,2);
		//	b1_output_epi16 = _mm_subs_epi16(b1_output_epi16, value128_epi32);
		//	b1_output_epi16 = _mm_slli_epi16(b1_output_epi16,1);
		//	b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, g1_output_epi16);


			 r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, limiterRGB);
			 r1_output_epi16 = _mm_subs_epu16(r1_output_epi16, limiterRGB);

			 g1_output_epi16 = _mm_adds_epi16(g1_output_epi16, limiterRGB);
			 g1_output_epi16 = _mm_subs_epu16(g1_output_epi16, limiterRGB);

			 b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, limiterRGB);
			 b1_output_epi16 = _mm_subs_epu16(b1_output_epi16, limiterRGB);

			 
			if(num_channels == 4)
			{
				a1_output_epi16 = ag1_output_epi16;

				a1_output_epi16 = _mm_adds_epi16(a1_output_epi16, limiterRGB); //12-bit limit
				a1_output_epi16 = _mm_subs_epu16(a1_output_epi16, limiterRGB);

				a1_output_epi16 = _mm_subs_epu16(a1_output_epi16, _mm_set1_epi16(alphacompandDCoffset)); //12-bit  -16
				a1_output_epi16 = _mm_slli_epi16(a1_output_epi16, 3);  //15-bit
				a1_output_epi16 = _mm_mulhi_epi16(a1_output_epi16, _mm_set1_epi16(alphacompandGain));

				a1_output_epi16 = _mm_adds_epi16(a1_output_epi16, limiterRGB); //12-bit limit
				a1_output_epi16 = _mm_subs_epu16(a1_output_epi16, limiterRGB);
			}

			zero_epi128 = _mm_setzero_si128();


			{

				__m128i bg_epi16;
				__m128i ra_epi16;
				__m128i bgra1_epi16;
				__m128i bgra2_epi16;

				// Interleave the first four blue and green values
				if(num_channels == 4)
					bg_epi16 = _mm_unpacklo_epi16(a1_output_epi16, r1_output_epi16);
				else
					bg_epi16 = _mm_unpacklo_epi16(a_epi16, r1_output_epi16);


				// Interleave the first four red and alpha values
				ra_epi16 = _mm_unpacklo_epi16(g1_output_epi16, b1_output_epi16);

				// Interleave the first pair of BGRA tuples
				bgra1_epi16 = _mm_unpacklo_epi32(bg_epi16, ra_epi16);

				// Interleave the second pair of BGRA tuples
				bgra2_epi16 = _mm_unpackhi_epi32(bg_epi16, ra_epi16);

				//12bit to 16-bit
				bgra1_epi16 = _mm_slli_epi16(bgra1_epi16, 4);
				bgra2_epi16 = _mm_slli_epi16(bgra2_epi16, 4);

				_mm_store_si128(B64Aptr128++, bgra1_epi16);
				_mm_store_si128(B64Aptr128++, bgra2_epi16);



				// Interleave the first four blue and green values
				if(num_channels == 4)
					bg_epi16 = _mm_unpackhi_epi16(a1_output_epi16, r1_output_epi16);
				else
					bg_epi16 = _mm_unpackhi_epi16(a_epi16, r1_output_epi16);

				// Interleave the first four red and alpha values
				ra_epi16 = _mm_unpackhi_epi16(g1_output_epi16, b1_output_epi16);

				// Interleave the first pair of BGRA tuples
				bgra1_epi16 = _mm_unpacklo_epi32(bg_epi16, ra_epi16);

				// Interleave the second pair of BGRA tuples
				bgra2_epi16 = _mm_unpackhi_epi32(bg_epi16, ra_epi16);

				//12bit to 16-bit
				bgra1_epi16 = _mm_slli_epi16(bgra1_epi16, 4);
				bgra2_epi16 = _mm_slli_epi16(bgra2_epi16, 4);

				_mm_store_si128(B64Aptr128++, bgra1_epi16);
				_mm_store_si128(B64Aptr128++, bgra2_epi16);
			}




			g2_output_epi16 = gg2_output_epi16;//_mm_srli_epi16(gg2_output_epi16,2);

			r2_output_epi16 = rg2_output_epi16;//_mm_srli_epi16(rg2_output_epi16,2);

			b2_output_epi16 = bg2_output_epi16;//_mm_srli_epi16(bg2_output_epi16,2);




			r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, limiterRGB);
			r2_output_epi16 = _mm_subs_epu16(r2_output_epi16, limiterRGB);

			g2_output_epi16 = _mm_adds_epi16(g2_output_epi16, limiterRGB);
			g2_output_epi16 = _mm_subs_epu16(g2_output_epi16, limiterRGB);

			b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, limiterRGB);
			b2_output_epi16 = _mm_subs_epu16(b2_output_epi16, limiterRGB);
			 
			if(num_channels == 4)
			{
				a2_output_epi16 = ag2_output_epi16;
				
				a2_output_epi16 = _mm_adds_epi16(a2_output_epi16, limiterRGB); //12-bit limit
				a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, limiterRGB);

				a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, _mm_set1_epi16(alphacompandDCoffset)); //12-bit  -16
				a2_output_epi16 = _mm_slli_epi16(a2_output_epi16, 3);  //15-bit
				a2_output_epi16 = _mm_mulhi_epi16(a2_output_epi16, _mm_set1_epi16(alphacompandGain));

				a2_output_epi16 = _mm_adds_epi16(a2_output_epi16, limiterRGB); //12-bit limit
				a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, limiterRGB);
			}

			{

				__m128i bg_epi16;
				__m128i ra_epi16;
				__m128i bgra1_epi16;
				__m128i bgra2_epi16;

				// Interleave the first four blue and green values
				if(num_channels == 4)	
					bg_epi16 = _mm_unpacklo_epi16(a2_output_epi16, r2_output_epi16);
				else
					bg_epi16 = _mm_unpacklo_epi16(a_epi16, r2_output_epi16);

				// Interleave the first four red and alpha values
				ra_epi16 = _mm_unpacklo_epi16(g2_output_epi16, b2_output_epi16);

				// Interleave the first pair of BGRA tuples
				bgra1_epi16 = _mm_unpacklo_epi32(bg_epi16, ra_epi16);

				// Interleave the second pair of BGRA tuples
				bgra2_epi16 = _mm_unpackhi_epi32(bg_epi16, ra_epi16);

				//12bit to 16-bit
				bgra1_epi16 = _mm_slli_epi16(bgra1_epi16, 4);
				bgra2_epi16 = _mm_slli_epi16(bgra2_epi16, 4);

				_mm_store_si128(B64Aptr128++, bgra1_epi16);
				_mm_store_si128(B64Aptr128++, bgra2_epi16);



				// Interleave the first four blue and green values
				if(num_channels == 4)	
					bg_epi16 = _mm_unpackhi_epi16(a2_output_epi16, r2_output_epi16);
				else
					bg_epi16 = _mm_unpackhi_epi16(a_epi16, r2_output_epi16);

				// Interleave the first four red and alpha values
				ra_epi16 = _mm_unpackhi_epi16(g2_output_epi16, b2_output_epi16);

				// Interleave the first pair of BGRA tuples
				bgra1_epi16 = _mm_unpacklo_epi32(bg_epi16, ra_epi16);

				// Interleave the second pair of BGRA tuples
				bgra2_epi16 = _mm_unpackhi_epi32(bg_epi16, ra_epi16);

				//12bit to 16-bit
				bgra1_epi16 = _mm_slli_epi16(bgra1_epi16, 4);
				bgra2_epi16 = _mm_slli_epi16(bgra2_epi16, 4);

				_mm_store_si128(B64Aptr128++, bgra1_epi16);
				_mm_store_si128(B64Aptr128++, bgra2_epi16);
			}
		}

		// Should have exited the loop with the column equal to the post processing column
	//	assert(column == post_column);

		colptr = (PIXEL16U *)B64Aptr128;


#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column ++)
		{
			int re,ge,be,ae;
			int ro,go,bo,ao;

			/***** First pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += g_lowpass_ptr[column - 1];
			even -= g_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += g_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += g_highpass_ptr[column];
			even = DivideByShift(even, 1);

			ge = even;


			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= g_lowpass_ptr[column - 1];
			odd += g_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += g_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= g_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			go = odd;


			/***** Pair of u chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += b_lowpass_ptr[column - 1];
			even -= b_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += b_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += b_highpass_ptr[column];
			even = DivideByShift(even, 1);

			be = even;


			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= b_lowpass_ptr[column - 1];
			odd += b_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += b_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= b_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			bo = odd;



			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += r_lowpass_ptr[column - 1];
			even -= r_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += r_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += r_highpass_ptr[column];
			even = DivideByShift(even, 1);

			re = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= r_lowpass_ptr[column - 1];
			odd += r_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += r_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= r_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			ro = odd;

//TODO -- alpha companding
			if(num_channels == 4)
			{
				// Apply the even reconstruction filter to the lowpass band
				even = 0;
				even += a_lowpass_ptr[column - 1];
				even -= a_lowpass_ptr[column + 1];
				even += 4; //DAN20050921
				even >>= 3;
				even += a_lowpass_ptr[column + 0];

				// Add the highpass correction
				even += a_highpass_ptr[column];
				even = DivideByShift(even, 1);

				ae = even;

				// Apply the odd reconstruction filter to the lowpass band
				odd = 0;
				odd -= a_lowpass_ptr[column - 1];
				odd += a_lowpass_ptr[column + 1];
				odd += 4; //DAN20050921
				odd >>= 3;
				odd += a_lowpass_ptr[column + 0];

				// Subtract the highpass correction
				odd -= a_highpass_ptr[column];
				odd = DivideByShift(odd, 1);

				ao = odd;

				// Remove the alpha encoding curve.
				//ae -= 16<<4;
				//ae <<= 8;
				//ae += 111;
				//ae /= 223;
				//12-bit SSE calibrated code
				//a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, _mm_set1_epi16(alphacompandDCoffset)); //12-bit  -16
				//a2_output_epi16 = _mm_slli_epi16(a2_output_epi16, 3);  //15-bit
				//a2_output_epi16 = _mm_mulhi_epi16(a2_output_epi16, _mm_set1_epi16(alphacompandGain));
				ae -= alphacompandDCoffset;
				ae <<= 3; //15-bit
				ae *= alphacompandGain;
				ae >>= 16; //12-bit
				if (ae < 0) ae = 0; else if (ae > 4095) ae = 4095;



				//ao -= 16<<4;
				//ao <<= 8;
				//ao += 111;
				//ao /= 223;
				//12-bit SSE calibrated code
				//a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, _mm_set1_epi16(alphacompandDCoffset)); //12-bit  -16
				//a2_output_epi16 = _mm_slli_epi16(a2_output_epi16, 3);  //15-bit
				//a2_output_epi16 = _mm_mulhi_epi16(a2_output_epi16, _mm_set1_epi16(alphacompandGain));
				ao -= alphacompandDCoffset;
				ao <<= 3; //15-bit
				ao *= alphacompandGain;
				ao >>= 16; //12-bit
				if (ao < 0) ao = 0; else if (ao > 4095) ao = 4095;

				
				*(colptr++) = SATURATE_16U(ae<<4);
				*(colptr++) = SATURATE_16U(re<<4);
				*(colptr++) = SATURATE_16U(ge<<4);
				*(colptr++) = SATURATE_16U(be<<4);
				*(colptr++) = SATURATE_16U(ao<<4);
				*(colptr++) = SATURATE_16U(ro<<4);
				*(colptr++) = SATURATE_16U(go<<4);
				*(colptr++) = SATURATE_16U(bo<<4);
			}
			else
			{
				*(colptr++) = 0xfff0; //a
				*(colptr++) = SATURATE_16U(re<<4);
				*(colptr++) = SATURATE_16U(ge<<4);
				*(colptr++) = SATURATE_16U(be<<4);
				*(colptr++) = 0xfff0; //a
				*(colptr++) = SATURATE_16U(ro<<4);
				*(colptr++) = SATURATE_16U(go<<4);
				*(colptr++) = SATURATE_16U(bo<<4);
			}
		}


		assert(column == last_column);


		//right hand border processing
		column = last_column - 1;
		colptr -= 8; // two B64A pixels

		// Process the last luma output points with special filters for the right border

		//Green
		even = 0;
		odd = 0;
		// Apply the even border filter to the lowpass band
		even += 5 * g_lowpass_ptr[column + 0];
		even += 4 * g_lowpass_ptr[column - 1];
		even -= 1 * g_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += g_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Save the luma result for later output in the correct order
		g_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * g_lowpass_ptr[column + 0];
		odd -=  4 * g_lowpass_ptr[column - 1];
		odd +=  1 * g_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= g_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Save the luma result for later output in the correct order
		g_odd_value = odd;



		//Red
		even = 0;
		odd = 0;
		// Apply the even border filter to the lowpass band
		even += 5 * r_lowpass_ptr[column + 0];
		even += 4 * r_lowpass_ptr[column - 1];
		even -= 1 * r_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += r_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Save the luma result for later output in the correct order
		r_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * r_lowpass_ptr[column + 0];
		odd -=  4 * r_lowpass_ptr[column - 1];
		odd +=  1 * r_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= r_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Save the luma result for later output in the correct order
		r_odd_value = odd;




		//Blue
		even = 0;
		odd = 0;
		// Apply the even border filter to the lowpass band
		even += 5 * b_lowpass_ptr[column + 0];
		even += 4 * b_lowpass_ptr[column - 1];
		even -= 1 * b_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += b_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Save the luma result for later output in the correct order
		b_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * b_lowpass_ptr[column + 0];
		odd -=  4 * b_lowpass_ptr[column - 1];
		odd +=  1 * b_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= b_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Save the luma result for later output in the correct order
		b_odd_value = odd;


		//Alpha
		if(num_channels == 4)
		{
			even = 0;
			odd = 0;
			// Apply the even border filter to the lowpass band
			even += 5 * a_lowpass_ptr[column + 0];
			even += 4 * a_lowpass_ptr[column - 1];
			even -= 1 * a_lowpass_ptr[column - 2];
			even += ROUNDING(even,8);
			even = DivideByShift(even, 3);

			// Add the highpass correction
			even += a_highpass_ptr[column];
			even = DivideByShift(even, 1);

			// Save the luma result for later output in the correct order
			a_even_value = even;

			// Apply the odd border filter to the lowpass band
			odd += 11 * a_lowpass_ptr[column + 0];
			odd -=  4 * a_lowpass_ptr[column - 1];
			odd +=  1 * a_lowpass_ptr[column - 2];
			odd += ROUNDING(odd,8);
			odd = DivideByShift(odd, 3);

			// Subtract the highpass correction
			odd -= a_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			// Save the luma result for later output in the correct order
			a_odd_value = odd;

			// Remove the alpha encoding curve.
			//a_even_value -= 16<<4;
			//a_even_value <<= 8;
			//a_even_value += 111;
			//a_even_value /= 223;
			//12-bit SSE calibrated code
			//a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, _mm_set1_epi16(alphacompandDCoffset)); //12-bit  -16
			//a2_output_epi16 = _mm_slli_epi16(a2_output_epi16, 3);  //15-bit
			//a2_output_epi16 = _mm_mulhi_epi16(a2_output_epi16, _mm_set1_epi16(alphacompandGain));
			a_even_value -= alphacompandDCoffset;
			a_even_value <<= 3; //15-bit
			a_even_value *= alphacompandGain;
			a_even_value >>= 16; //12-bit
			if (a_even_value < 0) a_even_value = 0; else if (a_even_value > 4095) a_even_value = 4095;

			//a_odd_value -= 16<<4;
			//a_odd_value <<= 8;
			//a_odd_value += 111;
			//a_odd_value /= 223;
			//12-bit SSE calibrated code
			//a2_output_epi16 = _mm_subs_epu16(a2_output_epi16, _mm_set1_epi16(alphacompandDCoffset)); //12-bit  -16
			//a2_output_epi16 = _mm_slli_epi16(a2_output_epi16, 3);  //15-bit
			//a2_output_epi16 = _mm_mulhi_epi16(a2_output_epi16, _mm_set1_epi16(alphacompandGain));
			a_odd_value -= alphacompandDCoffset;
			a_odd_value <<= 3; //15-bit
			a_odd_value *= alphacompandGain;
			a_odd_value >>= 16; //12-bit
			if (a_odd_value < 0) a_odd_value = 0; else if (a_odd_value > 4095) a_odd_value = 4095;


			*colptr = SATURATE_16U(a_even_value<<4);
			*(colptr+4) = SATURATE_16U(a_odd_value<<4);
		}


		colptr++; //a
		*(colptr++) = SATURATE_16U(r_even_value<<4);
		*(colptr++) = SATURATE_16U(g_even_value<<4);
		*(colptr++) = SATURATE_16U(b_even_value<<4);
		colptr++; //a
		*(colptr++) = SATURATE_16U(r_odd_value<<4);
		*(colptr++) = SATURATE_16U(g_odd_value<<4);
		*(colptr++) = SATURATE_16U(b_odd_value<<4);


		// Should have exited the loop at the column for right border processing
	//	assert(column == last_column);
		// Advance to the next row of coefficients in each channel
		g_lowpass_ptr += lowpass_pitch[0];
		b_lowpass_ptr += lowpass_pitch[1];
		r_lowpass_ptr += lowpass_pitch[2];
		g_highpass_ptr += highpass_pitch[0];
		b_highpass_ptr += highpass_pitch[1];
		r_highpass_ptr += highpass_pitch[2];

		if(num_channels == 4)
		{
			a_lowpass_ptr += lowpass_pitch[3];
			a_highpass_ptr += highpass_pitch[3];
		}

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}

// Used in RT RG30 playback
// Apply the inverse horizontal transform to reconstruct a strip of rows into packed RG30 pixels
void InvertHorizontalStrip16sRGB2RG30(HorizontalFilterParams)
{
	int num_channels = CODEC_NUM_CHANNELS;
	int height = roi.height;
	int width = roi.width;

	// Note that the u and v chroma values are swapped
	PIXEL *gg_lowpass_ptr = lowpass_band[0];
	PIXEL *rg_lowpass_ptr = lowpass_band[1];
	PIXEL *bg_lowpass_ptr = lowpass_band[2];
	PIXEL *gg_highpass_ptr = highpass_band[0];
	PIXEL *rg_highpass_ptr = highpass_band[1];
	PIXEL *bg_highpass_ptr = highpass_band[2];

	uint8_t *output = output_image;

	// Process 8 luma coefficients per loop iteration
	const int column_step = 8;

	// Need to process two luma coefficients up to the last column to allow for chroma output
	const int last_column = width;
	int post_column = last_column - (last_column % column_step);

	// Need at least four luma values of border processing up to the last column
	//const int post_border = 2;

	int channel;
	int row;

	// Compute the amount of scaling required to reduce the output precision
	//int descale_shift = (precision - 8);

	int shift = 8;

	float scale;

	scale = 4.0;

	shift-=2;

	format = format & 0xffff;// mask off color_space

	// Convert the pitch to units of pixels
	for (channel = 0; channel < num_channels; channel++)
	{
		lowpass_pitch[channel] /= sizeof(PIXEL);
		highpass_pitch[channel] /= sizeof(PIXEL);
	}

	output_pitch /= sizeof(uint8_t);

	// Adjust the end of the fast loop if necessary for border processing
	//if (post_column > (last_column - post_border))  // DAN08112004 - ignore end row calc -- use SSE to edge.
	//	post_column -= column_step;

	// Check that there is enough margin to accommodate border processing
//	assert(post_column <= (last_column - post_border));

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
#if (1 && XMMOPT)
		__m128i gg_low1_epi16;		// Lowpass coefficients
		__m128i gg_low2_epi16;
		__m128i bg_low1_epi16;
		__m128i bg_low2_epi16;
		__m128i rg_low1_epi16;
		__m128i rg_low2_epi16;

		__m128i gg_high1_epi16;		// Highpass coefficients
		__m128i gg_high2_epi16;
		__m128i bg_high1_epi16;
		__m128i bg_high2_epi16;
		__m128i rg_high1_epi16;
		__m128i rg_high2_epi16;

		// The fast loop merges values from different phases to allow aligned stores
		//__m128i *outptr = (__m128i *)&output[0];
		__m128i *RG30ptr128 = (__m128i *)&output[0];
		
		__m128i limiterRGB = _mm_set1_epi16(0x7fff - 0x0fff);

#endif
		//PIXEL16U *colptr = (PIXEL16U *)&output[0];
		uint32_t *RG30ptr = (uint32_t *)&output[0];

		int32_t gg_even_value;
		int32_t bg_even_value;
		int32_t rg_even_value;
		int32_t gg_odd_value;
		int32_t bg_odd_value;
		int32_t rg_odd_value;


		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two luma output points with special filters for the left border
		int32_t even = 0;
		int32_t odd = 0;


		// Apply the even reconstruction filter to the lowpass band
		even += 11 * gg_lowpass_ptr[column + 0];
		even -=  4 * gg_lowpass_ptr[column + 1];
		even +=  1 * gg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += gg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		gg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * gg_lowpass_ptr[column + 0];
		odd += 4 * gg_lowpass_ptr[column + 1];
		odd -= 1 * gg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= gg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
	//	odd >>= descale_shift;

		// Save the value for use in the fast loop
		gg_odd_value = odd;


		// Process the first two u chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * bg_lowpass_ptr[column + 0];
		even -=  4 * bg_lowpass_ptr[column + 1];
		even +=  1 * bg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += bg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		bg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * bg_lowpass_ptr[column + 0];
		odd += 4 * bg_lowpass_ptr[column + 1];
		odd -= 1 * bg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= bg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
	//	odd >>= descale_shift;

		// Save the value for use in the fast loop
		bg_odd_value = odd;


		// Process the first two v chroma output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * rg_lowpass_ptr[column + 0];
		even -=  4 * rg_lowpass_ptr[column + 1];
		even +=  1 * rg_lowpass_ptr[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += rg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Reduce the precision to eight bits
	//	even >>= descale_shift;

		// Save the value for use in the fast loop
		rg_even_value = even;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * rg_lowpass_ptr[column + 0];
		odd += 4 * rg_lowpass_ptr[column + 1];
		odd -= 1 * rg_lowpass_ptr[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= rg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Reduce the precision to eight bits
	//	odd >>= descale_shift;

		// Save the value for use in the fast loop
		rg_odd_value = odd;

#if (1 && XMMOPT)

		// Preload the first eight lowpass luma coefficients
		gg_low1_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[0]);

		// Preload the first eight highpass luma coefficients
		gg_high1_epi16 = _mm_load_si128((__m128i *)&gg_highpass_ptr[0]);

		// Preload the first eight lowpass u chroma coefficients
		bg_low1_epi16 = _mm_load_si128((__m128i *)&bg_lowpass_ptr[0]);

		// Preload the first eight highpass u chroma coefficients
		bg_high1_epi16 = _mm_load_si128((__m128i *)&bg_highpass_ptr[0]);

		// Preload the first eight lowpass v chroma coefficients
		rg_low1_epi16 = _mm_load_si128((__m128i *)&rg_lowpass_ptr[0]);

		// Preload the first eight highpass v chroma coefficients
 		rg_high1_epi16 = _mm_load_si128((__m128i *)&rg_highpass_ptr[0]);


		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m128i g1_output_epi16;
			__m128i g2_output_epi16;
			__m128i r1_output_epi16;
			__m128i r2_output_epi16;
			__m128i b1_output_epi16;
			__m128i b2_output_epi16;

			__m128i gg1_output_epi16;
			__m128i gg2_output_epi16;
			__m128i rg1_output_epi16;
			__m128i rg2_output_epi16;
			__m128i bg1_output_epi16;
			__m128i bg2_output_epi16;

			__m128i even_epi16;		// Result of convolution with even filter
			__m128i odd_epi16;		// Result of convolution with odd filter
			__m128i temp_epi16;
			__m128i zero_epi128;

			__m128i rr_epi32;
			__m128i gg_epi32;
			__m128i bb_epi32;

			__m128i out_epi16;		// Reconstructed data
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			__m128i low1_epi16;
			__m128i low2_epi16;
			__m128i high1_epi16;
			__m128i high2_epi16;

			//DAN031304 -- correct inverse filter
			__m128i half_epi16 = _mm_set1_epi16(4);
			__m128i offset_epi16 = _mm_set1_epi16(2048);

			uint32_t temp;		// Temporary register for last two values


			/***** Compute the first eight luma output values *****/

			// Preload the second eight lowpass coefficients
			gg_low2_epi16 = _mm_load_si128((__m128i *)&gg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			gg_high2_epi16 = _mm_load_si128((__m128i *)&gg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = gg_low1_epi16;
			high1_epi16 = gg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);


			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_odd_value, 1);

			// Save the eight luma values for packing later
			gg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			gg_even_value = (short)temp;
			gg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight luma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = gg_low2_epi16;
			high2_epi16 = gg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, gg_odd_value, 1);

			// Save the eight luma values for packing later
			gg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			gg_even_value = (short)temp;
			gg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are used later in the loop
			gg_low1_epi16 = gg_low2_epi16;

			// The second eight highpass coefficients are used later in the loop
			gg_high1_epi16 = gg_high2_epi16;


			/***** Compute the first eight u chroma output values *****/

			// Preload the second eight lowpass coefficients
			bg_low2_epi16 = _mm_load_si128((__m128i *)&bg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			bg_high2_epi16 = _mm_load_si128((__m128i *)&bg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = bg_low1_epi16;
			high1_epi16 = bg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			bg_even_value = (short)temp;
			bg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight u chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = bg_low2_epi16;
			high2_epi16 = bg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, bg_odd_value, 1);

			// Save the eight u chroma values for packing later
			bg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			bg_even_value = (short)temp;
			bg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			bg_low1_epi16 = bg_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			bg_high1_epi16 = bg_high2_epi16;



			/***** Compute the first eight v chroma output values *****/

			// Preload the second eight lowpass coefficients
			rg_low2_epi16 = _mm_load_si128((__m128i *)&rg_lowpass_ptr[column + 8]);

			// Preload the second eight highpass coefficients
			rg_high2_epi16 = _mm_load_si128((__m128i *)&rg_highpass_ptr[column + 8]);

			// Move the current set of coefficients to working registers
			low1_epi16 = rg_low1_epi16;
			high1_epi16 = rg_high1_epi16;

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift the highpass correction by one column
			high1_epi16 = _mm_srli_si128(high1_epi16, 1*2);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding1_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg1_output_epi16 = out_epi16;

			// Save the remaining two output values
			rg_even_value = (short)temp;
			rg_odd_value = (short)(temp >> 16);


			/***** Compute the second eight v chroma output values *****/

			// Move the next set of coefficients to working registers
			low2_epi16 = rg_low2_epi16;
			high2_epi16 = rg_high2_epi16;

			// Shift in the new pixels for the next stage of the loop
			low1_epi16 = _mm_srli_si128(low1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(low2_epi16, 4*2);
			low1_epi16 = _mm_or_si128(low1_epi16, temp_epi16);

			// Apply the even reconstruction filter to the lowpass band
			even_epi16 = low1_epi16;
			temp_epi16 = _mm_srli_si128(even_epi16, 2*2);
			even_epi16 = _mm_subs_epi16(even_epi16, temp_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			even_epi16 = _mm_adds_epi16(even_epi16, temp_epi16);

			// Shift in the next four highpass coefficients
			high1_epi16 = _mm_srli_si128(high1_epi16, 4*2);
			temp_epi16 = _mm_slli_si128(high2_epi16, 3*2);
			high1_epi16 = _mm_or_si128(high1_epi16, temp_epi16);

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, offset_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, offset_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
			odd_epi16 = _mm_srli_si128(low1_epi16, 2*2);
			temp_epi16 = low1_epi16;
			odd_epi16 = _mm_subs_epi16(odd_epi16, temp_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);
			temp_epi16 = _mm_srli_si128(low1_epi16, 1*2);
			odd_epi16 = _mm_adds_epi16(odd_epi16, temp_epi16);

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_adds_epi16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, offset_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the four even and four odd results
			out_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);

			// Reduce the precision to eight bits
	//		out_epi16 = _mm_adds_epi16(out_epi16, rounding2_pi16);
	//		out_epi16 = _mm_srli_epi16(out_epi16, descale_shift);

			// Combine the new output values with the two values from the previous phase
			out_epi16 = _mm_shuffle_epi32(out_epi16, _MM_SHUFFLE(2, 1, 0, 3));
			temp = _mm_cvtsi128_si32(out_epi16);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_even_value, 0);
			out_epi16 = _mm_insert_epi16(out_epi16, rg_odd_value, 1);

			// Save the eight u chroma values for packing later
			rg2_output_epi16 = out_epi16;

			// Save the remaining two output values
			rg_even_value = (short)temp;
			rg_odd_value = (short)(temp >> 16);

			// The second eight lowpass coefficients are the current values in the next iteration
			rg_low1_epi16 = rg_low2_epi16;

			// The second eight highpass coefficients are the current values in the next iteration
			rg_high1_epi16 = rg_high2_epi16;












			//r_output_epi16  = ((rg_output_epi16 - 32768)<<1)+gg_output_epi16
			//r_output_epi16  = ((rg_output_epi16>>3 - 32768>>3))+gg_output_epi16>>4


			g1_output_epi16 = gg1_output_epi16;

			r1_output_epi16 = rg1_output_epi16;//_mm_srli_epi16(rg1_output_epi16,2);
		//	r1_output_epi16 = _mm_subs_epi16(r1_output_epi16, value128_epi32);
		//	r1_output_epi16 = _mm_slli_epi16(r1_output_epi16,1);
		//	r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, g1_output_epi16);

			b1_output_epi16 = bg1_output_epi16;//_mm_srli_epi16(bg1_output_epi16,2);
		//	b1_output_epi16 = _mm_subs_epi16(b1_output_epi16, value128_epi32);
		//	b1_output_epi16 = _mm_slli_epi16(b1_output_epi16,1);
		//	b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, g1_output_epi16);


			 r1_output_epi16 = _mm_adds_epi16(r1_output_epi16, limiterRGB);
			 r1_output_epi16 = _mm_subs_epu16(r1_output_epi16, limiterRGB);

			 g1_output_epi16 = _mm_adds_epi16(g1_output_epi16, limiterRGB);
			 g1_output_epi16 = _mm_subs_epu16(g1_output_epi16, limiterRGB);

			 b1_output_epi16 = _mm_adds_epi16(b1_output_epi16, limiterRGB);
			 b1_output_epi16 = _mm_subs_epu16(b1_output_epi16, limiterRGB);


			zero_epi128 = _mm_setzero_si128();




			r1_output_epi16 = _mm_srli_epi16(r1_output_epi16, 2);
			g1_output_epi16 = _mm_srli_epi16(g1_output_epi16, 2);
			b1_output_epi16 = _mm_srli_epi16(b1_output_epi16, 2);

			// Interleave the first four blue and green values
			rr_epi32 = _mm_unpacklo_epi16(r1_output_epi16, zero_epi128);
			gg_epi32 = _mm_unpacklo_epi16(g1_output_epi16, zero_epi128);
			bb_epi32 = _mm_unpacklo_epi16(b1_output_epi16, zero_epi128);


			switch(format)
			{
			case DECODED_FORMAT_RG30:
			case DECODED_FORMAT_AB10:
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				bb_epi32 = _mm_slli_epi32(bb_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);
				break;
			case DECODED_FORMAT_R210:
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				// the algorithm is:
				// 1) [A B C D] => [B A D C]
				// 2) [B A D C] => [D C B A]

				// do first swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi16( rr_epi32, 8 ),
										_mm_srli_epi16( rr_epi32, 8 ) ); //swap it
				// do second swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi32( rr_epi32, 16 ),
										_mm_srli_epi32( rr_epi32, 16 ) ); //swap it
				break;

			case DECODED_FORMAT_DPX0:
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				rr_epi32 = _mm_slli_epi32(rr_epi32, 2);

				// the algorithm is:
				// 1) [A B C D] => [B A D C]
				// 2) [B A D C] => [D C B A]

				// do first swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi16( rr_epi32, 8 ),
										_mm_srli_epi16( rr_epi32, 8 ) ); //swap it
				// do second swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi32( rr_epi32, 16 ),
										_mm_srli_epi32( rr_epi32, 16 ) ); //swap it
				break;

			case DECODED_FORMAT_AR10:
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);
				break;
			default:
				assert(0); //unknown format
				break;
			}

			_mm_store_si128(RG30ptr128++, rr_epi32);


			rr_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
			gg_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
			bb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);

			switch(format)
			{
			case DECODED_FORMAT_RG30:
			case DECODED_FORMAT_AB10:
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				bb_epi32 = _mm_slli_epi32(bb_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);
				break;

			case DECODED_FORMAT_R210:
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				// the algorithm is:
				// 1) [A B C D] => [B A D C]
				// 2) [B A D C] => [D C B A]

				// do first swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi16( rr_epi32, 8 ),
										_mm_srli_epi16( rr_epi32, 8 ) ); //swap it
				// do second swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi32( rr_epi32, 16 ),
										_mm_srli_epi32( rr_epi32, 16 ) ); //swap it

				break;

			case DECODED_FORMAT_DPX0:
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				rr_epi32 = _mm_slli_epi32(rr_epi32, 2);

				// the algorithm is:
				// 1) [A B C D] => [B A D C]
				// 2) [B A D C] => [D C B A]

				// do first swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi16( rr_epi32, 8 ),
										_mm_srli_epi16( rr_epi32, 8 ) ); //swap it
				// do second swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi32( rr_epi32, 16 ),
										_mm_srli_epi32( rr_epi32, 16 ) ); //swap it

				break;

			case DECODED_FORMAT_AR10:
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);
				break;
			default:
				assert(0); //unknown format
				break;
			}
			_mm_store_si128(RG30ptr128++, rr_epi32);







			g2_output_epi16 = gg2_output_epi16;//_mm_srli_epi16(gg2_output_epi16,2);

			r2_output_epi16 = rg2_output_epi16;//_mm_srli_epi16(rg2_output_epi16,2);

			b2_output_epi16 = bg2_output_epi16;//_mm_srli_epi16(bg2_output_epi16,2);



			 r2_output_epi16 = _mm_adds_epi16(r2_output_epi16, limiterRGB);
			 r2_output_epi16 = _mm_subs_epu16(r2_output_epi16, limiterRGB);

			 g2_output_epi16 = _mm_adds_epi16(g2_output_epi16, limiterRGB);
			 g2_output_epi16 = _mm_subs_epu16(g2_output_epi16, limiterRGB);

			 b2_output_epi16 = _mm_adds_epi16(b2_output_epi16, limiterRGB);
			 b2_output_epi16 = _mm_subs_epu16(b2_output_epi16, limiterRGB);



			r1_output_epi16 = _mm_srli_epi16(r2_output_epi16, 2);
			g1_output_epi16 = _mm_srli_epi16(g2_output_epi16, 2);
			b1_output_epi16 = _mm_srli_epi16(b2_output_epi16, 2);

			// Interleave the first four blue and green values
			rr_epi32 = _mm_unpacklo_epi16(r1_output_epi16, zero_epi128);
			gg_epi32 = _mm_unpacklo_epi16(g1_output_epi16, zero_epi128);
			bb_epi32 = _mm_unpacklo_epi16(b1_output_epi16, zero_epi128);


			switch(format)
			{
			case DECODED_FORMAT_RG30:
			case DECODED_FORMAT_AB10:
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				bb_epi32 = _mm_slli_epi32(bb_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				_mm_store_si128(RG30ptr128++, rr_epi32);

				rr_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
				gg_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
				bb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);

				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				bb_epi32 = _mm_slli_epi32(bb_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				_mm_store_si128(RG30ptr128++, rr_epi32);
				break;
			case DECODED_FORMAT_R210:
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				// the algorithm is:
				// 1) [A B C D] => [B A D C]
				// 2) [B A D C] => [D C B A]

				// do first swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi16( rr_epi32, 8 ),
										_mm_srli_epi16( rr_epi32, 8 ) ); //swap it
				// do second swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi32( rr_epi32, 16 ),
										_mm_srli_epi32( rr_epi32, 16 ) ); //swap it

				_mm_store_si128(RG30ptr128++, rr_epi32);

				rr_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
				gg_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
				bb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);

				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				// the algorithm is:
				// 1) [A B C D] => [B A D C]
				// 2) [B A D C] => [D C B A]

				// do first swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi16( rr_epi32, 8 ),
										_mm_srli_epi16( rr_epi32, 8 ) ); //swap it
				// do second swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi32( rr_epi32, 16 ),
										_mm_srli_epi32( rr_epi32, 16 ) ); //swap it

				_mm_store_si128(RG30ptr128++, rr_epi32);
				break;

			case DECODED_FORMAT_DPX0:
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				rr_epi32 = _mm_slli_epi32(rr_epi32, 2);

				// the algorithm is:
				// 1) [A B C D] => [B A D C]
				// 2) [B A D C] => [D C B A]

				// do first swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi16( rr_epi32, 8 ),
										_mm_srli_epi16( rr_epi32, 8 ) ); //swap it
				// do second swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi32( rr_epi32, 16 ),
										_mm_srli_epi32( rr_epi32, 16 ) ); //swap it

				_mm_store_si128(RG30ptr128++, rr_epi32);

				rr_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
				gg_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
				bb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);

				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				rr_epi32 = _mm_slli_epi32(rr_epi32, 2);

				// the algorithm is:
				// 1) [A B C D] => [B A D C]
				// 2) [B A D C] => [D C B A]

				// do first swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi16( rr_epi32, 8 ),
										_mm_srli_epi16( rr_epi32, 8 ) ); //swap it
				// do second swap
				rr_epi32 = _mm_or_si128( _mm_slli_epi32( rr_epi32, 16 ),
										_mm_srli_epi32( rr_epi32, 16 ) ); //swap it

				_mm_store_si128(RG30ptr128++, rr_epi32);
				break;

			case DECODED_FORMAT_AR10:
				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				_mm_store_si128(RG30ptr128++, rr_epi32);

				rr_epi32 = _mm_unpackhi_epi16(r1_output_epi16, zero_epi128);
				gg_epi32 = _mm_unpackhi_epi16(g1_output_epi16, zero_epi128);
				bb_epi32 = _mm_unpackhi_epi16(b1_output_epi16, zero_epi128);

				rr_epi32 = _mm_slli_epi32(rr_epi32, 20);
				gg_epi32 = _mm_slli_epi32(gg_epi32, 10);

				rr_epi32 = _mm_add_epi32(rr_epi32, gg_epi32);
				rr_epi32 = _mm_add_epi32(rr_epi32, bb_epi32);

				_mm_store_si128(RG30ptr128++, rr_epi32);
				break;
			default:
				assert(0); //unknown format
				break;
			}
		}

		// Should have exited the loop with the column equal to the post processing column
	//	assert(column == post_column);

		RG30ptr = (uint32_t *)RG30ptr128;
#endif

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column ++)
		{
			int re,ge,be;
			int ro,go,bo;

			/***** First pair of luma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += gg_lowpass_ptr[column - 1];
			even -= gg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += gg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += gg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			ge = even;


			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= gg_lowpass_ptr[column - 1];
			odd += gg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += gg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= gg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			go = odd;


			/***** Pair of u chroma output values *****/

			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += bg_lowpass_ptr[column - 1];
			even -= bg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += bg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += bg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			be = even;


			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= bg_lowpass_ptr[column - 1];
			odd += bg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += bg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= bg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			bo = odd;



			// Apply the even reconstruction filter to the lowpass band
			even = 0;
			even += rg_lowpass_ptr[column - 1];
			even -= rg_lowpass_ptr[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += rg_lowpass_ptr[column + 0];

			// Add the highpass correction
			even += rg_highpass_ptr[column];
			even = DivideByShift(even, 1);

			re = even;

			// Apply the odd reconstruction filter to the lowpass band
			odd = 0;
			odd -= rg_lowpass_ptr[column - 1];
			odd += rg_lowpass_ptr[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += rg_lowpass_ptr[column + 0];

			// Subtract the highpass correction
			odd -= rg_highpass_ptr[column];
			odd = DivideByShift(odd, 1);

			ro = odd;

			re >>= 2;
			ge >>= 2;
			be >>= 2;
			ro >>= 2;
			go >>= 2;
			bo >>= 2;

			if(re < 0) re = 0; if(re > 1023) re = 1023;
			if(ge < 0) ge = 0; if(ge > 1023) ge = 1023;
			if(be < 0) be = 0; if(be > 1023) be = 1023;
			if(ro < 0) ro = 0; if(ro > 1023) ro = 1023;
			if(go < 0) go = 0; if(go > 1023) go = 1023;
			if(bo < 0) bo = 0; if(bo > 1023) bo = 1023;

			switch(format)
			{
			case DECODED_FORMAT_RG30:
			case DECODED_FORMAT_AB10:
				*(RG30ptr++) = (be<<20)+(ge<<10)+re;
				*(RG30ptr++) = (bo<<20)+(go<<10)+ro;
				break;
			case DECODED_FORMAT_AR10:
				*(RG30ptr++) = (re<<20)+(ge<<10)+be;
				*(RG30ptr++) = (ro<<20)+(go<<10)+bo;
				break;
			case DECODED_FORMAT_R210:
				*(RG30ptr++) = SwapInt32((re<<20)+(ge<<10)+(be));
				*(RG30ptr++) = SwapInt32((ro<<20)+(go<<10)+(bo));
				break;
			case DECODED_FORMAT_DPX0:
				*(RG30ptr++) = SwapInt32((re<<22)+(ge<<12)+(be<<2));
				*(RG30ptr++) = SwapInt32((ro<<22)+(go<<12)+(bo<<2));
				break;
			}
		}



		assert(column == last_column);


		//right hand border processing
		column = last_column - 1;
		RG30ptr -= 2; // two pixels

		// Process the last luma output points with special filters for the right border

		//Green
		even = 0;
		odd = 0;
		// Apply the even border filter to the lowpass band
		even += 5 * gg_lowpass_ptr[column + 0];
		even += 4 * gg_lowpass_ptr[column - 1];
		even -= 1 * gg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += gg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Save the luma result for later output in the correct order
		gg_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * gg_lowpass_ptr[column + 0];
		odd -=  4 * gg_lowpass_ptr[column - 1];
		odd +=  1 * gg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= gg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Save the luma result for later output in the correct order
		gg_odd_value = odd;



		//Red
		even = 0;
		odd = 0;
		// Apply the even border filter to the lowpass band
		even += 5 * rg_lowpass_ptr[column + 0];
		even += 4 * rg_lowpass_ptr[column - 1];
		even -= 1 * rg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += rg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Save the luma result for later output in the correct order
		rg_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * rg_lowpass_ptr[column + 0];
		odd -=  4 * rg_lowpass_ptr[column - 1];
		odd +=  1 * rg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= rg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Save the luma result for later output in the correct order
		rg_odd_value = odd;




		//Blue
		even = 0;
		odd = 0;
		// Apply the even border filter to the lowpass band
		even += 5 * bg_lowpass_ptr[column + 0];
		even += 4 * bg_lowpass_ptr[column - 1];
		even -= 1 * bg_lowpass_ptr[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += bg_highpass_ptr[column];
		even = DivideByShift(even, 1);

		// Save the luma result for later output in the correct order
		bg_even_value = even;

		// Apply the odd border filter to the lowpass band
		odd += 11 * bg_lowpass_ptr[column + 0];
		odd -=  4 * bg_lowpass_ptr[column - 1];
		odd +=  1 * bg_lowpass_ptr[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= bg_highpass_ptr[column];
		odd = DivideByShift(odd, 1);

		// Save the luma result for later output in the correct order
		bg_odd_value = odd;

		rg_even_value >>= 2;
		gg_even_value >>= 2;
		bg_even_value >>= 2;
		rg_odd_value >>= 2;
		gg_odd_value >>= 2;
		bg_odd_value >>= 2;

		if(rg_even_value < 0) rg_even_value = 0; if(rg_even_value > 1023) rg_even_value = 1023;
		if(gg_even_value < 0) gg_even_value = 0; if(gg_even_value > 1023) gg_even_value = 1023;
		if(bg_even_value < 0) bg_even_value = 0; if(bg_even_value > 1023) bg_even_value = 1023;
		if(rg_odd_value < 0) rg_odd_value = 0; if(rg_odd_value > 1023) rg_odd_value = 1023;
		if(gg_odd_value < 0) gg_odd_value = 0; if(gg_odd_value > 1023) gg_odd_value = 1023;
		if(bg_odd_value < 0) bg_odd_value = 0; if(bg_odd_value > 1023) bg_odd_value = 1023;

		switch(format)
		{
		case DECODED_FORMAT_RG30:
		case DECODED_FORMAT_AB10:
			*(RG30ptr++) = (bg_even_value<<20)+(gg_even_value<<10)+rg_even_value;
			*(RG30ptr++) = (bg_odd_value<<20)+(gg_odd_value<<10)+rg_odd_value;
			break;
		case DECODED_FORMAT_AR10:
			*(RG30ptr++) = (rg_even_value<<20)+(gg_even_value<<10)+bg_even_value;
			*(RG30ptr++) = (rg_odd_value<<20)+(gg_odd_value<<10)+bg_odd_value;
			break;
		case DECODED_FORMAT_R210:
			*(RG30ptr++) = SwapInt32((rg_even_value<<20)+(gg_even_value<<10)+(bg_even_value));
			*(RG30ptr++) = SwapInt32((rg_odd_value<<20)+(gg_odd_value<<10)+(bg_odd_value));
			break;
		case DECODED_FORMAT_DPX0:
			*(RG30ptr++) = SwapInt32((rg_even_value<<22)+(gg_even_value<<12)+(bg_even_value<<2));
			*(RG30ptr++) = SwapInt32((rg_odd_value<<22)+(gg_odd_value<<12)+(bg_odd_value<<2));
			break;
		}


		// Should have exited the loop at the column for right border processing
	//	assert(column == last_column);
		// Advance to the next row of coefficients in each channel
		gg_lowpass_ptr += lowpass_pitch[0];
		bg_lowpass_ptr += lowpass_pitch[1];
		rg_lowpass_ptr += lowpass_pitch[2];
		gg_highpass_ptr += highpass_pitch[0];
		bg_highpass_ptr += highpass_pitch[1];
		rg_highpass_ptr += highpass_pitch[2];

		// Advance the output pointer to the next row
		output += output_pitch;
	}
}



#if 0

// Apply the inverse horizontal transform to reconstruct a strip of rows (old version using MMX)
void InvertHorizontalStrip16sToRow16u(PIXEL *lowpass_band, int lowpass_pitch,
									  PIXEL *highpass_band, int highpass_pitch,
									  PIXEL16U *output, int output_pitch,
									  ROI roi, int precision)
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	const int column_step = 4;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Calculate the shift required to output 16-bit pixels
	int scale_shift = (16 - precision);
//	int protection = 0x7fff - 2047;
	int protection = 0x7fff - (2<<precision) + 1;


	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL16U);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m64 low1_pi16;	// Lowpass coefficients
		__m64 low2_pi16;
		__m64 high1_pi16;	// Highpass coefficients
		__m64 high2_pi16;

		__m64 overflowprotect_pi16 = _mm_set1_pi16(protection);

		PIXEL16U *colptr;

		int32_t even;
		int32_t odd;

		// The fast loop computes output points starting at the third column
		__m64 *outptr = (__m64 *)&output[2];

		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Scale the result to the full 16-bit range
		even <<= scale_shift;

		// Place the even result in the even column
		//even >>= _INVERSE_TEMPORAL_PRESCALE;
		output[0] = SATURATE_16U(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Scale the result to the full 16-bit range
		odd <<= scale_shift;

		// Place the odd result in the odd column
		//odd >>= _INVERSE_TEMPORAL_PRESCALE;
		output[1] = SATURATE_16U(odd);

		// Preload the first four lowpass coefficients
		low1_pi16 = *((__m64 *)&lowpass[column]);

		// Preload the first four highpass coefficients
		high1_pi16 = *((__m64 *)&highpass[column]);

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m64 even_pi16;	// Result of convolution with even filter
			__m64 odd_pi16;		// Result of convolution with odd filter
			__m64 temp_pi16;
			__m64 out1_pi16;	// Reconstructed data - first set of four
			__m64 out2_pi16;	// Reconstructed data - second set of four
			__m64 mask_pi16;
			__m64 lsb_pi16;
			__m64 sign_pi16;
			__m64 high_pi16;
			__m64 half_pi16 = _mm_set1_pi16(4); //DAN031604 4 to 6

			// Right shift to reduce the output pixel size to one byte
			//__m64 scale_si64 = _mm_cvtsi32_si64(scale_shift);

			// Preload the next four lowpass coefficients
			low2_pi16 = *((__m64 *)&lowpass[column+4]);

#define COLUMN_0		_MM_SHUFFLE(0, 3, 2, 1)
#define COLUMN_PLUS_1	_MM_SHUFFLE(1, 0, 3, 2)

			/***** Compute the first two even and two odd output points *****/

			// Apply the even reconstruction filter to the lowpass band
		/*	even_pi16 = low1_pi16;  //- 1
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);  // temp = low[0]<<3
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1)); // + 0
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); // + 1
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);

			#if 1
			// Apply the rounding adjustment
			even_pi16 = _mm_adds_pi16(even_pi16, _mm_set1_pi16(4));
			#endif
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);
	*/
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_PLUS_1); 			// [col+1]
			even_pi16 = _mm_subs_pi16(low1_pi16, temp_pi16);  					// [col-1] - [col+1]
			even_pi16 = _mm_adds_pi16(even_pi16, half_pi16);  					// [col-1] - [col+1] + 4
			even_pi16 = _mm_srai_pi16(even_pi16, 3);		  					// ([col-1] - [col+1] + 4) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_0); 	//
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); 					// (([col-1] - [col+1] + 4) >> 3) + [col+0]

			// Shift the highpass correction by one column
			high1_pi16 = _mm_srli_si64(high1_pi16, 16);

			// Prescale for 8bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			even_pi16 = _mm_adds_pi16(even_pi16, overflowprotect_pi16);
			even_pi16 = _mm_subs_pu16(even_pi16, overflowprotect_pi16);
			even_pi16 = _mm_srai_pi16(even_pi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
	/*		odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);

			#if 1
			// Apply the rounding adjustment
			odd_pi16 = _mm_adds_pi16(odd_pi16, _mm_set1_pi16(4));
			#endif
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);
	*/

			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_PLUS_1); 			// [col+1]
			odd_pi16 = _mm_subs_pi16(temp_pi16, low1_pi16);  					// [col+1] - [col-1]
			odd_pi16 = _mm_adds_pi16(odd_pi16, half_pi16);  					// [col+1] - [col-1] + 4
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);		  						// ([col+1] - [col-1] + 4) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_0); 	//
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16); 						// (([col+1] - [col-1] + 4) >> 3) + [col+0]

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			odd_pi16 = _mm_adds_pi16(odd_pi16, overflowprotect_pi16);
			odd_pi16 = _mm_subs_pu16(odd_pi16, overflowprotect_pi16);
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out1_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());

			// Scale the result to the full 16-bit range
			out1_pi16 = _mm_slli_pi16(out1_pi16, scale_shift);

			// Store the first four results
			*(outptr++) = out1_pi16;


			/***** Compute the second two even and two odd output points *****/

			// Preload the highpass correction
			high2_pi16 = *((__m64 *)&highpass[column+4]);

			// Shift in the new pixels for the next stage of the loop
			low1_pi16 = _mm_srli_si64(low1_pi16, 32);
			temp_pi16 = _mm_slli_si64(low2_pi16, 32);
			low1_pi16 = _mm_or_si64(low1_pi16, temp_pi16);

			// Apply the even reconstruction filter to the lowpass band
	/*		even_pi16 = low1_pi16;
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);

			#if 1
			// Apply the rounding adjustment
			even_pi16 = _mm_adds_pi16(even_pi16, _mm_set1_pi16(4));
			#endif
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);
	*/
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_PLUS_1); 			// [col+1]
			even_pi16 = _mm_subs_pi16(low1_pi16, temp_pi16);  					// [col-1] - [col+1]
			even_pi16 = _mm_adds_pi16(even_pi16, half_pi16);  					// [col-1] - [col+1] + 4
			even_pi16 = _mm_srai_pi16(even_pi16, 3);		  					// ([col-1] - [col+1] + 4) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_0); 	//
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); 					// (([col-1] - [col+1] + 4) >> 3) + [col+0]


			// Shift in the next two highpass coefficients
			high1_pi16 = _mm_srli_si64(high1_pi16, 2*16);
			high1_pi16 = _mm_or_si64(high1_pi16, _mm_slli_si64(high2_pi16, 16));

			// Prescale for 8bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			even_pi16 = _mm_adds_pi16(even_pi16, overflowprotect_pi16);
			even_pi16 = _mm_subs_pu16(even_pi16, overflowprotect_pi16);
			even_pi16 = _mm_srai_pi16(even_pi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
	/*		odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);

			#if 1
			// Apply the rounding adjustment
			odd_pi16 = _mm_adds_pi16(odd_pi16, _mm_set1_pi16(4));
			#endif
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);
	*/
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_PLUS_1); 			// [col+1]
			odd_pi16 = _mm_subs_pi16(temp_pi16, low1_pi16);  					// [col+1] - [col-1]
			odd_pi16 = _mm_adds_pi16(odd_pi16, half_pi16);  					// [col+1] - [col-1] + 4
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);		  						// ([col+1] - [col-1] + 4) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_0); 	//
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16); 						// (([col+1] - [col-1] + 4) >> 3) + [col+0]

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			odd_pi16 = _mm_adds_pi16(odd_pi16, overflowprotect_pi16);
			odd_pi16 = _mm_subs_pu16(odd_pi16, overflowprotect_pi16);
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out2_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());

			// Scale the result to the full 16-bit range
			out2_pi16 = _mm_slli_pi16(out2_pi16, scale_shift);

			// Store the second four results
			*(outptr++) = out2_pi16;

			// The second four lowpass coefficients will be the current values
			low1_pi16 = low2_pi16;

			// The second four highpass coefficients will be the current values
			high1_pi16 = high2_pi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

		// The fast processing loop is one column behind the actual column
		column++;

		// Process the rest of the columns up to the last column in the row
		colptr = (PIXEL16U *)outptr;

		for (; column < last_column; column++)
		{
			int32_t even = 0;		// Result of convolution with even filter
			int32_t odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even <<= scale_shift;

			// Place the even result in the even column
			//even >>= _INVERSE_TEMPORAL_PRESCALE;
			*(colptr++) = SATURATE_16U(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd <<= scale_shift;

			// Place the odd result in the odd column
			//odd >>= _INVERSE_TEMPORAL_PRESCALE;
			*(colptr++) = SATURATE_16U(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Scale the result to the full 16-bit range
		even <<= scale_shift;

		// Place the even result in the even column
		//even >>= _INVERSE_TEMPORAL_PRESCALE;
		*(colptr++) = SATURATE_16U(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Scale the result to the full 16-bit range
		odd <<= scale_shift;

		// Place the odd result in the odd column
		//odd >>= _INVERSE_TEMPORAL_PRESCALE;
		*(colptr++) = SATURATE_16U(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}

	//_mm_empty();	// Clear the mmx register state
}

#else

// Apply the inverse horizontal transform to reconstruct a strip of rows (new version using SSE2)
void InvertHorizontalStrip16sToRow16u(PIXEL *lowpass_band, int lowpass_pitch,
									  PIXEL *highpass_band, int highpass_pitch,
									  PIXEL16U *output, int output_pitch,
									  ROI roi, int precision)
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	const int last_column = width - 1;

	// The fast loop processes eight columns per iteration
	const int column_step = 8;

	// The fast loop preloads eight columns of coefficients for the next iteration
	const int fast_loop_width = 2 * column_step;

	// Must terminate the fast loop when it reaches the post processing column
	const int fast_loop_column = width - (width % column_step) - fast_loop_width;
	const int post_column = fast_loop_column - (fast_loop_width % column_step);

	// Calculate the shift required to output 16-bit pixels
	int scale_shift = (16 - precision);

	//	int protection = 0x7fff - 2047;
	int protection = 0x7fff - (2 << precision) + 1;

	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m128i low1_epi16;		// Lowpass coefficients
		__m128i low2_epi16;
		__m128i high1_epi16;	// Highpass coefficients
		__m128i high2_epi16;

		__m128i lsh1_epi16;		// Coefficients left shifted by one column
		__m128i rsh1_epi16;		// Coefficients right shifted by one column

		__m128i even_epi16;		// Result of convolution with even filter
		__m128i odd_epi16;		// Result of convolution with odd filter
		//__m128i temp_epi16;
		__m128i out1_epi16;		// First group of eight output values
		__m128i out2_epi16;		// Second group of eight output values

		__m128i half_epi16 = _mm_set1_epi16(4);		//DAN031604 4 to 6

		__m128i protection_epi16 = _mm_set1_epi16(protection);

		PIXEL16U *colptr;

		int low1;

		int32_t even;
		int32_t odd;

		// The prolog to the fast loop computes output points starting at the first column
		__m128i *outptr = (__m128i *)&output[0];

		// Start processing at the beginning of the row
		int column = 0;

		// Load the first eight lowpass coefficients
		//low1_epi16 = *((__m128i *)&lowpass[column]);
		low1_epi16 = _mm_load_si128((__m128i *)&lowpass[0]);

		// Load the first eight highpass coefficients
		//high1_epi16 = *((__m128i *)&highpass[column]);
		high1_epi16 = _mm_load_si128((__m128i *)&highpass[0]);

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even, 8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Scale the result to the full 16-bit range
		//even <<= scale_shift;

		// Place the even result in the even column
		//even >>= _INVERSE_TEMPORAL_PRESCALE;
		//output[0] = SATURATE_16U(even);
		even <<= scale_shift;
		even = SATURATE_16U(even); //DAN20070813 -- fix for left edge of alpha channels.
		even >>= scale_shift;

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd, 8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Scale the result to the full 16-bit range
		//odd <<= scale_shift;

		// Place the odd result in the odd column
		//odd >>= _INVERSE_TEMPORAL_PRESCALE;
		//output[1] = SATURATE_16U(odd);
		odd <<= scale_shift;
		odd = SATURATE_16U(odd);//DAN20070813 -- fix for left edge of alpha channels.
		odd >>= scale_shift;

		// Save the even and odd output values for use in the prolog to the fast loop


		// Must have enough coefficients to use the fast loop
		if (fast_loop_width <= width)
		{
			// Compute the first group of eight output pairs (even and odd)

			// Preload the next eight lowpass coefficients
			//low1_epi16 = *((__m128i *)&lowpass[column]);
			low2_epi16 = _mm_load_si128((__m128i *)&lowpass[column_step]);

			// Preload the next eight highpass coefficients
			//high1_epi16 = *((__m128i *)&highpass[column]);
			high2_epi16 = _mm_load_si128((__m128i *)&highpass[column_step]);

			// Shift the coefficients by one column to the left and right
			lsh1_epi16 = _mm_slli_si128(low1_epi16, 2);				// [col-1]
			rsh1_epi16 = _mm_srli_si128(low1_epi16, 2);				// [col+1]

			// Insert the first coefficient from the next group into the last coefficient in this group
			rsh1_epi16 = _mm_insert_epi16(rsh1_epi16, _mm_extract_epi16(low2_epi16, 0), 7);

			// Apply the three point filter for the even output values
			even_epi16 = _mm_subs_epi16(lsh1_epi16, rsh1_epi16);	// [col-1] - [col+1]
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);  	// [col-1] - [col+1] + 4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);				// ([col-1] - [col+1] + 4) >> 3
			even_epi16 = _mm_adds_epi16(even_epi16, low1_epi16); 	// (([col-1] - [col+1] + 4) >> 3) + [col+0]

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, protection_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, protection_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the three point filter for the odd output values
			odd_epi16 = _mm_subs_epi16(rsh1_epi16, lsh1_epi16);		// [col+1] - [col-1]
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);  	// [col+1] - [col-1] + 4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);				// ([col+1] - [col-1] + 4) >> 3
			odd_epi16 = _mm_adds_epi16(odd_epi16, low1_epi16);		// (([col+1] - [col-1] + 4) >> 3) + [col+0]

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, protection_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, protection_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Insert the even and odd output values from the left border
			even_epi16 = _mm_insert_epi16(even_epi16, even, 0);
			odd_epi16 = _mm_insert_epi16(odd_epi16, odd, 0);

			// Interleave the even and odd results
			out1_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			out2_epi16 = _mm_unpackhi_epi16(even_epi16, odd_epi16);

			// Scale the result to the full 16-bit range
			out1_epi16 = _mm_slli_epi16(out1_epi16, scale_shift);
			out2_epi16 = _mm_slli_epi16(out2_epi16, scale_shift);

			// Store the first eight even and odd pairs from the first eight coefficients
			_mm_storeu_si128(outptr++, out1_epi16);

			// Store the second eight even and odd pairs from the first eight coefficients
			_mm_storeu_si128(outptr++, out2_epi16);

			// Save the last coefficient from this group of coefficients
			low1 = _mm_extract_epi16(low1_epi16, 7);

			// The fast loop starts with the second group of coefficients
			low1_epi16 = low2_epi16;
			high1_epi16 = high2_epi16;

			// Done processing the first eight columns of lowpass and highpass coefficients
			column += column_step;
		}
		else
		{
			// Scale the even and odd output values to the full 16-bit range
			even <<= scale_shift;
			odd <<= scale_shift;

			// Store the even and odd output values from the left border
			output[0] = SATURATE_16U(even);
			output[1] = SATURATE_16U(odd);
		}


		// Process eight lowpass and highpass coefficients per iteration of the fast loop
		for (; column < post_column; column += column_step)
		{
			////__m128i mask_epi16;
			////__m128i lsb_epi16;
			////__m128i sign_epi16;
			//__m128i high_epi16;

			// Preload the next eight lowpass coefficients
			low2_epi16 = _mm_load_si128((__m128i *)&lowpass[column + column_step]);

			// Preload the next eight highpass coefficients
			high2_epi16 = _mm_load_si128((__m128i *)&highpass[column + column_step]);

			// Shift the coefficients by one column to the left and right
			lsh1_epi16 = _mm_slli_si128(low1_epi16, 2);				// [col-1]
			rsh1_epi16 = _mm_srli_si128(low1_epi16, 2);				// [col+1]

			// Fill the first coefficient with the last coefficient from the previous group
			lsh1_epi16 = _mm_insert_epi16(lsh1_epi16, low1, 0);

			// Fill the last coefficient with the first coefficient from the next group
			rsh1_epi16 = _mm_insert_epi16(rsh1_epi16, _mm_extract_epi16(low2_epi16, 0), 7);

			// Apply the three point filter for the even output values
			even_epi16 = _mm_subs_epi16(lsh1_epi16, rsh1_epi16);	// [col-1] - [col+1]
			even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);  	// [col-1] - [col+1] + 4
			even_epi16 = _mm_srai_epi16(even_epi16, 3);				// ([col-1] - [col+1] + 4) >> 3
			even_epi16 = _mm_adds_epi16(even_epi16, low1_epi16); 	// (([col-1] - [col+1] + 4) >> 3) + [col+0]

			// Add the highpass correction and divide by two
			even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
			even_epi16 = _mm_adds_epi16(even_epi16, protection_epi16);
			even_epi16 = _mm_subs_epu16(even_epi16, protection_epi16);
			even_epi16 = _mm_srai_epi16(even_epi16, 1);

			// Apply the three point filter for the odd output values
			odd_epi16 = _mm_subs_epi16(rsh1_epi16, lsh1_epi16);		// [col+1] - [col-1]
			odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);  	// [col+1] - [col-1] + 4
			odd_epi16 = _mm_srai_epi16(odd_epi16, 3);				// ([col+1] - [col-1] + 4) >> 3
			odd_epi16 = _mm_adds_epi16(odd_epi16, low1_epi16);		// (([col+1] - [col-1] + 4) >> 3) + [col+0]

			// Subtract the highpass correction and divide by two
			odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
			odd_epi16 = _mm_adds_epi16(odd_epi16, protection_epi16);
			odd_epi16 = _mm_subs_epu16(odd_epi16, protection_epi16);
			odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

			// Interleave the even and odd results
			out1_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
			out2_epi16 = _mm_unpackhi_epi16(even_epi16, odd_epi16);

			// Scale the result to the full 16-bit range
			out1_epi16 = _mm_slli_epi16(out1_epi16, scale_shift);
			out2_epi16 = _mm_slli_epi16(out2_epi16, scale_shift);

			// Store the first eight even and odd pairs from the first eight coefficients
			_mm_storeu_si128(outptr++, out1_epi16);

			// Store the second eight even and odd pairs from the first eight coefficients
			_mm_storeu_si128(outptr++, out2_epi16);

			// Save the last coefficient from this group of coefficients
			low1 = _mm_extract_epi16(low1_epi16, 7);

			// The next iteration of this loop starts with the second group of coefficients
			low1_epi16 = low2_epi16;
			high1_epi16 = high2_epi16;
		}

		// Should have exited the loop at the post processing column
		assert(column == post_column);

		// The next phase will store eight pairs of output values (the last pair is bad)
		colptr = (PIXEL16U *)outptr;

		// Process the next eight coefficients which have already been loaded

		// Shift the coefficients by one column to the left and right
		lsh1_epi16 = _mm_slli_si128(low1_epi16, 2);				// [col-1]
		rsh1_epi16 = _mm_srli_si128(low1_epi16, 2);				// [col+1]

		// Fill the first coefficient with the last coefficient from the previous group
		lsh1_epi16 = _mm_insert_epi16(lsh1_epi16, low1, 0);

		// Apply the three point filter for the even output values
		even_epi16 = _mm_subs_epi16(lsh1_epi16, rsh1_epi16);	// [col-1] - [col+1]
		even_epi16 = _mm_adds_epi16(even_epi16, half_epi16);  	// [col-1] - [col+1] + 4
		even_epi16 = _mm_srai_epi16(even_epi16, 3);				// ([col-1] - [col+1] + 4) >> 3
		even_epi16 = _mm_adds_epi16(even_epi16, low1_epi16); 	// (([col-1] - [col+1] + 4) >> 3) + [col+0]

		// Add the highpass correction and divide by two
		even_epi16 = _mm_adds_epi16(even_epi16, high1_epi16);
		even_epi16 = _mm_adds_epi16(even_epi16, protection_epi16);
		even_epi16 = _mm_subs_epu16(even_epi16, protection_epi16);
		even_epi16 = _mm_srai_epi16(even_epi16, 1);

		// Apply the three point filter for the odd output values
		odd_epi16 = _mm_subs_epi16(rsh1_epi16, lsh1_epi16);		// [col+1] - [col-1]
		odd_epi16 = _mm_adds_epi16(odd_epi16, half_epi16);  	// [col+1] - [col-1] + 4
		odd_epi16 = _mm_srai_epi16(odd_epi16, 3);				// ([col+1] - [col-1] + 4) >> 3
		odd_epi16 = _mm_adds_epi16(odd_epi16, low1_epi16);		// (([col+1] - [col-1] + 4) >> 3) + [col+0]

		// Subtract the highpass correction and divide by two
		odd_epi16 = _mm_subs_epi16(odd_epi16, high1_epi16);
		odd_epi16 = _mm_adds_epi16(odd_epi16, protection_epi16);
		odd_epi16 = _mm_subs_epu16(odd_epi16, protection_epi16);
		odd_epi16 = _mm_srai_epi16(odd_epi16, 1);

		// Interleave the even and odd results
		out1_epi16 = _mm_unpacklo_epi16(even_epi16, odd_epi16);
		out2_epi16 = _mm_unpackhi_epi16(even_epi16, odd_epi16);

		// Scale the result to the full 16-bit range
		out1_epi16 = _mm_slli_epi16(out1_epi16, scale_shift);
		out2_epi16 = _mm_slli_epi16(out2_epi16, scale_shift);

		// Store the first eight even and odd pairs from the first eight coefficients
		_mm_storeu_si128(outptr++, out1_epi16);

		// Store the second eight even and odd pairs from the first eight coefficients
		_mm_storeu_si128(outptr++, out2_epi16);

		// Have processed only seven columns because the last column is bad
		column += 7;
		colptr += (2 * 7);

		// Process the rest of the columns up to the last column in the row
		for (; column < last_column; column++)
		{
			even = 0;
			odd = 0;

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even <<= scale_shift;

			// Place the even result in the even column
			//even >>= _INVERSE_TEMPORAL_PRESCALE;
			*(colptr++) = SATURATE_16U(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd <<= scale_shift;

			// Place the odd result in the odd column
			//odd >>= _INVERSE_TEMPORAL_PRESCALE;
			*(colptr++) = SATURATE_16U(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Scale the result to the full 16-bit range
		even <<= scale_shift;

		// Place the even result in the even column
		//even >>= _INVERSE_TEMPORAL_PRESCALE;
		*(colptr++) = SATURATE_16U(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Scale the result to the full 16-bit range
		odd <<= scale_shift;

		// Place the odd result in the odd column
		//odd >>= _INVERSE_TEMPORAL_PRESCALE;
		*(colptr++) = SATURATE_16U(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}
}

#endif



// Apply the inverse horizontal transform to reconstruct a strip of rows (new version using SSE2)
void InvertHorizontalBypassStrip16sToRow16u(PIXEL *lowpass_band, int lowpass_pitch,
									  PIXEL16U *output, int output_pitch,
									  ROI roi, int precision)
{
	int height = roi.height;
	int width = roi.width<<1;
	PIXEL *lowpass = lowpass_band;

	// Calculate the shift required to output 16-bit pixels
	int scale_shift = (16 - precision)-1;

	//	int protection = 0x7fff - 2047;
	int protection = 0x7fff - (2 << precision) + 1;

	int row;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL);

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{	
		PIXEL16U *colptr = output;
		__m128i low_epi16;
		int column = 0;
		int width8 = (width>>3)<<3;
		__m128i protection_epi16 = _mm_set1_epi16(protection);
		// Process the rest of the columns up to the last column in the row

		if(ISALIGNED16(lowpass) && ISALIGNED16(colptr))
		{
			for (; column < width8; column+=8)
			{
				low_epi16 = _mm_load_si128((__m128i *)&lowpass[column]);
				low_epi16 = _mm_adds_epi16(low_epi16, protection_epi16);
				low_epi16 = _mm_subs_epu16(low_epi16, protection_epi16);
				low_epi16 = _mm_slli_epi16(low_epi16, scale_shift);
				_mm_store_si128((__m128i *)&colptr[column], low_epi16);
			}
		}
		else
		{
			for (; column < width8; column+=8)
			{
				low_epi16 = _mm_loadu_si128((__m128i *)&lowpass[column]);
				low_epi16 = _mm_adds_epi16(low_epi16, protection_epi16);
				low_epi16 = _mm_subs_epu16(low_epi16, protection_epi16);
				low_epi16 = _mm_slli_epi16(low_epi16, scale_shift);
				_mm_storeu_si128((__m128i *)&colptr[column], low_epi16);
			}
		}

		for (; column < width; column++)
		{
			colptr[column] = lowpass[column] << scale_shift;
		}

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		output += output_pitch;
	}
}


#if 0
// Apply the inverse horizontal transform to reconstruct a strip of rows
void InvertHorizontalStripRGB444ToB64A(PIXEL *lowpass_band, int lowpass_pitch,
									   PIXEL *highpass_band, int highpass_pitch,
									   PIXEL16U *output, int output_pitch,
									   ROI roi, int precision)
{
	int height = roi.height;
	int width = roi.width;
	PIXEL *lowpass = lowpass_band;
	PIXEL *highpass = highpass_band;
	const int column_step = 4;
	const int last_column = width - 1;
	int post_column = last_column - (last_column % column_step);
	int row;

	// Calculate the shift required to output 16-bit pixels
	int scale_shift = (16 - precision);
//	int protection = 0x7fff - 2047;
	int protection = 0x7fff - (2<<precision) + 1;

	// Convert the pitch to units of pixels
	lowpass_pitch /= sizeof(PIXEL);
	highpass_pitch /= sizeof(PIXEL);
	output_pitch /= sizeof(PIXEL16U);

	// Adjust the end of the fast loop if necessary
	if (post_column == last_column)
		post_column -= column_step;

	// Process each row of the strip
	for (row = 0; row < height; row++)
	{
		__m64 low1_pi16;	// Lowpass coefficients
		__m64 low2_pi16;
		__m64 high1_pi16;	// Highpass coefficients
		__m64 high2_pi16;

		__m64 overflowprotect_pi16 = _mm_set1_pi16(protection);

		PIXEL16U *colptr;

		int32_t even;
		int32_t odd;

		// The fast loop computes output points starting at the third column
		__m64 *outptr = (__m64 *)&output[2];

		// Start processing at the beginning of the row
		int column = 0;

		// Process the first two output points with special filters for the left border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 11 * lowpass[column + 0];
		even -=  4 * lowpass[column + 1];
		even +=  1 * lowpass[column + 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Scale the result to the full 16-bit range
		even <<= scale_shift;

		// Place the even result in the even column
		//even >>= _INVERSE_TEMPORAL_PRESCALE;
		output[0] = SATURATE_16U(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 5 * lowpass[column + 0];
		odd += 4 * lowpass[column + 1];
		odd -= 1 * lowpass[column + 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Scale the result to the full 16-bit range
		odd <<= scale_shift;

		// Place the odd result in the odd column
		//odd >>= _INVERSE_TEMPORAL_PRESCALE;
		output[1] = SATURATE_16U(odd);

		// Preload the first four lowpass coefficients
		low1_pi16 = *((__m64 *)&lowpass[column]);

		// Preload the first four highpass coefficients
		high1_pi16 = *((__m64 *)&highpass[column]);

		// The reconstruction filters use pixels starting at the first column
		for (; column < post_column; column += column_step)
		{
			__m64 even_pi16;	// Result of convolution with even filter
			__m64 odd_pi16;		// Result of convolution with odd filter
			__m64 temp_pi16;
			__m64 out1_pi16;	// Reconstructed data - first set of four
			__m64 out2_pi16;	// Reconstructed data - second set of four
			__m64 mask_pi16;
			__m64 lsb_pi16;
			__m64 sign_pi16;
			__m64 high_pi16;
			__m64 half_pi16 = _mm_set1_pi16(4); //DAN031604 4 to 6

			// Right shift to reduce the output pixel size to one byte
			//__m64 scale_si64 = _mm_cvtsi32_si64(scale_shift);

			// Preload the next four lowpass coefficients
			low2_pi16 = *((__m64 *)&lowpass[column+4]);

#define COLUMN_0		_MM_SHUFFLE(0, 3, 2, 1)
#define COLUMN_PLUS_1	_MM_SHUFFLE(1, 0, 3, 2)

			/***** Compute the first two even and two odd output points *****/

			// Apply the even reconstruction filter to the lowpass band
		/*	even_pi16 = low1_pi16;  //- 1
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);  // temp = low[0]<<3
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1)); // + 0
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2)); // + 1
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);

			#if 1
			// Apply the rounding adjustment
			even_pi16 = _mm_adds_pi16(even_pi16, _mm_set1_pi16(4));
			#endif
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);
	*/
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_PLUS_1); 			// [col+1]
			even_pi16 = _mm_subs_pi16(low1_pi16, temp_pi16);  					// [col-1] - [col+1]
			even_pi16 = _mm_adds_pi16(even_pi16, half_pi16);  					// [col-1] - [col+1] + 4
			even_pi16 = _mm_srai_pi16(even_pi16, 3);		  					// ([col-1] - [col+1] + 4) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_0); 	//
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); 					// (([col-1] - [col+1] + 4) >> 3) + [col+0]

			// Shift the highpass correction by one column
			high1_pi16 = _mm_srli_si64(high1_pi16, 16);

			// Prescale for 8bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			even_pi16 = _mm_adds_pi16(even_pi16, overflowprotect_pi16);
			even_pi16 = _mm_subs_pu16(even_pi16, overflowprotect_pi16);
			even_pi16 = _mm_srai_pi16(even_pi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
	/*		odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);

			#if 1
			// Apply the rounding adjustment
			odd_pi16 = _mm_adds_pi16(odd_pi16, _mm_set1_pi16(4));
			#endif
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);
	*/

			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_PLUS_1); 			// [col+1]
			odd_pi16 = _mm_subs_pi16(temp_pi16, low1_pi16);  					// [col+1] - [col-1]
			odd_pi16 = _mm_adds_pi16(odd_pi16, half_pi16);  					// [col+1] - [col-1] + 4
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);		  						// ([col+1] - [col-1] + 4) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_0); 	//
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16); 						// (([col+1] - [col-1] + 4) >> 3) + [col+0]

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			odd_pi16 = _mm_adds_pi16(odd_pi16, overflowprotect_pi16);
			odd_pi16 = _mm_subs_pu16(odd_pi16, overflowprotect_pi16);
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out1_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());

			// Scale the result to the full 16-bit range
			out1_pi16 = _mm_slli_pi16(out1_pi16, scale_shift);

			// Store the first four results
			*(outptr++) = out1_pi16;


			/***** Compute the second two even and two odd output points *****/

			// Preload the highpass correction
			high2_pi16 = *((__m64 *)&highpass[column+4]);

			// Shift in the new pixels for the next stage of the loop
			low1_pi16 = _mm_srli_si64(low1_pi16, 32);
			temp_pi16 = _mm_slli_si64(low2_pi16, 32);
			low1_pi16 = _mm_or_si64(low1_pi16, temp_pi16);

			// Apply the even reconstruction filter to the lowpass band
	/*		even_pi16 = low1_pi16;
			temp_pi16 = _mm_slli_pi16(low1_pi16, 3);
			temp_pi16 = _mm_shuffle_pi16(temp_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16);
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			even_pi16 = _mm_subs_pi16(even_pi16, temp_pi16);

			#if 1
			// Apply the rounding adjustment
			even_pi16 = _mm_adds_pi16(even_pi16, _mm_set1_pi16(4));
			#endif
			// Divide by eight
			even_pi16 = _mm_srai_pi16(even_pi16, 3);
	*/
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_PLUS_1); 			// [col+1]
			even_pi16 = _mm_subs_pi16(low1_pi16, temp_pi16);  					// [col-1] - [col+1]
			even_pi16 = _mm_adds_pi16(even_pi16, half_pi16);  					// [col-1] - [col+1] + 4
			even_pi16 = _mm_srai_pi16(even_pi16, 3);		  					// ([col-1] - [col+1] + 4) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_0); 	//
			even_pi16 = _mm_adds_pi16(even_pi16, temp_pi16); 					// (([col-1] - [col+1] + 4) >> 3) + [col+0]


			// Shift in the next two highpass coefficients
			high1_pi16 = _mm_srli_si64(high1_pi16, 2*16);
			high1_pi16 = _mm_or_si64(high1_pi16, _mm_slli_si64(high2_pi16, 16));

			// Prescale for 8bit output - DAN 4/5/02
			high_pi16 = high1_pi16;

			// Add the highpass correction
			even_pi16 = _mm_adds_pi16(even_pi16, high_pi16);
			even_pi16 = _mm_adds_pi16(even_pi16, overflowprotect_pi16);
			even_pi16 = _mm_subs_pu16(even_pi16, overflowprotect_pi16);
			even_pi16 = _mm_srai_pi16(even_pi16, 1);

			// Apply the odd reconstruction filter to the lowpass band
	/*		odd_pi16 = _mm_slli_pi16(low1_pi16, 3);
			odd_pi16 = _mm_shuffle_pi16(odd_pi16, _MM_SHUFFLE(0, 3, 2, 1));
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, _MM_SHUFFLE(1, 0, 3, 2));
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16);
			odd_pi16 = _mm_subs_pi16(odd_pi16, low1_pi16);

			#if 1
			// Apply the rounding adjustment
			odd_pi16 = _mm_adds_pi16(odd_pi16, _mm_set1_pi16(4));
			#endif
			// Divide by eight
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);
	*/
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_PLUS_1); 			// [col+1]
			odd_pi16 = _mm_subs_pi16(temp_pi16, low1_pi16);  					// [col+1] - [col-1]
			odd_pi16 = _mm_adds_pi16(odd_pi16, half_pi16);  					// [col+1] - [col-1] + 4
			odd_pi16 = _mm_srai_pi16(odd_pi16, 3);		  						// ([col+1] - [col-1] + 4) >> 3
			temp_pi16 = _mm_shuffle_pi16(low1_pi16, COLUMN_0); 	//
			odd_pi16 = _mm_adds_pi16(odd_pi16, temp_pi16); 						// (([col+1] - [col-1] + 4) >> 3) + [col+0]

			// Subtract the highpass correction
			odd_pi16 = _mm_subs_pi16(odd_pi16, high_pi16);
			odd_pi16 = _mm_adds_pi16(odd_pi16, overflowprotect_pi16);
			odd_pi16 = _mm_subs_pu16(odd_pi16, overflowprotect_pi16);
			odd_pi16 = _mm_srai_pi16(odd_pi16, 1);

			// Interleave the even and odd results
			out2_pi16 = _mm_unpacklo_pi16(even_pi16, odd_pi16);
			//out_pi16 = _mm_max_pi16(out_pi16, _mm_setzero_si64());

			// Scale the result to the full 16-bit range
			out2_pi16 = _mm_slli_pi16(out2_pi16, scale_shift);

			// Store the second four results
			*(outptr++) = out2_pi16;

			// The second four lowpass coefficients will be the current values
			low1_pi16 = low2_pi16;

			// The second four highpass coefficients will be the current values
			high1_pi16 = high2_pi16;
		}

		// Should have exited the loop with the column equal to the post processing column
		assert(column == post_column);

		// The fast processing loop is one column behind the actual column
		column++;

		// Process the rest of the columns up to the last column in the row
		colptr = (PIXEL16U *)outptr;

		for (; column < last_column; column++)
		{
			int32_t even = 0;		// Result of convolution with even filter
			int32_t odd = 0;		// Result of convolution with odd filter

			// Apply the even reconstruction filter to the lowpass band
			even += lowpass[column - 1];
			even -= lowpass[column + 1];
			even += 4; //DAN20050921
			even >>= 3;
			even += lowpass[column + 0];

			// Add the highpass correction
			even += highpass[column];
			even = DivideByShift(even, 1);

			// Reduce the precision to eight bits
			even <<= scale_shift;

			// Place the even result in the even column
			//even >>= _INVERSE_TEMPORAL_PRESCALE;
			*(colptr++) = SATURATE_16U(even);

			// Apply the odd reconstruction filter to the lowpass band
			odd -= lowpass[column - 1];
			odd += lowpass[column + 1];
			odd += 4; //DAN20050921
			odd >>= 3;
			odd += lowpass[column + 0];

			// Subtract the highpass correction
			odd -= highpass[column];
			odd = DivideByShift(odd, 1);

			// Reduce the precision to eight bits
			odd <<= scale_shift;

			// Place the odd result in the odd column
			//odd >>= _INVERSE_TEMPORAL_PRESCALE;
			*(colptr++) = SATURATE_16U(odd);
		}

		// Should have exited the loop at the column for right border processing
		assert(column == last_column);

		// Process the last two output points with special filters for the right border
		even = 0;
		odd = 0;

		// Apply the even reconstruction filter to the lowpass band
		even += 5 * lowpass[column + 0];
		even += 4 * lowpass[column - 1];
		even -= 1 * lowpass[column - 2];
		even += ROUNDING(even,8);
		even = DivideByShift(even, 3);

		// Add the highpass correction
		even += highpass[column];
		even = DivideByShift(even, 1);

		// Scale the result to the full 16-bit range
		even <<= scale_shift;

		// Place the even result in the even column
		//even >>= _INVERSE_TEMPORAL_PRESCALE;
		*(colptr++) = SATURATE_16U(even);

		// Apply the odd reconstruction filter to the lowpass band
		odd += 11 * lowpass[column + 0];
		odd -=  4 * lowpass[column - 1];
		odd +=  1 * lowpass[column - 2];
		odd += ROUNDING(odd,8);
		odd = DivideByShift(odd, 3);

		// Subtract the highpass correction
		odd -= highpass[column];
		odd = DivideByShift(odd, 1);

		// Scale the result to the full 16-bit range
		odd <<= scale_shift;

		// Place the odd result in the odd column
		//odd >>= _INVERSE_TEMPORAL_PRESCALE;
		*(colptr++) = SATURATE_16U(odd);

		// Advance to the next row of coefficients and output values
		lowpass += lowpass_pitch;
		highpass += highpass_pitch;
		output += output_pitch;
	}

	//_mm_empty();	// Clear the mmx register state
}
#endif



void
InvertHorizontalStrip16sToRow16uPlanar(HorizontalFilterParams)
{
	int i;
	int channels = decoder->codec.num_channels;
	int stripwidthC = roi.width/2;

	if(false == ALPHAOUTPUT(decoder->frame.format) && decoder->codec.encoded_format != ENCODED_FORMAT_BAYER)
		channels = 3;

	if(decoder->frame.resolution == DECODED_RESOLUTION_HALF_HORIZONTAL)
	{
		roi.width >>= 1;
		stripwidthC >>= 1;
	}

	for(i=0; i<channels; i++)
	{
		if(i > 0 && decoder->codec.encoded_format == ENCODED_FORMAT_YUV_422)
			roi.width = stripwidthC; //DAN20080730 -- added to allow this routine to handle 4:2:2 sources.

		if(decoder->frame.resolution == DECODED_RESOLUTION_HALF_HORIZONTAL)
		{
			InvertHorizontalBypassStrip16sToRow16u(
				lowpass_band[i], lowpass_pitch[i],
				(PIXEL16U*)output_image, output_pitch, roi,
				precision);
		}
		else
		{
			InvertHorizontalStrip16sToRow16u(
				lowpass_band[i], lowpass_pitch[i],
				highpass_band[i], highpass_pitch[i],
				(PIXEL16U*)output_image, output_pitch, roi,
				precision);
		}

		//output_buffer += output_pitch/channels;  //DAN20080702 -- this only works for 4:4:4:(4) and bayer sources.
		//output_buffer += lowpass_pitch[i];
		output_image += roi.width * 4;
	}
}



// Apply the inverse horizontal transform to reconstruct a strip of rows (new version using SSE2)
void
InvertHorizontalStrip16sYUVtoRGB(HorizontalFilterParams)
{
	int i;
	int channels = decoder->codec.num_channels;
	//uint8_t *chroma_buffer = output_image;
	uint8_t *plane_array[TRANSFORM_MAX_CHANNELS] = {0};
	int plane_pitch[TRANSFORM_MAX_CHANNELS] = {0};
	//unsigned short scanline[4096*6];
	uint8_t *sptr, *sptr2;
	int local_pitch = roi.width * 2 * 2 * 2; // 422 yuv 16-bit per channel

	void *scratch = NULL;
	size_t scratchsize = 0;

	scratch = decoder->threads_buffer[thread_index];
	scratchsize = decoder->threads_buffer_size;
	if(((int)scratchsize) < local_pitch)
	{
		assert(0);
		return;
	}

	//sptr = (uint8_t *)&scanline[0];
	sptr = scratch;
	sptr = sptr2 = (uint8_t *)((((uintptr_t)sptr)+15) & ~0xf);

	for(i=0; i<channels; i++)
	{
		int channel_pitch;
		ROI newstrip = roi;


		if(i>0)
		{
			newstrip.width >>= 1;
		}
		channel_pitch = newstrip.width*2*2;

		InvertHorizontalStrip16sToRow16u(
			lowpass_band[i], lowpass_pitch[i],
			highpass_band[i], highpass_pitch[i],
			(PIXEL16U*)sptr2, channel_pitch, newstrip,
			precision);

		plane_array[i] = (uint8_t *)sptr2;
		plane_pitch[i] = channel_pitch;

		if(i == 0)
			sptr2 += channel_pitch*2;
		else
			sptr2 += channel_pitch*2;
	}

	{
		ROI newstrip;

		newstrip.width = roi.width*2;
		newstrip.height = roi.height;
		if(channels >= 3)
			ConvertRow16uToDitheredBuffer(decoder, plane_array, plane_pitch, newstrip,
								   output_image, output_pitch, newstrip.width*2,
								   format, decoder->frame.colorspace);
	}
}


// Apply the inverse horizontal transform to reconstruct a strip of rows (new version using SSE2)
void
InvertHorizontalStrip16sThruActiveMetadata(HorizontalFilterParams)
{
	int i;
	int channels = decoder->codec.num_channels;
	//uint8_t *chroma_buffer = output_image;
	uint8_t *plane_array[TRANSFORM_MAX_CHANNELS] = {0};
	int plane_pitch[TRANSFORM_MAX_CHANNELS] = {0};
	unsigned short *scanline2;
	unsigned short *scanline3;
	unsigned short *scanline4;

	uint8_t *sptr, *sptr2;
	int local_pitch;

	uint8_t *scratch = decoder->threads_buffer[thread_index];
	size_t scratchsize = decoder->threads_buffer_size;

	scanline2 = (unsigned short *)scratch;
	scanline3 = (unsigned short *)(scratch + ((scratchsize/3)&0xffffff00));
	scanline4 = (unsigned short *)(scratch + ((scratchsize*2/3)&0xffffff00));


	sptr = (uint8_t *)&scanline2[0];
	sptr = sptr2 = (uint8_t *)((((uintptr_t)sptr)+15) & ~0xf);

	if(decoder->frame.resolution == DECODED_RESOLUTION_HALF_HORIZONTAL)
	{
		roi.width >>= 1;
	}
	
	local_pitch = roi.width * 2 * 2 * 2; // 422 yuv 16-bit per channel

	for(i=0; i<channels; i++)
	{
		ROI newstrip = roi;

		if(i>0)
		{
			newstrip.width >>= 1;
		}

		if(decoder->frame.resolution == DECODED_RESOLUTION_HALF_HORIZONTAL)
		{
			InvertHorizontalBypassStrip16sToRow16u(
				lowpass_band[i], lowpass_pitch[i],
				(PIXEL16U*)sptr2, local_pitch, newstrip,
				precision);
		}
		else
		{
			InvertHorizontalStrip16sToRow16u(
				lowpass_band[i], lowpass_pitch[i],
				highpass_band[i], highpass_pitch[i],
				(PIXEL16U*)sptr2, local_pitch, newstrip,
				precision);
		}

		plane_array[i] = (uint8_t *)sptr2;
		plane_pitch[i] = local_pitch;

		sptr2 += newstrip.width*2*2;
	}

	{
		ROI newstrip;
		uint8_t *sptr3, *sptr4;

		sptr = (uint8_t *)&scanline3[0];
		sptr = sptr3 = (uint8_t *)((((uintptr_t)sptr)+15) & ~0xf);
		sptr = (uint8_t *)&scanline4[0];
		sptr = sptr4 = (uint8_t *)((((uintptr_t)sptr)+15) & ~0xf);
		
		newstrip.width = roi.width*2;
		newstrip.height = 1;
		{
			unsigned char *output = (unsigned char *)output_image;
			int i;
			int height = roi.height;

			for(i=0; i<height; i++)
			{
				int whitebitdepth = 16;
				int flags = 0;
				int colorspace = decoder->frame.colorspace & (8|3); // VSRGB is done in cube

				ConvertYUVRow16uToBGRA64(plane_array, plane_pitch, newstrip,
					(unsigned char *)sptr3, newstrip.width, output_pitch,
					COLOR_FORMAT_RGB_8PIXEL_PLANAR, colorspace, &whitebitdepth, &flags);

				//flags = (ACTIVEMETADATA_PRESATURATED|ACTIVEMETADATA_SRC_8PIXEL_PLANAR);

				//TODO: Get the scanline number
				sptr = sptr3;
				if(decoder->apply_color_active_metadata)
					sptr = (uint8_t *)ApplyActiveMetaData(decoder, newstrip.width, 1, -1,
						(uint32_t *)sptr3, (uint32_t *)sptr4,
						decoder->frame.format, &whitebitdepth, &flags);					
				
				if(decoder->frame.colorspace & COLOR_SPACE_VS_RGB)
				{
					ConvertCGRGBtoVSRGB((PIXEL *)sptr, newstrip.width, whitebitdepth, flags);
				}

				ConvertLinesToOutput(decoder, newstrip.width, 1, 1, (PIXEL16U *)sptr,
					output, output_pitch, decoder->frame.format, whitebitdepth, flags);

				plane_array[0] += plane_pitch[0];
				plane_array[1] += plane_pitch[1];
				plane_array[2] += plane_pitch[2];

				output += output_pitch;
			}
		}
	}
}

